<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Probability | Quantitative Methods 1, ZHAW</title>
  <meta name="description" content="Script Quantitative Methods 1, ZHAW," />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Probability | Quantitative Methods 1, ZHAW" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Script Quantitative Methods 1, ZHAW," />
  <meta name="github-repo" content="jdegenfellner/Script_QM1_ZHAW" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Probability | Quantitative Methods 1, ZHAW" />
  
  <meta name="twitter:description" content="Script Quantitative Methods 1, ZHAW," />
  

<meta name="author" content="Jürgen Degenfellner" />


<meta name="date" content="2024-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="descriptive_stats.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.2.2/leaflet.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods 1</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#books-i-can-highly-recommend"><i class="fa fa-check"></i><b>1.1</b> Books I can (highly) recommend:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#section"><i class="fa fa-check"></i><b>1.2</b> <img src="images/Rlogo.png" height="20px"/></a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#additional-tools"><i class="fa fa-check"></i><b>1.3</b> Additional Tools</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#workflow-suggestion"><i class="fa fa-check"></i><b>1.4</b> Workflow suggestion</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#orientation-for-the-course-and-script"><i class="fa fa-check"></i><b>1.5</b> Orientation for the course and script</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#warning-of-incompleteness"><i class="fa fa-check"></i><b>1.6</b> Warning of incompleteness</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probs.html"><a href="probs.html"><i class="fa fa-check"></i><b>2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probs.html"><a href="probs.html#frequentist-vs.-bayesian-statistics"><i class="fa fa-check"></i><b>2.1</b> Frequentist vs. Bayesian statistics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="probs.html"><a href="probs.html#frequentist-statistics"><i class="fa fa-check"></i><b>2.1.1</b> Frequentist statistics</a></li>
<li class="chapter" data-level="2.1.2" data-path="probs.html"><a href="probs.html#bayesian-statistics"><i class="fa fa-check"></i><b>2.1.2</b> Bayesian statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probs.html"><a href="probs.html#foundations-of-probability-theory"><i class="fa fa-check"></i><b>2.2</b> Foundations of probability theory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="probs.html"><a href="probs.html#Questions_about_the_1000-researcher_experiment"><i class="fa fa-check"></i><b>2.2.1</b> Questions about the 1000 researcher-experiment (among many others):</a></li>
<li class="chapter" data-level="2.2.2" data-path="probs.html"><a href="probs.html#axioms_of_probability_theory"><i class="fa fa-check"></i><b>2.2.2</b> Axioms of probability theory</a></li>
<li class="chapter" data-level="2.2.3" data-path="probs.html"><a href="probs.html#independence-of-events"><i class="fa fa-check"></i><b>2.2.3</b> Independence of events</a></li>
<li class="chapter" data-level="2.2.4" data-path="probs.html"><a href="probs.html#difference-between-independence-and-disjointness"><i class="fa fa-check"></i><b>2.2.4</b> Difference between independence and disjointness</a></li>
<li class="chapter" data-level="2.2.5" data-path="probs.html"><a href="probs.html#Answers_Questions_about_the_1000-researcher_experiment"><i class="fa fa-check"></i><b>2.2.5</b> Answers to questions about the 1000 researcher-experiment (among many others):</a></li>
<li class="chapter" data-level="2.2.6" data-path="probs.html"><a href="probs.html#addition_of_probabilities"><i class="fa fa-check"></i><b>2.2.6</b> Addition of probabilities</a></li>
<li class="chapter" data-level="2.2.7" data-path="probs.html"><a href="probs.html#probabilities_for_health_sciences"><i class="fa fa-check"></i><b>2.2.7</b> Probabilities for health science</a></li>
<li class="chapter" data-level="2.2.8" data-path="probs.html"><a href="probs.html#discrete_vs_continuous_probability_distributions"><i class="fa fa-check"></i><b>2.2.8</b> Discrete vs. continuous probability distributions</a></li>
<li class="chapter" data-level="2.2.9" data-path="probs.html"><a href="probs.html#prominent_probability_distributions_in_health_sciences"><i class="fa fa-check"></i><b>2.2.9</b> Examples of prominent probability distributions used in health sciences</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probs.html"><a href="probs.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="probs.html"><a href="probs.html#exercise1"><i class="fa fa-check"></i><b>2.3.1</b> [M] Exercise 1 - Throwing a die very often</a></li>
<li class="chapter" data-level="2.3.2" data-path="probs.html"><a href="probs.html#exercise2"><i class="fa fa-check"></i><b>2.3.2</b> [D] Exercise 2 - Bayes-teaser</a></li>
<li class="chapter" data-level="2.3.3" data-path="probs.html"><a href="probs.html#exercise3"><i class="fa fa-check"></i><b>2.3.3</b> [E] Exercise 3 - Find journals</a></li>
<li class="chapter" data-level="2.3.4" data-path="probs.html"><a href="probs.html#exercise4"><i class="fa fa-check"></i><b>2.3.4</b> [M] Exercise 4 - Independent and disjoint</a></li>
<li class="chapter" data-level="2.3.5" data-path="probs.html"><a href="probs.html#exercise5"><i class="fa fa-check"></i><b>2.3.5</b> [M] Exercise 5 - Variance</a></li>
<li class="chapter" data-level="2.3.6" data-path="probs.html"><a href="probs.html#exercise6"><i class="fa fa-check"></i><b>2.3.6</b> [E] Exercise 6 - Three researchers</a></li>
<li class="chapter" data-level="2.3.7" data-path="probs.html"><a href="probs.html#exercise7"><i class="fa fa-check"></i><b>2.3.7</b> [E] Exercise 7 - Conditional probability</a></li>
<li class="chapter" data-level="2.3.8" data-path="probs.html"><a href="probs.html#exercise8"><i class="fa fa-check"></i><b>2.3.8</b> [E] Exercise 8 - Invent a discrete probability distribution</a></li>
<li class="chapter" data-level="2.3.9" data-path="probs.html"><a href="probs.html#exercise9"><i class="fa fa-check"></i><b>2.3.9</b> [E] Exercise 9 - Continuous probability distributions</a></li>
<li class="chapter" data-level="2.3.10" data-path="probs.html"><a href="probs.html#exercise10"><i class="fa fa-check"></i><b>2.3.10</b> [M] Exercise 10 - MSc-ZHAW-distribution</a></li>
<li class="chapter" data-level="2.3.11" data-path="probs.html"><a href="probs.html#exercise11"><i class="fa fa-check"></i><b>2.3.11</b> [M] Exercise 11 - Independence and disjointness for dice events</a></li>
<li class="chapter" data-level="2.3.12" data-path="probs.html"><a href="probs.html#exercise12"><i class="fa fa-check"></i><b>2.3.12</b> [D] Exercise 12 - Student’s <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probs.html"><a href="probs.html#solutions"><i class="fa fa-check"></i><b>2.4</b> Solutions</a></li>
<li class="chapter" data-level="2.5" data-path="probs.html"><a href="probs.html#sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german"><i class="fa fa-check"></i><b>2.5</b> Sample exam questions for this chapter (in German since exam is in German)</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="probs.html"><a href="probs.html#question-1---independence-and-disjointness"><i class="fa fa-check"></i><b>2.5.1</b> Question 1 - Independence and disjointness</a></li>
<li class="chapter" data-level="2.5.2" data-path="probs.html"><a href="probs.html#frage-2---bedingte-wahrscheinlichkeit"><i class="fa fa-check"></i><b>2.5.2</b> Frage 2 - Bedingte Wahrscheinlichkeit</a></li>
<li class="chapter" data-level="2.5.3" data-path="probs.html"><a href="probs.html#frage-3---erwartungswert-und-varianz"><i class="fa fa-check"></i><b>2.5.3</b> Frage 3 - Erwartungswert und Varianz</a></li>
<li class="chapter" data-level="2.5.4" data-path="probs.html"><a href="probs.html#frage-4---dichtefunktion"><i class="fa fa-check"></i><b>2.5.4</b> Frage 4 - Dichtefunktion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptive_stats.html"><a href="descriptive_stats.html"><i class="fa fa-check"></i><b>3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="descriptive_stats.html"><a href="descriptive_stats.html#example_study1_physio"><i class="fa fa-check"></i><b>3.1</b> Example: Descriptive statistics in health sciences</a></li>
<li class="chapter" data-level="3.2" data-path="descriptive_stats.html"><a href="descriptive_stats.html#univarate-vs.-bivariate-statisics"><i class="fa fa-check"></i><b>3.2</b> Univarate vs. bivariate statisics</a></li>
<li class="chapter" data-level="3.3" data-path="descriptive_stats.html"><a href="descriptive_stats.html#the-histogram"><i class="fa fa-check"></i><b>3.3</b> The histogram</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="descriptive_stats.html"><a href="descriptive_stats.html#example-in-the-wild"><i class="fa fa-check"></i><b>3.3.1</b> Example in the wild</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="descriptive_stats.html"><a href="descriptive_stats.html#q-q-plots"><i class="fa fa-check"></i><b>3.4</b> Q-Q Plots</a></li>
<li class="chapter" data-level="3.5" data-path="descriptive_stats.html"><a href="descriptive_stats.html#correlation"><i class="fa fa-check"></i><b>3.5</b> Correlation</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="descriptive_stats.html"><a href="descriptive_stats.html#example-in-the-wild-1"><i class="fa fa-check"></i><b>3.5.1</b> Example in the wild</a></li>
<li class="chapter" data-level="3.5.2" data-path="descriptive_stats.html"><a href="descriptive_stats.html#spearman-correlation"><i class="fa fa-check"></i><b>3.5.2</b> Spearman correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="descriptive_stats.html"><a href="descriptive_stats.html#exercises-1"><i class="fa fa-check"></i><b>3.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="descriptive_stats.html"><a href="descriptive_stats.html#d-exercise-1---recreate-table-with-fake-data"><i class="fa fa-check"></i><b>3.6.1</b> [D] Exercise 1 - Recreate table with fake data</a></li>
<li class="chapter" data-level="3.6.2" data-path="descriptive_stats.html"><a href="descriptive_stats.html#e-exercise-2---outliers-and-estimates"><i class="fa fa-check"></i><b>3.6.2</b> [E] Exercise 2 - Outliers and estimates</a></li>
<li class="chapter" data-level="3.6.3" data-path="descriptive_stats.html"><a href="descriptive_stats.html#exercise3_descriptive_stats"><i class="fa fa-check"></i><b>3.6.3</b> [E] Exercise 3 - Recreating data in Table 2</a></li>
<li class="chapter" data-level="3.6.4" data-path="descriptive_stats.html"><a href="descriptive_stats.html#exercise4_descriptive_stats"><i class="fa fa-check"></i><b>3.6.4</b> [E] Exercise 4 - Z-scores</a></li>
<li class="chapter" data-level="3.6.5" data-path="descriptive_stats.html"><a href="descriptive_stats.html#exercise5_descriptive_stats"><i class="fa fa-check"></i><b>3.6.5</b> [M] Exercise 5 - Correlation</a></li>
<li class="chapter" data-level="3.6.6" data-path="descriptive_stats.html"><a href="descriptive_stats.html#m-exercise-6---bike-parking-locations-in-switzerland"><i class="fa fa-check"></i><b>3.6.6</b> [M] Exercise 6 - Bike parking locations in Switzerland</a></li>
<li class="chapter" data-level="3.6.7" data-path="descriptive_stats.html"><a href="descriptive_stats.html#e-exercise-7---median-mean-and-mode"><i class="fa fa-check"></i><b>3.6.7</b> [E] Exercise 7 - Median, Mean, and Mode</a></li>
<li class="chapter" data-level="3.6.8" data-path="descriptive_stats.html"><a href="descriptive_stats.html#e-exercise-8---correlation-by-hand"><i class="fa fa-check"></i><b>3.6.8</b> [E] Exercise 8 - Correlation by hand</a></li>
<li class="chapter" data-level="3.6.9" data-path="descriptive_stats.html"><a href="descriptive_stats.html#exercise9_descriptive_stats"><i class="fa fa-check"></i><b>3.6.9</b> [E] Exercise 9 - Q-Q plot</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="descriptive_stats.html"><a href="descriptive_stats.html#solutions-1"><i class="fa fa-check"></i><b>3.7</b> Solutions</a></li>
<li class="chapter" data-level="3.8" data-path="descriptive_stats.html"><a href="descriptive_stats.html#sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german-1"><i class="fa fa-check"></i><b>3.8</b> Sample exam questions for this chapter (in German since exam is in German)</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="descriptive_stats.html"><a href="descriptive_stats.html#frage-1---mittelwert-median-modus"><i class="fa fa-check"></i><b>3.8.1</b> Frage 1 - Mittelwert, Median, Modus</a></li>
<li class="chapter" data-level="3.8.2" data-path="descriptive_stats.html"><a href="descriptive_stats.html#frage-2---normalverteilung"><i class="fa fa-check"></i><b>3.8.2</b> Frage 2 - Normalverteilung</a></li>
<li class="chapter" data-level="3.8.3" data-path="descriptive_stats.html"><a href="descriptive_stats.html#frage-3---korrelation"><i class="fa fa-check"></i><b>3.8.3</b> Frage 3 - Korrelation</a></li>
<li class="chapter" data-level="3.8.4" data-path="descriptive_stats.html"><a href="descriptive_stats.html#frage-4"><i class="fa fa-check"></i><b>3.8.4</b> Frage 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayes_statistics.html"><a href="bayes_statistics.html"><i class="fa fa-check"></i><b>4</b> Bayes statistics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bayes_statistics.html"><a href="bayes_statistics.html#derivation-of-bayes-theorem"><i class="fa fa-check"></i><b>4.1</b> Derivation of Bayes’ theorem</a></li>
<li class="chapter" data-level="4.2" data-path="bayes_statistics.html"><a href="bayes_statistics.html#bayes-theorem-in-the-context-of-parameter-estimation"><i class="fa fa-check"></i><b>4.2</b> Bayes’ theorem in the context of parameter estimation</a></li>
<li class="chapter" data-level="4.3" data-path="bayes_statistics.html"><a href="bayes_statistics.html#examples"><i class="fa fa-check"></i><b>4.3</b> Examples</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bayes_statistics.html"><a href="bayes_statistics.html#example_defective_products"><i class="fa fa-check"></i><b>4.3.1</b> Example 1 - defective products</a></li>
<li class="chapter" data-level="4.3.2" data-path="bayes_statistics.html"><a href="bayes_statistics.html#example_defective_products_extended"><i class="fa fa-check"></i><b>4.3.2</b> Example 2 - extending the defective products example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bayes_statistics.html"><a href="bayes_statistics.html#highest-density-intervals-hdi"><i class="fa fa-check"></i><b>4.4</b> Highest Density Intervals (HDI)</a></li>
<li class="chapter" data-level="4.5" data-path="bayes_statistics.html"><a href="bayes_statistics.html#bayesian_t_test"><i class="fa fa-check"></i><b>4.5</b> Bayesian <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="bayes_statistics.html"><a href="bayes_statistics.html#example---bayesian-t-test"><i class="fa fa-check"></i><b>4.5.1</b> Example - Bayesian <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="bayes_statistics.html"><a href="bayes_statistics.html#bayesian-updating"><i class="fa fa-check"></i><b>4.6</b> Bayesian updating</a></li>
<li class="chapter" data-level="4.7" data-path="bayes_statistics.html"><a href="bayes_statistics.html#more-complex-parameter-spaces"><i class="fa fa-check"></i><b>4.7</b> More complex parameter spaces</a></li>
<li class="chapter" data-level="4.8" data-path="bayes_statistics.html"><a href="bayes_statistics.html#advantagesdisadvantages-of-bayesian-statistics"><i class="fa fa-check"></i><b>4.8</b> Advantages/disadvantages of Bayesian statistics</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="bayes_statistics.html"><a href="bayes_statistics.html#some-advantages"><i class="fa fa-check"></i><b>4.8.1</b> (Some) Advantages</a></li>
<li class="chapter" data-level="4.8.2" data-path="bayes_statistics.html"><a href="bayes_statistics.html#some-disadvantages"><i class="fa fa-check"></i><b>4.8.2</b> (Some) Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="bayes_statistics.html"><a href="bayes_statistics.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
<li class="chapter" data-level="4.10" data-path="bayes_statistics.html"><a href="bayes_statistics.html#exercise_defective_product_rate"><i class="fa fa-check"></i><b>4.10</b> [E] Exercise 1 - defective product rate</a></li>
<li class="chapter" data-level="4.11" data-path="bayes_statistics.html"><a href="bayes_statistics.html#h-exercise-2---bayesian-updating"><i class="fa fa-check"></i><b>4.11</b> [H] Exercise 2 - Bayesian updating</a></li>
<li class="chapter" data-level="4.12" data-path="bayes_statistics.html"><a href="bayes_statistics.html#solutions-2"><i class="fa fa-check"></i><b>4.12</b> Solutions</a></li>
<li class="chapter" data-level="4.13" data-path="bayes_statistics.html"><a href="bayes_statistics.html#sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german-2"><i class="fa fa-check"></i><b>4.13</b> Sample exam questions for this chapter (in German since exam is in German)</a>
<ul>
<li class="chapter" data-level="4.13.1" data-path="bayes_statistics.html"><a href="bayes_statistics.html#frage-1"><i class="fa fa-check"></i><b>4.13.1</b> Frage 1</a></li>
<li class="chapter" data-level="4.13.2" data-path="bayes_statistics.html"><a href="bayes_statistics.html#frage-2"><i class="fa fa-check"></i><b>4.13.2</b> Frage 2</a></li>
<li class="chapter" data-level="4.13.3" data-path="bayes_statistics.html"><a href="bayes_statistics.html#frage-3"><i class="fa fa-check"></i><b>4.13.3</b> Frage 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nhst.html"><a href="nhst.html"><i class="fa fa-check"></i><b>5</b> Null Hypothesis Significance Testing (NHST)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="nhst.html"><a href="nhst.html#example-in-the-literature"><i class="fa fa-check"></i><b>5.1</b> Example in the literature</a></li>
<li class="chapter" data-level="5.2" data-path="nhst.html"><a href="nhst.html#binomial-test"><i class="fa fa-check"></i><b>5.2</b> Binomial test</a></li>
<li class="chapter" data-level="5.3" data-path="nhst.html"><a href="nhst.html#proportions-test"><i class="fa fa-check"></i><b>5.3</b> Proportions test</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="nhst.html"><a href="nhst.html#one-sample-case"><i class="fa fa-check"></i><b>5.3.1</b> One sample case</a></li>
<li class="chapter" data-level="5.3.2" data-path="nhst.html"><a href="nhst.html#proportions_test_more_samples"><i class="fa fa-check"></i><b>5.3.2</b> More than one proportion</a></li>
<li class="chapter" data-level="5.3.3" data-path="nhst.html"><a href="nhst.html#fishers-exact-test"><i class="fa fa-check"></i><b>5.3.3</b> Fisher’s exact test</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="nhst.html"><a href="nhst.html#classical-t-test"><i class="fa fa-check"></i><b>5.4</b> (Classical) <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="nhst.html"><a href="nhst.html#one-sample-t-test"><i class="fa fa-check"></i><b>5.4.1</b> One sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.4.2" data-path="nhst.html"><a href="nhst.html#two-sample-t-test"><i class="fa fa-check"></i><b>5.4.2</b> Two sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="nhst.html"><a href="nhst.html#correlation-test"><i class="fa fa-check"></i><b>5.5</b> Correlation test</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="nhst.html"><a href="nhst.html#classical-correlation-test"><i class="fa fa-check"></i><b>5.5.1</b> Classical correlation test</a></li>
<li class="chapter" data-level="5.5.2" data-path="nhst.html"><a href="nhst.html#bootstrap-confidence-interval-for-the-correlation-coefficient"><i class="fa fa-check"></i><b>5.5.2</b> Bootstrap confidence interval for the correlation coefficient</a></li>
<li class="chapter" data-level="5.5.3" data-path="nhst.html"><a href="nhst.html#comparision-with-bayesian-approach"><i class="fa fa-check"></i><b>5.5.3</b> Comparision with Bayesian approach</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="nhst.html"><a href="nhst.html#error_types"><i class="fa fa-check"></i><b>5.6</b> Type 1 and Type 2 errors</a></li>
<li class="chapter" data-level="5.7" data-path="nhst.html"><a href="nhst.html#the-frequentist-confidence-interval"><i class="fa fa-check"></i><b>5.7</b> The frequentist confidence interval</a></li>
<li class="chapter" data-level="5.8" data-path="nhst.html"><a href="nhst.html#simulations-based-approaches"><i class="fa fa-check"></i><b>5.8</b> Simulations based approaches</a></li>
<li class="chapter" data-level="5.9" data-path="nhst.html"><a href="nhst.html#exercises-3"><i class="fa fa-check"></i><b>5.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="nhst.html"><a href="nhst.html#exercise1_nhst"><i class="fa fa-check"></i><b>5.9.1</b> Exercise 1 - frequentist confidence interval</a></li>
<li class="chapter" data-level="5.9.2" data-path="nhst.html"><a href="nhst.html#exercise2_nhst"><i class="fa fa-check"></i><b>5.9.2</b> Exercise 2 - everything becomes “significant”</a></li>
<li class="chapter" data-level="5.9.3" data-path="nhst.html"><a href="nhst.html#exercise3_nhst"><i class="fa fa-check"></i><b>5.9.3</b> Exercise 3 - binomial test</a></li>
<li class="chapter" data-level="5.9.4" data-path="nhst.html"><a href="nhst.html#exercise4_nhst"><i class="fa fa-check"></i><b>5.9.4</b> Exercise 4 - proportions test</a></li>
<li class="chapter" data-level="5.9.5" data-path="nhst.html"><a href="nhst.html#exercise5_nhst"><i class="fa fa-check"></i><b>5.9.5</b> Exercise 5 - proportions test 2</a></li>
<li class="chapter" data-level="5.9.6" data-path="nhst.html"><a href="nhst.html#exercise6_nhst"><i class="fa fa-check"></i><b>5.9.6</b> Exercise 6 - correlation coefficent</a></li>
<li class="chapter" data-level="5.9.7" data-path="nhst.html"><a href="nhst.html#exercise7_nhst"><i class="fa fa-check"></i><b>5.9.7</b> Exercise 7 - coverage frequency of CI</a></li>
<li class="chapter" data-level="5.9.8" data-path="nhst.html"><a href="nhst.html#exercise8_nhst"><i class="fa fa-check"></i><b>5.9.8</b> Exercise 8 - <span class="math inline">\(\chi^2\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="nhst.html"><a href="nhst.html#sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german-3"><i class="fa fa-check"></i><b>5.10</b> Sample exam questions for this chapter (in German since exam is in German)</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="nhst.html"><a href="nhst.html#frage-1-1"><i class="fa fa-check"></i><b>5.10.1</b> Frage 1</a></li>
<li class="chapter" data-level="5.10.2" data-path="nhst.html"><a href="nhst.html#frage-2-1"><i class="fa fa-check"></i><b>5.10.2</b> Frage 2</a></li>
<li class="chapter" data-level="5.10.3" data-path="nhst.html"><a href="nhst.html#frage-3---t-test"><i class="fa fa-check"></i><b>5.10.3</b> Frage 3 - <span class="math inline">\(t\)</span>-Test</a></li>
<li class="chapter" data-level="5.10.4" data-path="nhst.html"><a href="nhst.html#frage-4-1"><i class="fa fa-check"></i><b>5.10.4</b> Frage 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods 1, ZHAW</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probs" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Probability<a href="probs.html#probs" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Probability is a measure of the likelihood that an event will occur. Probability is quantified as a number between 0 and 1 (or 0 to 100%),
where 0 indicates impossibility and 1 indicates certainty, although we will see later that a <a href="https://stats.stackexchange.com/questions/273382/how-can-the-probability-of-each-point-be-zero-in-continuous-random-variable">probability of 0</a> does not necessarily mean that such an event can never occur.
The higher the probability of an event, the more likely it is that the event will occur.</p>
<p><strong>Why is probability important in our field of study (applied health sciences)?</strong></p>
<p>Quantative research methods (often a code name for statistics) use probability theory to make statements about a larger
population or a data generating process (DGP), as it should be more appropriately called.</p>
<p>In observational studies, we often make statements about associations between variables.</p>
<p>In experimental studies (e.g., a randomized controlled trial), we often try to make statements about the effect of an intervention on a
certain outcome - for instance if a therapy lowers <a href="https://jamanetwork.com/journals/jama/fullarticle/2794765">pain</a> by at least 1 point better compared to usual therapies.</p>
<p>Probability theory has its roots in gambling and betting. <a href="https://en.wikipedia.org/wiki/Blaise_Pascal">Blaise Pascal</a> wrote a letter to <a href="https://en.wikipedia.org/wiki/Pierre_de_Fermat">Pierre de Fermat</a> in 1654 when a
French essayist Antoine Gombaud, intrigued by gambling, sought to solve “the problem of points,”
first posed by Luca Paccioli in 1494. The problem asked how to fairly divide the winnings if a game is interrupted before
its conclusion. Gombaud approached mathematician Blaise Pascal, who collaborated with Pierre de Fermat. Together, they laid
the groundwork for modern probability theory. Fermat’s method involved listing all possible outcomes and calculating each player’s
chance of winning, while Pascal developed a backward induction algorithm to assign probabilities. Their work revolutionized mathematics
and influenced fields like economics and actuarial science.</p>
<p>Philosophically speaking, we could distinguish between two flavors of probability: Probabilities for events that are repeatable respectively have already happened, and probabilities for events that haven’t happened yet.</p>
<p>An example for a repeatable event is getting a 6 when throwing a fair die. We can do this experiment right now by fetching a die and throwing it.</p>
<p>An example for the latter is the probability of a patient dying within the next 5 years after a certain diagnosis.
It is hard to argue that this experiment would be repeatable under (almost) identical conditions since every patient is different whereas the dice are typically much more similar.
Here, we could at least put forward that other similar patients have a certain proportion of dying within 5 years.</p>
<p>There are of course events that have not happend ever before, like the creation of artificial general intelligence (<a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a>).
Nevertheless, one can still try to assign probabilities <a href="https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/">when</a> such an event would happen.</p>
<div id="frequentist-vs.-bayesian-statistics" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Frequentist vs. Bayesian statistics<a href="probs.html#frequentist-vs.-bayesian-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two main schools of thought in statistics: Frequentist and Bayesian. Often one hears that there is a “war” <a href="https://www.youtube.com/watch?v=8wVq5aGzSqY&amp;t=22s&amp;ab_channel=VeryNormal">between the two</a>.</p>
<p>It is <em>not</em> our place to say which one is better. Both have their strengths and weaknesses and are used in different contexts.</p>
<p>I would consider the rapant misuse of <a href="https://www.sciencedirect.com/science/article/abs/pii/S0037196308000620?via%3Dihub"><span class="math inline">\(p\)</span>-values</a> and the
cookbook-like application of frequentist statistics as a weakness of this approach (in its widely used form at least).
Of course, this is not the method’s fault but the fault of the user.</p>
<p>Bayesian statistics is often considered more intuitive and flexible. It is also more computationally demanding and requires prior knowledge which is argued to be subjective.
Computation time is sometimes still an issue in comparison for instance in regression modelling when using an end user laptop.
It is also argued that for large sample size frequentist and Bayesian statistics converge to the same result.</p>
<p>There are very smart proponents on both sides and we will try to use and contrast both techniques throughout this script whenever convenient.</p>
<p>Especially one of the early eminent statisticians, <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>, was an oponent of Bayesian statistics,
or as he called it: <a href="https://errorstatistics.com/wp-content/uploads/2016/02/fisher-1930-inverse-probability.pdf">“inverse probability”</a>.</p>
<p>The only thing we are interested in is the practical application of both methods in the field of applied health sciences.
How well can we describe data and make predictions, how well can we learn from data in our field?</p>
<div id="frequentist-statistics" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Frequentist statistics<a href="probs.html#frequentist-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Frequentist statistics is based on the idea that <a href="https://en.wikipedia.org/wiki/Frequentist_probability">probability</a> is the long-run
frequency of events. For instance, if I throw a fair die 1000 times, the frequency of getting a 3 is (approximately) <span class="math inline">\(\frac{1}{6}\)</span>. In the limit,
if I throw the die infinitely many times, the frequency of getting a 3 will converge to <span class="math inline">\(\frac{1}{6}\)</span>. In mathematical notation, we would write</p>
<p><span class="math display">\[
\mathbb{P}(\text{getting a 3}) = \lim_{n \to \infty} \frac{\text{Number of 3s in } n \text{ throws}}{n} = \frac{1}{6},
\]</span></p>
<p>where <span class="math inline">\(\mathbb{P}\)</span> is the probability measure which we will define more formally later (see <a href="probs.html#exercise1">Exercise 1</a>).</p>
<p>More genereally, in frequentist statistics, we are looking for a fixed but unknown parameter from an underlying data generating process (DGP).
In the dice example, the process of repeatedly throwing the die is the data generating process.
Basically, we could estimate the parameter of interest arbitrarily well by reapeated drawing from the DGP if we had enough data.</p>
<p><strong>Example</strong>: Throw your (fair or unfair) die often enough and you will get a good estimate of the probability of getting a 3.</p>
<p><strong>Example</strong>: We could try to estimate the mean birth weight of all babies from smoking parents born in Switzerland in 2022.
We would draw a (random) sample of birthweights and calculate the mean. With a sample large enough, we could estimate this parameter fairly well.
With all birthweights, we would know the true mean of the population of interest (for that year alone).</p>
</div>
<div id="bayesian-statistics" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Bayesian statistics<a href="probs.html#bayesian-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bayesian statistics, on the other hand, is based on the idea that <a href="https://en.wikipedia.org/wiki/Bayesian_probability">probability</a> is a measure of our uncertainty about an event or a parameter.
Here, we use <em>prior</em> (i.e., before/outside of our experiment) knowledge about a parameter and update this knowledge with new data using the famous Bayes’ theorem:</p>
<p><span class="math display">\[
p(\theta | \text{data}) = \frac{p(\text{data} | \theta) \cdot p(\theta)}{p(\text{data})},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(p(\theta | \text{data})\)</span> is the <strong>posterior probability</strong>: the updated probability of the parameter <span class="math inline">\(\theta\)</span> given the observed data.</p></li>
<li><p><span class="math inline">\(p(\text{data} | \theta)\)</span> is the <strong>likelihood</strong>: the probability of observing the data given a certain value of the parameter <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(p(\theta)\)</span> is the <strong>prior probability</strong>: the initial belief about the parameter <span class="math inline">\(\theta\)</span> before seeing the data.</p></li>
<li><p><span class="math inline">\(p(\text{data})\)</span> is the <strong>marginal likelihood</strong> or <strong>evidence</strong>: the probability of observing the data under all possible parameter values.</p></li>
</ul>
<div id="example1_physio" class="section level4 hasAnchor" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Example in applied health sciences (physiotherapy)<a href="probs.html#example1_physio" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose you’re a physiotherapist trying to estimate the probability that a new therapy improves the mobility of patients with chronic back pain (Improvement Yes/No).
You already have some prior knowledge (based on previous studies or expert opinions) that suggests the therapy works for 30% of patients.
This is your <strong>prior knowledge</strong>: <span class="math inline">\(\theta = 0.30\)</span>, where <span class="math inline">\(\theta\)</span> is the probability that the therapy is effective.
Your colleagues are not convinced and argue that the probability is 40%.
Now, you run a small trial with 50 patients and observe that 22 of them showed a clinically relevant improvement in mobility (self-reported from the patient).
This new data (the result of the trial) <em>updates</em> your belief about the effectiveness of the therapy.
Using Bayes’ theorem (<a href="probs.html#exercise2">Exercise 2</a>), you combine the prior knowledge <span class="math inline">\(\theta = 0.30\)</span> with the likelihood of the new data <span class="math inline">\(p(\text{data} | \theta)\)</span>, and you calculate
the <strong>posterior probability</strong>, <span class="math inline">\(p(\theta | \text{data})\)</span>, which reflects your updated belief about the effectiveness of the therapy after observing the trial data.
We could assign the probability of <span class="math inline">\(\theta = 0.3\)</span> or <span class="math inline">\(\theta = 0.4\)</span> equally: <span class="math inline">\(p(\theta = 0.3) = p(\theta = 0.4) = 0.5.\)</span></p>
<p>Bayesian analysis allows you to update your estimates as new evidence becomes available, providing a flexible framework for decision-making in health sciences.</p>
</div>
</div>
</div>
<div id="foundations-of-probability-theory" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Foundations of probability theory<a href="probs.html#foundations-of-probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We need to know some basic concepts of probability theory in order to dive in deeper. We will try to introduce them playfully and find formality as we go along.
As stated above, in the frequentist sense, we are interested in the long-run frequency of events. How often does an event occur if we repeat the random experiment many times?</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Eureka_(word)">Eureka</a>?</strong></p>
<p>Let’s imagine we are in a research department with 1000 researchers all trying to answer the same question:
Does the new physiotherapy work (e.g., reduce pain by 1 point better than the usual treatment)? Let’s assume (unrealistically) that they are all working on this one question and they are not
talking about their experiments or their research methodology to each other (assumption of independence). The statistician in the department has calculated
(due to the variability of such treatment effects in the relevant population and theoretical considerations)
that even under the assumption of the therapy is not working <em>at all</em> - <strong>which we will assume for the time being</strong> - , one would see an effect just <em>by chance</em> in 4% of the study results.</p>
<p>What would be considered a discovery under these cicumstances?</p>
<p>We now conduct an experiment. All 1000 researchers are conducting a study with 50 patients to answer the same question. This is our random experiment (instead of throwing dice).
Instead of throwing a fair die, we do a round of “research” with 1000 researchers. You as observer
give the assignment to the researchers and come back as soon as all 1000 researeches have finished their experiments. Again, the are not taking to each other and we can
(unrealistically) assume that their results will be not influenced by each other.</p>
<p><img src="images/thousand_researchers_ducks.jpg" /></p>
<p>Now we could ask different questions:</p>
<div id="Questions_about_the_1000-researcher_experiment" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Questions about the 1000 researcher-experiment (among many others):<a href="probs.html#Questions_about_the_1000-researcher_experiment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>If you had to bet, how many experiments showed a treatment effect if you assume that the therapy is not working at all?</li>
<li>If you get 137 results showing a treatment effect, would you be surprised? Would you reject the assumption, that the therapy is not working at all? Why?</li>
<li>How many experiments (would you expect) showed a treatment effect if you assume that the therapy is “working” (positive result by chance) in 12% (instead of 4%) of the patients?</li>
<li>Assuming that you have 47 results showing a treatment effect and your marketing lead is asking you to write a press release stating that 47
out of 50 studies showed a treatment effect. What is the problem?</li>
<li>Assuming one very motivated researcher has tested 65 (secondary) hypotheses in her experiments and found 4 results that are difficult to explain by chance alone. What is the problem?</li>
<li>Suppose there are many large research departments in the world with 1000 researchers. How strongly would the number of positive results vary between these large departments?</li>
</ol>
<p>We will try to answer these questions <a href="probs.html#Answers_Questions_about_the_1000-researcher_experiment">below</a>.</p>
<p>First, it seems intuitive that Probability <em>within</em> an experiment should add up <em>if</em> the events are <strong>disjoint</strong>.
The event <span class="math inline">\(A_1=\)</span> “only researcher 45 gets a positive result” and <span class="math inline">\(A_2=\)</span> “only researcher 897 gets a positive result” are mutually exclusive.
If only researcher 45 finds an effect, then researcher 897 does not find an effect and vice versa. They cannot happen at the same time within that one
experiment. Hence, the two events are said to be disjoint.
If we add up the probabilities of all mutually exclusive events, we should get 1, or 100%.
We say that the probability of all elementary events (called <span class="math inline">\(\omega\)</span>) sums to 1. Let’s look at a <a href="https://en.wikipedia.org/wiki/Venn_diagram">Venn diagram</a> to illustrate the concept of being mutually exclusive (disjoint).</p>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Again, this refers to being mutually exclusive within our 1000-researcher experiment.
Both events cannot happen at the same time in this context, so we assign <span class="math inline">\(0\)</span> to the event that both occur simultaneously: <span class="math inline">\(\mathbb{P}(A_1 \cap A_2)=0\)</span>. The <span class="math inline">\(\cap\)</span>-Symbol refers to all elementary events
that are in both <a href="https://en.wikipedia.org/wiki/Set_(mathematics)">sets</a>. In our case we have the sets</p>
<p><span class="math display">\[A_1 = \{ (\dots ,R_{45} = pos, \dots ,R_{897} = neg, \dots) \}\]</span></p>
<p><span class="math display">\[and\]</span></p>
<p><span class="math display">\[A_2 = \{ (\dots ,R_{45} = neg, \dots , R_{897} = pos, \dots) \}.\]</span></p>
<p>An example of <strong>non-disjoint</strong> events (within our 1000-researcher experiment) would be the event <span class="math inline">\(A_1=\)</span> “only researcher 45 gets a positive result” and
the event <span class="math inline">\(A_3=\)</span> “only researcher 45 or only reasearcher 67 gets a positive result”.
Which researchers got a positive result in both events? The answer is: Researcher 45. Hence, the two events are said to be non-mutually exclusive. We can’t just
add up the probabilities (of events <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_3\)</span>) here, since we would count the probability of researcher 45 twice.
The sets look like this:</p>
<p><span class="math display">\[A_1 = \{ (\dots ,R_{45} = pos, \dots) \}\]</span></p>
<p><span class="math display">\[and\]</span></p>
<p><span class="math display">\[A_3 = \{ (\dots ,R_{45} = pos, \dots), (\dots ,R_{67} = pos, \dots) \}.\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>How many elementary events are in the set of all possible outcomes of our 1000-researcher experiment? For every researcher, there are two possible outcomes: positive or negative result.
Hence, we have <span class="math inline">\(2 \cdot 2 \cdot 2 \cdots = 2^{1000}\)</span> elementary events in our set of all possible outcomes.
This is a very large number (<span class="math inline">\(\sim 10^{300}\)</span>) - more than there are particles in the universe (<span class="math inline">\(\sim 10^{80}\)</span>).</p>
<p>We call the set of all elementary events <span class="math inline">\(\Omega\)</span> (the Greek letter Omega):
<span class="math display">\[\Omega = \{ \omega_1, \omega_2, \cdots, \omega_{2^{1000}} \}.\]</span></p>
<p>Note, that <strong>we collect elementary events to form events</strong> like we just did for event <span class="math inline">\(A_3\)</span>.</p>
<p>Note, that the <strong><span class="math inline">\(2^{1000}\)</span> elementary events in the 1000 researcher experiment are also disjoint</strong>. Why? For every elementary event,
certain researchers found something and others did not. The combinations are all different from each other. Hence,
all the elementary events cannot happen at the same time within that one experiment. All of them are disjoint.</p>
<p>The probability of the event “” (nothing occurred) should be zero (<span class="math inline">\(\mathbb{P}(\emptyset)=0\)</span>), were “” denotes
the event that no researcher gets a positive or negative result ( = <span class="math inline">\(\emptyset\)</span>, the so-called empty set).
This is impossible due to the design of the experiment.
We would therefore define this probability as zero and (if we can count the number of different outcomes)
this event can indeed <em>never</em> happen.</p>
<p>Obviously, the probability of an event should at a mininum be zero and at a maximum be one:</p>
<p><span class="math display">\[0 \le \mathbb{P}(A) \le 1.\]</span></p>
</div>
<div id="axioms_of_probability_theory" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Axioms of probability theory<a href="probs.html#axioms_of_probability_theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can summarize these informally stated properties more formally (<a href="https://altexploit.wordpress.com/wp-content/uploads/2017/07/a-n-kolmogorov-foundations-of-the-theory-of-probability-chelsea-pub-co-1960.pdf">Kolmogorov’s axioms</a>):</p>
<p><span class="math display">\[\begin{align}
1. &amp;\ \mathbb{P}(\emptyset) = 0 \text{: Probability of the &quot;impossible&quot; event should be zero.}\\
2. &amp;\ \mathbb{P}(\Omega) = 1 \text{: Probability, that any outcome occurs in our random experiment.}\\
3. &amp;\ \text{If } A_1, A_2,... \text{ pairwise disjoint: } \mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mathbb{P}(A_i)
\end{align}\]</span></p>
<p>The <span class="math inline">\(\infty\)</span>-symbol in <strong>axiom 3</strong> comes into play if we are dealing with (potentially) infinitely many events.
For instance, we could ask for the number of researchers we need to look at until we see the first
positive result (<a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a>).
We could find the first positive result in the first researcher, or the second, etc. There is no upper limit.</p>
<p>As concrete example for law 3 in our example, we can put the following:
<span class="math display">\[\begin{align}
\scriptsize \mathbb{P}(\text{&quot;(only) researchers 34, 56 and 777 get a pos. result&quot; or &quot;(only) researchers 1 and 5 get a pos. results&quot;}) =\\
\scriptsize \mathbb{P}(\text{&quot;(only) researchers 34, 56 and 777 get a pos. result&quot;}) + \mathbb{P}(\text{&quot;(only) researchers 1 and 5 get a pos. results&quot;})
\end{align}\]</span></p>
<p>Since the researchers are working independently from each other, we can simply multiply the probabilities of their individual positive or negative results in
our larger 1000-researcher experiment. For example, for the first probability there are exactly 3 positive results (=effect found) and 997 negative results (=no effect found).
This can be calculated as:
<span class="math inline">\(0.04 \cdot 0.04 \cdot 0.04 \cdot \underbrace{0.96 \cdots 0.96}_{\text{997 times}} = 0.04^3 \cdot 0.96^{997}\)</span>, which yields a very small number (<span class="math inline">\(1.350826 \cdot 10^{-22}\)</span>)
since we are fixating on specific researchers to find the effect.
If we relax the question to the <em>number of researchers</em> that find an effect,
we get much larger numbers. We say, the number <span class="math inline">\(X\)</span> of positive results under <span class="math inline">\(H_0\)</span> (there is no true effect)) for a positive effect is <strong>binomially distributed</strong>: <span class="math inline">\(X \sim Bin(n=1000, p=0.04)\)</span>.
The YouTube-channel <a href="https://www.youtube.com/@3blue1brown">3Blue1Brown</a> is highly recommended in general. You should watch this
<a href="https://www.youtube.com/watch?v=8idr1WZ1A7Q&amp;ab_channel=3Blue1Brown">video</a> on the binomial distribution to get a clearer picture.
This <a href="https://www.youtube.com/watch?v=WWv0RUxDfbs&amp;ab_channel=KhanAcademy">video</a> from <a href="https://www.youtube.com/@khanacademy">KhanAcademy</a> could also be helpful.
In our example, the probability that exactly 3 researchers find an effect is <span class="math inline">\(\binom{1000}{3} \cdot 0.04^3 \cdot 0.96^{997} = 2.244627 \cdot 10^{-14}\)</span>.
Still small, but much higher than before. Of course, the commands in R can be found easily via Google or your favourite large language model (LLM):
“Give me the commands for the binomial distribution in R and a nice example too”.
Note that the sum of all elementary events (all possible outcomes) indeed adds up to 1 in our 1000-researcher-experiment: <span class="math inline">\(\sum_{i=0}^{1000} \binom{1000}{i}0.04^i 0.96^{1000-i} = 1\)</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="probs.html#cb1-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>, <span class="at">size =</span> <span class="dv">1000</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="probs.html#cb3-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">80</span>, <span class="at">prob =</span> <span class="fl">0.04</span>, <span class="at">size =</span> <span class="dv">1000</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="probs.html#cb5-1" tabindex="-1"></a><span class="co"># = 1 since the other probabilities are very small</span></span></code></pre></div>
<p>As we will see later, <strong>axiom 1</strong> above does not mean, that the event can never occur.
For every continuous random variable (e.g. with a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal</a>
or a <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">uniform distribution</a>), the probability of a single point is zero.
This <a href="https://www.youtube.com/watch?v=ZA4JkHKZM50&amp;list=PLIhj5_pQwhMQfGZNQZREjDdJRt0t0sn51&amp;index=10&amp;ab_channel=3Blue1Brown">video</a> could help.</p>
<p><strong>Axiom 2</strong> is always true. Some result <em>has</em> to occur in our random experiment.
What is <span class="math inline">\(\Omega\)</span> again? In our countable case of researchers, <span class="math inline">\(\Omega = \{ \omega_1, \omega_2, \cdots, \omega_{2^{1000}} \}\)</span> would be the set of all possible outcomes if we let 1000 researchers conduct the experiment.
Each researcher can either find an effect or not. Hence, we have <span class="math inline">\(2^{1000}\)</span> possible outcomes of our 1000-researcher experiment. This is a <em>very</em> large number. Adding up all
these probabilities would sum to 1 according to axiom 3. Combining different elementary events <span class="math inline">\(\omega\)</span> from the whole collection of possible outcomes <span class="math inline">\(\Omega\)</span>
gives us “events” like the ones we used above (<span class="math inline">\(A_1, A_2, A_3\)</span>).</p>
<p>Note that there is a <strong>difference between the elementary experiment of the individual researcher (finding an effect or not) and the whole experiment</strong> of
1000 researchers we are looking at (simultaneously).
Do not make the mistake to add the single probabilities of finding an effect (under <span class="math inline">\(H_0\)</span>) of 0.04 to get the probability of finding an effect in the whole experiment:
This would result in: <span class="math inline">\(1000 \cdot 0.04 = 40 &gt; 1\)</span>, which is hardly a probability anymore.</p>
<p>This leads us to the concept of independence of events.</p>
</div>
<div id="independence-of-events" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Independence of events<a href="probs.html#independence-of-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if the occurrence of one event does not affect the occurrence of the other event. In plain English,
the probability of event <span class="math inline">\(A\)</span> happening is the same whether event <span class="math inline">\(B\)</span> happens or not. Mathematically, we can write this as:</p>
<p><span class="math display">\[\mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B)\]</span> or equivalently:
<span class="math display">\[\mathbb{P}(A | B) = \mathbb{P}(A).\]</span></p>
<p>A simple <strong>example in our context</strong>:
The probability of researcher 45 finding an effect (event A) is the same whether researcher 67 finds
an effect (event B) or not since they are not communicating with each other.
This is the reason why we just multiplied the probabilities of the individual researchers finding
an effect/not finding an effect to get the probability of the whole elementary event of the
1000-researcher experiment.</p>
<p>Above, we used the very important concept of <strong>conditional probability</strong>.
The probability of event <span class="math inline">\(A\)</span> <em>given</em> that event <span class="math inline">\(B\)</span> has occurred (not necessarily chronologically different!) is denoted as</p>
<p><span class="math display">\[\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}.\]</span></p>
<p>This <a href="https://www.youtube.com/watch?v=ibINrxJLvlM&amp;ab_channel=Dr.TreforBazett">video</a> explains it well.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>For the probability of event A, we are now <em>only</em> interested in the light-grey area with respect to the whole area of event B
since event B is our reference frame now (as opposed to the whole space <span class="math inline">\(\Omega\)</span> before).
Note that even if the probability of event A changes when B has happend, B could still have no <em>causal</em> effect on A. They could have a common cause, for instance.</p>
<p><strong>Example in our context</strong>:
Let’s assume researchers 45 and 67 would not be independent. We would for instance find that the probability of 45 is higher than 4% if we knew that 67 found the effect.
This does not necessarily mean that researcher 67 causes researcher 45 to find an effect. It could
might as well be that their statistical training was very similar and they both made the same mistake in their analysis.</p>
</div>
<div id="difference-between-independence-and-disjointness" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Difference between independence and disjointness<a href="probs.html#difference-between-independence-and-disjointness" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are four possible scenarios when considering two events:</p>
<p><strong>Example 1: disjoint but not independent</strong></p>
<ul>
<li>Event A: Patient receives treatment A.</li>
<li>Event B: Patient receives treatment B (or is in the control group).</li>
</ul>
<p>These two events, A and B, are disjoint because a patient cannot receive both treatments at the same time.
If a patient receives treatment A, he/she cannot receive treatment B (and vice versa), meaning the events cannot occur together in this setting. Thus, <span class="math inline">\(P(A \cap B) = 0\)</span>.
However, these events are not independent, because the probability of receiving one treatment depends on not receiving the other.
In this setup, if the patient received treatment A, the probability of receiving treatment B is zero: <span class="math inline">\(\mathbb{P}(B|A) = 0\)</span>.
The probability of the patient receiving therapy B could be 50% (if randomized): <span class="math inline">\(\mathbb{P}(B) = 0.5\)</span>. Hence, they are dependent.</p>
<p><strong>Example 2: independent but not disjoint</strong></p>
<ul>
<li>Event A: The patient shows a treatment effect during a study.</li>
<li>Event B: The patient wins the lottery during the study.</li>
</ul>
<p>These two events are independent because the probability of a patient showing a treatment effect is not influenced by whether they win the lottery or not
(at least if we assume that lottery participants do not have different properties compared to non-lottery particiants that are conducive to showing a treatment effect).
Also, the probability of winning the lottery is not influenced by whether the patient shows a treatment effect or not. We would probably see a surge
in volunteers in our studies.
The events are unrelated: one depends on the treatment, while the other is purely a matter of luck.
However, these events are not disjoint because both can happen at the same time.
A patient could experience the treatment effect and also win the lottery during the study. Thus, <span class="math inline">\(P(A \cap B) \neq 0\)</span> , meaning both events can occur together.</p>
<p><strong>Example 3: neither independent nor disjoint</strong></p>
<ul>
<li>Event A: The patient shows a treatment effect during a study.</li>
<li>Event B: The patient is a heavily motivated and self-sufficient.</li>
</ul>
<p>These two events are neither independent nor disjoint. The patient’s motivation could influence the treatment effect (if for instance home exercises
are part of the therapy), making the events dependent.
However, the patient’s motivation is not mutually exclusive with the treatment effect: The patient can be heavily motivated and show a
treatment effect at the same time.
Hence, the events are not disjoint either. They can occur together.</p>
<p><strong>Example 4: independent and disjoint</strong></p>
<p>See <a href="probs.html#exercise4">Exercise 4</a>.</p>
</div>
<div id="Answers_Questions_about_the_1000-researcher_experiment" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Answers to questions about the 1000 researcher-experiment (among many others):<a href="probs.html#Answers_Questions_about_the_1000-researcher_experiment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Maybe, we can already answer some of the <a href="probs.html#Questions_about_the_1000-researcher_experiment">questions from above</a> using what we have learned so far.</p>
<p><strong>For the first question</strong> we would probably bet on the maximum probability of the binomial distribution. The number of positive experiments out of <span class="math inline">\(1000\)</span> has to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1000\)</span>.
Each one of them has <span class="math inline">\(0.04\)</span> probability of happening. With R, we quickly calculate the maximum probability:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="probs.html#cb6-1" tabindex="-1"></a><span class="co"># Calculate the maximum probability using binomial distribution</span></span>
<span id="cb6-2"><a href="probs.html#cb6-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># number of researchers</span></span>
<span id="cb6-3"><a href="probs.html#cb6-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.04</span>  <span class="co"># probability of a treatment effect for each researcher</span></span>
<span id="cb6-4"><a href="probs.html#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="probs.html#cb6-5" tabindex="-1"></a><span class="co"># Calculate the probabilities for each possible number of positive results</span></span>
<span id="cb6-6"><a href="probs.html#cb6-6" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span>n, <span class="at">size =</span> n, <span class="at">prob =</span> p)</span>
<span id="cb6-7"><a href="probs.html#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="probs.html#cb6-8" tabindex="-1"></a><span class="co"># Find the number of experiments with the highest probability</span></span>
<span id="cb6-9"><a href="probs.html#cb6-9" tabindex="-1"></a><span class="co"># index of the maximum probability starting with 1</span></span>
<span id="cb6-10"><a href="probs.html#cb6-10" tabindex="-1"></a>max_prob_number <span class="ot">&lt;-</span> <span class="fu">which.max</span>(probs)</span>
<span id="cb6-11"><a href="probs.html#cb6-11" tabindex="-1"></a></span>
<span id="cb6-12"><a href="probs.html#cb6-12" tabindex="-1"></a><span class="co"># Show the result</span></span>
<span id="cb6-13"><a href="probs.html#cb6-13" tabindex="-1"></a>max_prob_number <span class="sc">-</span> <span class="dv">1</span> <span class="co"># since we started with 0</span></span></code></pre></div>
<pre><code>## [1] 40</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="probs.html#cb8-1" tabindex="-1"></a><span class="co"># 40 is the most likely number of positive results</span></span>
<span id="cb8-2"><a href="probs.html#cb8-2" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">39</span><span class="sc">:</span><span class="dv">41</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)</span></code></pre></div>
<pre><code>## [1] 0.06417798 0.06424483 0.06267788</code></pre>
<p>Now, let’s visualize the binomial distribution for this case using base R syntax:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="probs.html#cb10-1" tabindex="-1"></a><span class="co"># Plot the binomial distribution</span></span>
<span id="cb10-2"><a href="probs.html#cb10-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">0</span><span class="sc">:</span>n, probs, <span class="at">type =</span> <span class="st">&quot;h&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb10-3"><a href="probs.html#cb10-3" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Number of positive results&quot;</span>,</span>
<span id="cb10-4"><a href="probs.html#cb10-4" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>,</span>
<span id="cb10-5"><a href="probs.html#cb10-5" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>),</span>
<span id="cb10-6"><a href="probs.html#cb10-6" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Binomial distribution for treatment effect </span></span>
<span id="cb10-7"><a href="probs.html#cb10-7" tabindex="-1"></a><span class="st">     (yes/no) in 1000 researchers&quot;</span>)</span>
<span id="cb10-8"><a href="probs.html#cb10-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> max_prob_number <span class="sc">-</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Note, this form of distribution looks like a bell curve, aka a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>,
probably the most important distribution in statistics. One can <a href="https://www.m-hikari.com/imf/imf-2017/9-12-2017/p/baguiIMF9-12-2017.pdf">show formally</a> that the binomial distribution converges to the normal distribution under certain conditions.
So, if we would only have one shot to predict the number of researches reporting a treatment effect under the assumption that no treatment exists, we would bet on 40.
This guess would also not be too bad considering the whole range (0 to 1000) since we can expect the number of successes above, let’s say, 65 and below, let’s say, 15 to be very unlikely.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="probs.html#cb11-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">14</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)) <span class="co"># prob of 14 or less</span></span></code></pre></div>
<pre><code>## [1] 1.384829e-06</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="probs.html#cb13-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">66</span><span class="sc">:</span><span class="dv">1000</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)) <span class="co"># prob of 66 or more</span></span></code></pre></div>
<pre><code>## [1] 7.160623e-05</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="probs.html#cb15-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">15</span><span class="sc">:</span><span class="dv">65</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)) <span class="co"># prob of 15 to 65</span></span></code></pre></div>
<pre><code>## [1] 0.999927</code></pre>
<p>We can easily draw from a binomial distribution in R.
We now do the 1000-researcher experiment 10000 times and look at
the histogram of the number of positive results:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="probs.html#cb17-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb17-2"><a href="probs.html#cb17-2" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb17-3"><a href="probs.html#cb17-3" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">10000</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)</span>
<span id="cb17-4"><a href="probs.html#cb17-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">value =</span> data), <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.factor</span>(value))) <span class="sc">+</span></span>
<span id="cb17-5"><a href="probs.html#cb17-5" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">width =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb17-6"><a href="probs.html#cb17-6" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">as.character</span>(<span class="dv">1000</span> <span class="sc">*</span> <span class="fl">0.04</span>),</span>
<span id="cb17-7"><a href="probs.html#cb17-7" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb17-8"><a href="probs.html#cb17-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Bar Plot of 10000 1000-researcher Experiments&quot;</span>,</span>
<span id="cb17-9"><a href="probs.html#cb17-9" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Number of Effects Found&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>) <span class="sc">+</span></span>
<span id="cb17-10"><a href="probs.html#cb17-10" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb17-11"><a href="probs.html#cb17-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>As you can see, the realized number of found effects matches well the theoretical probabilites
given by the binomial distribution. Note, that not necessarily 40 is the most often found number of effects
among 1000 researchers, but we are very close.</p>
<p><strong>The second question</strong> <a href="probs.html#Questions_about_the_1000-researcher_experiment">above</a>
asked about observing 137 positively reporting researchers.
We can calculate the probability of observing 137 or more positive results using the <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>
(plug into the formula):
<span class="math inline">\(\mathbb{P}(\text{observing 137 or more}) = \sum_{i=137}^{1000} \binom{1000}{i}0.04^i (1-0.04)^{1000-i}\)</span>:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="probs.html#cb18-1" tabindex="-1"></a><span class="co"># Calculate the probability of observing 137 or more positive results</span></span>
<span id="cb18-2"><a href="probs.html#cb18-2" tabindex="-1"></a><span class="co"># (using the complement rule)</span></span>
<span id="cb18-3"><a href="probs.html#cb18-3" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">136</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>))</span></code></pre></div>
<pre><code>## [1] 5.551115e-16</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="probs.html#cb20-1" tabindex="-1"></a><span class="co"># Compare to winning the Swiss lottery</span></span>
<span id="cb20-2"><a href="probs.html#cb20-2" tabindex="-1"></a>(<span class="dv">1</span> <span class="sc">/</span> <span class="dv">31474716</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">136</span>, <span class="at">size =</span> <span class="dv">1000</span>, <span class="at">prob =</span> <span class="fl">0.04</span>)))</span></code></pre></div>
<pre><code>## [1] 57234507</code></pre>
<p>57 million times less likely than winning the Swiss lottery. If this event would happen, we would probably reject the assumption that the therapy is not working at all.</p>
<p>In the calculation above, we used the <a href="https://en.wikipedia.org/wiki/Complement_(set_theory)">complement rule</a> to calculate the probability of
observing 137 or more positive results: <span class="math inline">\(1 - \mathbb{P}(\text{observing 136 or less})\)</span>.</p>
<p>In general, for an event <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[\mathbb{P}(A^C) = 1 - \mathbb{P}(A),\]</span></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="probs.html#cb22-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-2"><a href="probs.html#cb22-2" tabindex="-1"></a>plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb22-3"><a href="probs.html#cb22-3" tabindex="-1"></a>  <span class="fu">geom_rect</span>(<span class="fu">aes</span>(<span class="at">xmin =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">xmax =</span> <span class="dv">5</span>, <span class="at">ymin =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">ymax =</span> <span class="dv">5</span>),</span>
<span id="cb22-4"><a href="probs.html#cb22-4" tabindex="-1"></a>            <span class="at">fill =</span> <span class="st">&quot;lightgray&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb22-5"><a href="probs.html#cb22-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb22-6"><a href="probs.html#cb22-6" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">60</span>, <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb22-7"><a href="probs.html#cb22-7" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">label =</span> <span class="st">&quot;A&quot;</span>, <span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb22-8"><a href="probs.html#cb22-8" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="sc">-</span><span class="fl">4.5</span>, <span class="at">y =</span> <span class="fl">4.5</span>, <span class="at">label =</span> <span class="st">&quot;Ω&quot;</span>, <span class="at">size =</span> <span class="dv">6</span>, <span class="at">hjust =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-9"><a href="probs.html#cb22-9" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">y =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">label =</span> <span class="st">&quot;A^C&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb22-10"><a href="probs.html#cb22-10" tabindex="-1"></a>  <span class="fu">coord_fixed</span>() <span class="sc">+</span></span>
<span id="cb22-11"><a href="probs.html#cb22-11" tabindex="-1"></a>  <span class="fu">theme_void</span>()</span>
<span id="cb22-12"><a href="probs.html#cb22-12" tabindex="-1"></a><span class="fu">print</span>(plot)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>where <span class="math inline">\(A^C\)</span> comprises all elementary events that are not in <span class="math inline">\(A\)</span>. In our case, the compliment of observing 136 or less is observing 137 or more and vice versa:
<span class="math inline">\(\mathbb{P}(0, \dots, 136) = 1 - \mathbb{P}((0, \dots, 136)^C) = 1 - \mathbb{P}(137, \dots, 1000)\)</span>.</p>
<p><strong>The third question</strong> <a href="probs.html#Questions_about_the_1000-researcher_experiment">above</a>
asked about the expected number of positive results if the therapy is working in 12% of the patients.
As you can probably guess by now: We would guess <span class="math inline">\(1000 \times 0.12 = 120\)</span> positive results.
This is the so-called <strong><a href="https://en.wikipedia.org/wiki/Expected_value">expected value</a></strong> <span class="math inline">\(\mathbb{E}(X)\)</span> of the <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>.
It is not always the maximum probability (the so-called <a href="https://en.wikipedia.org/wiki/Mode_(statistics)">mode</a>) of the distribution though:
Consider a binomial distribution <span class="math inline">\(\text{Bin}(10, 0.77)\)</span>:</p>
<ul>
<li>The <strong>mean</strong> is <span class="math inline">\(\mathbb{E}(X) = 10 \times 0.77 = 7.7\)</span>. This number is not an integer and we can therefore not calculate the density at this point.</li>
<li>The <strong><a href="https://en.wikipedia.org/wiki/Mode_(statistics)">mode</a></strong> is 8:</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="probs.html#cb23-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb23-2"><a href="probs.html#cb23-2" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb23-3"><a href="probs.html#cb23-3" tabindex="-1"></a>           <span class="at">p_x =</span> <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.77</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb23-4"><a href="probs.html#cb23-4" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>))</span></code></pre></div>
<pre><code>##   x       p_x
## 1 7 0.2343149
## 2 8 0.2941670
## 3 9 0.2188489</code></pre>
<p><strong>The fourth question</strong> <a href="probs.html#Questions_about_the_1000-researcher_experiment">above</a>
asked about the problem of writing a press release stating that 47 out of 50 studies showed a treatment effect.
Well, this would be scientific fraud and a case of <a href="https://en.wikipedia.org/wiki/Survivorship_bias#:~:text=Survivorship%20bias%20or%20survival%20bias,conclusions%20because%20of%20incomplete%20data.">survivorship bias</a>.
You only look at the studies that showed a treatment effect and ignore the ones that did not or you restrict the number of studies to a certain number lower than the true number.
This is also relevant in <a href="https://www.investopedia.com/terms/s/survivorship-bias-risk.asp#:~:text=This%20risk%20arises%20from%20focusing,or%20removed%20from%20performance%20data.">finance</a>.
You may want to read this excellent <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">article</a> by John Ioannidis for a humbling big-picture of how relevant published results can be.</p>
<p><strong>The fifth question</strong> <a href="probs.html#Questions_about_the_1000-researcher_experiment">above</a>
asked about the problem of <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem">multiple testing</a> and related to the previous question.
If you test many hypotheses, you will find some “significant” results by chance alone.
One could also call the practice of testing many hypotheses to find “significant” ones <a href="https://en.wikipedia.org/wiki/Data_dredging"><span class="math inline">\(p\)</span>-hacking</a>.
This should be absolutely avoided. Unfornately, it is still common practice in many fields. Often it happens unconsciously.
Example: If you test 100 hypotheses simultaneously at a significance level of 4%, you would expect 4 “significant” results by chance alone.
If you report those 4 results as legitimate finding, you are p-hacking. When reading a scientific article, watch out for large amounts of <span class="math inline">\(p\)</span>-values and
their (over-)interpretation as “significant” (relevant) or “non-significant” (not relevant).
This <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">article</a> is recommendable to get away from a too strict dichotomous interpretation of research results.</p>
<p><strong>The sixth question</strong> <a href="probs.html#Questions_about_the_1000-researcher_experiment">above</a>
asked about the variation of positive results between large research departments. This demands the very important concept of <a href="https://en.wikipedia.org/wiki/Variance">variance</a>:
The expected quadratic deviation from the mean: <span class="math inline">\(\mathbb{V}ar(X) = \mathbb{E} \{ (\mathbb{E}(X) - X)^2 \}\)</span>. In simple terms:
How much does the number of positive results vary around the mean of 40 on average?
See also <a href="probs.html#exercise5">Exercise 5</a>. Maybe this <a href="https://www.youtube.com/watch?v=SzZ6GpcfoQY">video</a> helps as well.</p>
</div>
<div id="addition_of_probabilities" class="section level3 hasAnchor" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Addition of probabilities<a href="probs.html#addition_of_probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="probs.html#axioms_of_probability_theory">Above</a> in axiom 3, we stated that the probability of the union of pairwise disjoint events is the sum of the
probabilities of the individual events.
What if the events are not disjoint? For simplicity, let’s consider only 2 researchers (doing 2 parallel experiments) and define event <span class="math inline">\(A_1\)</span> as “researcher 1 finds an effect” and
<span class="math inline">\(A_2\)</span> as “researcher 2 finds an effect”.
What is the probability that at least one of the researchers finds an effect?
Our event space
<span class="math inline">\(\Omega = \{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \}.\)</span></p>
<p><span class="math inline">\(\sum_{\omega_i} \mathbb{P}(\omega_i) = 0.04^2 + 0.04 \times 0.96 + 0.96 \times 0.04 + 0.96^2 = 1\)</span></p>
<p><span class="math inline">\(A_1 \cup A_2 = \{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos)\}\)</span></p>
<p><span class="math inline">\(A_1 = \{ (R1pos, R2pos), (R1pos, R2neg)\}\)</span></p>
<p><span class="math inline">\(A_2 = \{ (R1pos, R2pos), (R1neg, R2pos)\}\)</span></p>
<p><span class="math inline">\(\mathbb{P}(A_1) = 0.04^2 + 0.04 \times 0.96\)</span> (First researcher finds an effect or both find an effect)</p>
<p><span class="math inline">\(\mathbb{P}(A_2) = 0.04^2 + 0.96 \times 0.04\)</span> (Second researcher finds an effect or both find an effect)</p>
<p>In, general, we can write the <a href="https://en.wikipedia.org/wiki/Probability#Not_(necessarily)_mutually_exclusive_events">probability of the union of two events</a> as:
<span class="math inline">\(\mathbb{P}(A_1 \cup A_2) = \mathbb{P}(A_1) + \mathbb{P}(A_2) - \mathbb{P}(A_1 \cap A_2)\)</span></p>
<p>Put in the values:
<span class="math inline">\(0.04^2 + 0.04 \times 0.96 +\)</span>
<span class="math inline">\(0.04^2 + 0.96 \times 0.04 - 0.04^2=\)</span>
<span class="math inline">\(0.04^2 + 0.04 \times 0.96 + 0.96 \times 0.04. = 0.0784\)</span>.</p>
<p>Or simpler with the <strong>complement rule</strong>:</p>
<p><span class="math inline">\(\mathbb{P}(A_1 \cup A_2) = 1 - \mathbb{P}(\text{neither }A_1 \text{ nor }A_2) = 1-0.96^2 = 0.0784\)</span>.</p>
<p>See also <a href="probs.html#exercise6">Exercise 6</a>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Here is another helpful depiction of the situation:</p>
<div class="grViz html-widget html-fill-item" id="htmlwidget-d822fcf29241ed269a80" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-d822fcf29241ed269a80">{"x":{"diagram":"\ndigraph {\n  graph [layout = dot, rankdir = TB]\n  \n  # Nodes\n  A [label = \"Researcher 1: Finds effect\", shape = box]\n  B [label = \"Researcher 1: No effect\", shape = box]\n  C [label = \"Researcher 2: Finds effect\", shape = box]\n  D [label = \"Researcher 2: No effect\", shape = box]\n  E [label = \"Researcher 2: Finds effect\", shape = box]\n  F [label = \"Researcher 2: No effect\", shape = box]\n  \n  # Paths\n  A -> C [label = \"Effect found\"]\n  A -> D [label = \"No effect\"]\n  B -> E [label = \"Effect found\"]\n  B -> F [label = \"No effect\"]\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>So, the probability of at least one researcher finding an effect is the sum of the probabilities of the individual researchers finding an effect minus
the probability of both finding an effect, which is the same as that both or exactly one of them finds an effect.</p>
<p>We can also visualize the 4 disjoint elementary events
<span class="math display">\[\Omega = \{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \}\]</span>
in a <a href="https://en.wikipedia.org/wiki/Venn_diagram">Venn diagram</a>. The probabilites of these
4 events in the event space <span class="math inline">\(\Omega\)</span> must add up to 1 since they are disjoint and one of them has to happen. There is no “room” left.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="probabilities_for_health_sciences" class="section level3 hasAnchor" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Probabilities for health science<a href="probs.html#probabilities_for_health_sciences" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have learned a lot so far: The axioms of probability theory, the difference between independence and disjointness, and the addition of probabilities.</p>
<p><strong>How does probability theory fit into the big picture of statistics for health sciences?</strong></p>
<p>In many health-related studies, we want to perform one or more of the following tasks:</p>
<ul>
<li>Estimate proportions (e.g., the proportion of patients with lower back pain. How big is the problem from a public health perspective?),</li>
<li>Test hypotheses (e.g., whether a new therapy is superior to the standard therapy. How sure can we be that the new therapy is better?
What is the probability that the treatment effect is between x and y points on some scale?),</li>
<li>Estimate therapy effects (e.g., the effect of a new therapy on pain reduction:
How many points does the pain decrease? How is the pain reduction distributed?
Are there outliers and why? Are there participants that to not benefit from the therapy?)</li>
</ul>
<p>In all such cases, probability theory is the established tool to answer questions that are afflicted with uncertainty. Would there be no variation in results/effects,
we would probably argue differently. In our world, probability theory is the tool to <strong>quantify uncertainty</strong>.</p>
<p>We can always ask ourselves: <strong>Where is this entity (proportion, effect, etc.) with which frequency/probability</strong>?</p>
</div>
<div id="discrete_vs_continuous_probability_distributions" class="section level3 hasAnchor" number="2.2.8">
<h3><span class="header-section-number">2.2.8</span> Discrete vs. continuous probability distributions<a href="probs.html#discrete_vs_continuous_probability_distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As one of the most prominent examples of a discrete distribution, we have already seen
the <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a> in our 1000-researcher-experiment.
A special case of it is the <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>,
where you only throw the coin once or let one researcher conduct the experiment.</p>
<p>As an example of a continuous distribution we have mentioned the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> above.
It is the most important distribution in statistics for reasons that become increasingly clear as we go along.</p>
<p>One of them is the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a> which we have already mentioned in the
introduction slides. Feel free to watch this <a href="https://www.youtube.com/watch?v=zeJD6dqJ5lo&amp;t=28s&amp;ab_channel=3Blue1Brown">video</a>.</p>
<p>The theorem states that, under appropriate conditions, the distribution of a normalized version of the <strong>sample
mean</strong></p>
<p><span class="math display">\[\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i\]</span></p>
<p><strong>converges to a standard normal distribution</strong>.
By this theorem, we can link <strong>any</strong> distribution to the normal distribution.</p>
<p>Discrete or continuous, the <strong>goal</strong> is the same: We want to now <strong>where</strong> the realization
of my <strong><a href="https://www.youtube.com/watch?v=Y9nsL69CwbU&amp;t=18s&amp;ab_channel=VeryNormal">random variable</a></strong>
lands <strong>with what probability</strong> when I do the experiment.</p>
<ul>
<li><p>How often will I get heads?</p></li>
<li><p>How often will the researcher find an effect?</p></li>
<li><p>With what probability will I get a pain-score reduction of at least 1 point in this patient in front of me given his/her
characteristics and history?</p></li>
<li><p>When looking at ZHAW students, female, soccer lovers; what kind of hourly intense sports activity
can I expect and does that differ from other groups?</p></li>
</ul>
<div id="discrete_probability_distributions" class="section level4 hasAnchor" number="2.2.8.1">
<h4><span class="header-section-number">2.2.8.1</span> Discrete probability distrubtions are used when we can count the outcomes, which includes infinitely many.<a href="probs.html#discrete_probability_distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Some examples of <a href="https://en.wikipedia.org/wiki/Probability_distribution#Discrete_probability_distribution">discrete probability distributions</a> are:</p>
<ul>
<li><strong><a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a></strong>: A single trial with two outcomes (e.g., find an effect or do not find an effect).</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial distribution</a></strong>: The number of successes in a fixed number of trials (e.g., the number of false effects found among 1000 researchers).</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a></strong>: The number of events in a fixed interval of time or space.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric distribution</a></strong>: The number of trials until the first success. This number has no upper limit.</li>
</ul>
<p>We always assign probabilites to the countable outcomes of these distributions, like in the example of the binomial distribution when
we throw the dice 20 times and are interested in the number of 3s:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="probs.html#cb25-1" tabindex="-1"></a><span class="co"># Define parameters for the binomial distribution</span></span>
<span id="cb25-2"><a href="probs.html#cb25-2" tabindex="-1"></a>x_values <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span></span>
<span id="cb25-3"><a href="probs.html#cb25-3" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(x_values, <span class="at">size =</span> <span class="dv">20</span>, <span class="at">prob =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">6</span>)</span>
<span id="cb25-4"><a href="probs.html#cb25-4" tabindex="-1"></a></span>
<span id="cb25-5"><a href="probs.html#cb25-5" tabindex="-1"></a><span class="co"># Plot the binomial distribution with styling</span></span>
<span id="cb25-6"><a href="probs.html#cb25-6" tabindex="-1"></a><span class="fu">plot</span>(x_values, probabilities, <span class="at">type =</span> <span class="st">&quot;h&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb25-7"><a href="probs.html#cb25-7" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>,</span>
<span id="cb25-8"><a href="probs.html#cb25-8" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Binomial Probability Distribution (n = 20, p = 1/6)&quot;</span>)</span>
<span id="cb25-9"><a href="probs.html#cb25-9" tabindex="-1"></a></span>
<span id="cb25-10"><a href="probs.html#cb25-10" tabindex="-1"></a><span class="co"># Add points for clarity</span></span>
<span id="cb25-11"><a href="probs.html#cb25-11" tabindex="-1"></a><span class="fu">points</span>(x_values, probabilities, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Each outcome has a probability <span class="math inline">\(&gt;0\)</span> assigned to it. The sum of all probabilities is 1: <span class="math inline">\(\sum_{i \in \text{Possible outcomes}} \mathbb{P}(X=i) = 1\)</span>.
For every event, we just add the probabilities of the elementary outcomes that are in the event:</p>
<p><span class="math inline">\(\mathbb{P}(X \in (3,8,9,14)) = \mathbb{P}(X = 3) + \mathbb{P}(X = 8) + \mathbb{P}(X = 9) + \mathbb{P}(X = 14)\)</span>.
This principle is true for all discrete probability distributions. Rather simple and elegant:</p>
<p><span class="math display">\[\sum_{i} \mathbb{P}(X = x_i) = 1,\]</span></p>
<p>where <span class="math inline">\(X\)</span> ist the <a href="(https://www.youtube.com/watch?v=Y9nsL69CwbU&amp;t=18s&amp;ab_channel=VeryNormal)">random variable</a> (which takes values <span class="math inline">\(x_i\)</span> when the random experiment is conducted) and <span class="math inline">\(x_i\)</span> are the possible outcomes of <span class="math inline">\(X\)</span>.</p>
<p>We could <strong>invent our own discrete probability distribution</strong> instantly (see also <a href="probs.html#exercise8">Exercise 8</a>),
we’ll call it the <strong>MSc-ZHAW-distribution</strong>:</p>
<p>Let <span class="math inline">\(X \in \mathbb{Z}\)</span>. Every whole number gets the following probability:
<span class="math inline">\(\mathbb{P}(X=0) = 0.1\)</span> and for <span class="math inline">\(x_i \neq 0\)</span>: <span class="math inline">\(\mathbb{P}(X = x_i) = 0.2^{|x_i|}\)</span>.
The sum of all probabilities is: <span class="math inline">\(\sum_{x_i \in \mathbb{Z}} \mathbb{P}(X=x_i) = \mathbb{P}(X=0) + 2 \cdot \sum_{i \in \mathbb{N}} 0.2^i =
0.1 + 2 \cdot \frac{0.2}{1-0.2} = 0.6\)</span>. Hence, we need to divide every probability by 0.6 to get in sum 1.
The final definition is then:</p>
<p><span class="math inline">\(\mathbb{P}(X=0) = \frac{1}{6}\)</span> and for <span class="math inline">\(x_i \neq 0\)</span>: <span class="math inline">\(\mathbb{P}(X = x_i) = \frac{5}{3} 0.2^{|x_i|}\)</span>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="probs.html#cb26-1" tabindex="-1"></a><span class="co"># Define the probability function</span></span>
<span id="cb26-2"><a href="probs.html#cb26-2" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="cf">function</span>(X) {</span>
<span id="cb26-3"><a href="probs.html#cb26-3" tabindex="-1"></a>  <span class="cf">if</span> (X <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb26-4"><a href="probs.html#cb26-4" tabindex="-1"></a>    <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="dv">6</span>)</span>
<span id="cb26-5"><a href="probs.html#cb26-5" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb26-6"><a href="probs.html#cb26-6" tabindex="-1"></a>    <span class="fu">return</span>((<span class="dv">5</span> <span class="sc">/</span> <span class="dv">3</span>) <span class="sc">*</span> (<span class="fl">0.2</span><span class="sc">^</span><span class="fu">abs</span>(X)))</span>
<span id="cb26-7"><a href="probs.html#cb26-7" tabindex="-1"></a>  }</span>
<span id="cb26-8"><a href="probs.html#cb26-8" tabindex="-1"></a>}</span>
<span id="cb26-9"><a href="probs.html#cb26-9" tabindex="-1"></a></span>
<span id="cb26-10"><a href="probs.html#cb26-10" tabindex="-1"></a><span class="co"># Create a sequence of X values from -10 to 10</span></span>
<span id="cb26-11"><a href="probs.html#cb26-11" tabindex="-1"></a>x_values <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb26-12"><a href="probs.html#cb26-12" tabindex="-1"></a></span>
<span id="cb26-13"><a href="probs.html#cb26-13" tabindex="-1"></a><span class="co"># Compute the probabilities for each X value</span></span>
<span id="cb26-14"><a href="probs.html#cb26-14" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">sapply</span>(x_values, P)</span>
<span id="cb26-15"><a href="probs.html#cb26-15" tabindex="-1"></a></span>
<span id="cb26-16"><a href="probs.html#cb26-16" tabindex="-1"></a><span class="co"># Plot the probabilities</span></span>
<span id="cb26-17"><a href="probs.html#cb26-17" tabindex="-1"></a><span class="fu">plot</span>(x_values, probabilities, <span class="at">type =</span> <span class="st">&quot;h&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb26-18"><a href="probs.html#cb26-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>,</span>
<span id="cb26-19"><a href="probs.html#cb26-19" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;MSc-ZHAW Probability Distribution of X from -10 to 10&quot;</span>)</span>
<span id="cb26-20"><a href="probs.html#cb26-20" tabindex="-1"></a></span>
<span id="cb26-21"><a href="probs.html#cb26-21" tabindex="-1"></a><span class="co"># Add points for clarity</span></span>
<span id="cb26-22"><a href="probs.html#cb26-22" tabindex="-1"></a><span class="fu">points</span>(x_values, probabilities, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="probs.html#cb27-1" tabindex="-1"></a><span class="co"># Check if it sums to 1 (approximately):</span></span>
<span id="cb27-2"><a href="probs.html#cb27-2" tabindex="-1"></a>x_values <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1000</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb27-3"><a href="probs.html#cb27-3" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">sapply</span>(x_values, P))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Deviations from zero (<span class="math inline">\(\pm 1\)</span>) are highly likely with this distribution. The probability of <span class="math inline">\(X=0\)</span> is also rather high with <span class="math inline">\(\frac{1}{6}\)</span>.
Larger deviations from zero are less likely and go exponentially towards zero (very fast). So we would expect almost never to see values outside
of <span class="math inline">\(\pm 10\)</span>. This does of course not mean that we will never see them. Do the experiment often enough and you will see them with
probability 1 (see <a href="probs.html#exercise10">Exercise 10</a>).</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="probs.html#cb29-1" tabindex="-1"></a>x_values <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="sc">-</span><span class="dv">1000</span><span class="sc">:</span><span class="dv">1000</span>, <span class="sc">-</span><span class="dv">10</span><span class="sc">:</span><span class="dv">10</span>) <span class="co"># exclude values from -10 to 10</span></span>
<span id="cb29-2"><a href="probs.html#cb29-2" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">sapply</span>(x_values, P))</span></code></pre></div>
<pre><code>## [1] 8.533333e-08</code></pre>
<p><strong>Expectation <span class="math inline">\(\mathbb{E}(X)\)</span> of a discrete random variable</strong>: The expectation of a discrete random variable <span class="math inline">\(X\)</span> is defined as:</p>
<p><span class="math display">\[\mu = \mathbb{E}(X) = \sum x_i \cdot \mathbb{P}(X = x_i),\]</span></p>
<p>a weighted sum of possible values <span class="math inline">\(x_i\)</span> with their respsective probabilities <span class="math inline">\(\mathbb{P}(X = x_i)\)</span>.</p>
<p>The term “expectation” is probably somewhat misleading. It is not necessarily the value we “expect to see” when we do the experiment. For instance,
the expected value of a Bernoulli distribution is: <span class="math inline">\(\mu = \mathbb{E}(X) = 0 \cdot (1-p) + 1 \cdot p = p\)</span>, which could be <span class="math inline">\(0.5\)</span>. The individual outcomes are <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>,
and not <span class="math inline">\(0.5\)</span>. But <span class="math inline">\(0.5\)</span> would be the mean of the outcomes of many experiments.</p>
<p>The expectation can be interpreted as the <a href="https://mathcenter.oxford.emory.edu/site/math117/expectedValueAndCenterOfMass/">center of mass</a>
of the distribution. It is the value that the distribution “balances” around.</p>
<p>Maybe this <a href="https://www.youtube.com/watch?v=KLs_7b7SKi4">video</a> helps too.</p>
<p>The cool thing is that we can learn the true (but unknown) expectation of a distribution by the sample mean. The more samples we collect, the closer we will be.
This is (roughly) the statement of the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>:</p>
<p><span class="math display">\[\bar{X}_n \rightarrow \mu = \mathbb{E}(X) \quad \text{as} \quad n \rightarrow \infty.\]</span></p>
<p>See <a href="https://github.com/jdegenfellner/ZHAW_Teaching/blob/main/Law_of_Large_Numbers_Dice_animation.R">here</a> for an animated example of this law.</p>
<p><strong>Remember</strong>: The sample mean <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\)</span> is a (really good) <strong>estimator</strong> for the expectation <span class="math inline">\(\mu = \mathbb{E}(X)\)</span> of a distribution.
This is true for discrete and continuous distributions.</p>
<p>The <strong>variance of a discrete random variable</strong> is defined as:</p>
<p><span class="math display">\[\mathbb{V}ar(X) = \mathbb{E} \{ (\mathbb{E}(X) - X)^2 \} = \sum_i (\mathbb{E}(X) - x_i)^2 \mathbb{P}(X = x_i),\]</span></p>
<p>the expected squared deviation from the mean. It is a measure of how much the values of the random variable differ from the mean.
<span class="math inline">\((\mathbb{E}(X) - x_i)^2\)</span> quantifies the deviation from the mean. We weight this deviation with the probability of such
a deviation happening. So a large deviation results only in a large variance if it is likely to happen.</p>
<p><strong>Remember</strong>: The sample variance <span class="math inline">\(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\)</span> is a so-called <strong>estimator</strong>
for the variance <span class="math inline">\(\mathbb{V}ar(X)\)</span> of a distribution.
This is true for discrete and continuous distributions.</p>
<p>A more natural interpretation of variability is the <strong>standard deviation</strong>:</p>
<p><span class="math display">\[\sigma = \sqrt{\mathbb{V}ar(X)},\]</span></p>
<p>since it’s on the same scale as X (e.g. <span class="math inline">\(m^2\)</span> or <span class="math inline">\(kg\)</span>).</p>
</div>
<div id="continuous_probability_distributions" class="section level4 hasAnchor" number="2.2.8.2">
<h4><span class="header-section-number">2.2.8.2</span> Continuous probability distributions are used when we cannot count the outcomes.<a href="probs.html#continuous_probability_distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The most famous continuous probability distribution is the
<a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>.
This <a href="https://www.youtube.com/watch?v=k5sbE1_MDwU&amp;ab_channel=VeryNormal">video</a>
about probability distributions in general might be helpful.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="probs.html#cb31-1" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb31-2"><a href="probs.html#cb31-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(pacman)) <span class="fu">install.packages</span>(<span class="st">&quot;pacman&quot;</span>)</span></code></pre></div>
<pre><code>## Loading required package: pacman</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="probs.html#cb33-1" tabindex="-1"></a>pacman<span class="sc">::</span><span class="fu">p_load</span>(ggplot2) <span class="co"># Installs and loads the package at the same time</span></span>
<span id="cb33-2"><a href="probs.html#cb33-2" tabindex="-1"></a></span>
<span id="cb33-3"><a href="probs.html#cb33-3" tabindex="-1"></a><span class="co"># Define parameters for the normal distribution</span></span>
<span id="cb33-4"><a href="probs.html#cb33-4" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">0</span>    <span class="co"># Mean</span></span>
<span id="cb33-5"><a href="probs.html#cb33-5" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># Standard deviation</span></span>
<span id="cb33-6"><a href="probs.html#cb33-6" tabindex="-1"></a></span>
<span id="cb33-7"><a href="probs.html#cb33-7" tabindex="-1"></a><span class="co"># Define the limits for the area to be shaded</span></span>
<span id="cb33-8"><a href="probs.html#cb33-8" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span>  <span class="co"># Lower bound</span></span>
<span id="cb33-9"><a href="probs.html#cb33-9" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span>   <span class="co"># Upper bound</span></span>
<span id="cb33-10"><a href="probs.html#cb33-10" tabindex="-1"></a></span>
<span id="cb33-11"><a href="probs.html#cb33-11" tabindex="-1"></a><span class="co"># Create a sequence of x values to evaluate the PDF</span></span>
<span id="cb33-12"><a href="probs.html#cb33-12" tabindex="-1"></a>x_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(mu <span class="sc">-</span> <span class="dv">4</span> <span class="sc">*</span> sigma, mu <span class="sc">+</span> <span class="dv">4</span> <span class="sc">*</span> sigma, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb33-13"><a href="probs.html#cb33-13" tabindex="-1"></a></span>
<span id="cb33-14"><a href="probs.html#cb33-14" tabindex="-1"></a><span class="co"># Compute the corresponding density values using dnorm</span></span>
<span id="cb33-15"><a href="probs.html#cb33-15" tabindex="-1"></a>y_vals <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x_vals, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb33-16"><a href="probs.html#cb33-16" tabindex="-1"></a></span>
<span id="cb33-17"><a href="probs.html#cb33-17" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb33-18"><a href="probs.html#cb33-18" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_vals, <span class="at">density =</span> y_vals)</span>
<span id="cb33-19"><a href="probs.html#cb33-19" tabindex="-1"></a></span>
<span id="cb33-20"><a href="probs.html#cb33-20" tabindex="-1"></a><span class="co"># Create a subset of the data for shading the area between a and b</span></span>
<span id="cb33-21"><a href="probs.html#cb33-21" tabindex="-1"></a>df_shaded <span class="ot">&lt;-</span> df[df<span class="sc">$</span>x <span class="sc">&gt;=</span> a <span class="sc">&amp;</span> df<span class="sc">$</span>x <span class="sc">&lt;=</span> b, ]</span>
<span id="cb33-22"><a href="probs.html#cb33-22" tabindex="-1"></a></span>
<span id="cb33-23"><a href="probs.html#cb33-23" tabindex="-1"></a><span class="co"># Plot the normal density and shade the area between a and b</span></span>
<span id="cb33-24"><a href="probs.html#cb33-24" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> density)) <span class="sc">+</span></span>
<span id="cb33-25"><a href="probs.html#cb33-25" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span>  <span class="co"># Use linewidth instead of size</span></span>
<span id="cb33-26"><a href="probs.html#cb33-26" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="at">data =</span> df_shaded, <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> density),</span>
<span id="cb33-27"><a href="probs.html#cb33-27" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span>  <span class="co"># Shaded area</span></span>
<span id="cb33-28"><a href="probs.html#cb33-28" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="fu">paste</span>(<span class="st">&quot;Standard Normal Distribution: </span></span>
<span id="cb33-29"><a href="probs.html#cb33-29" tabindex="-1"></a><span class="st">  N(&quot;</span>, mu, <span class="st">&quot;, &quot;</span>, sigma<span class="sc">^</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)) <span class="sc">+</span></span>
<span id="cb33-30"><a href="probs.html#cb33-30" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;X&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-31"><a href="probs.html#cb33-31" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-32"><a href="probs.html#cb33-32" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb33-33"><a href="probs.html#cb33-33" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> mu, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-34"><a href="probs.html#cb33-34" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> mu <span class="sc">+</span> <span class="fl">0.2</span>, <span class="at">y =</span> <span class="fu">max</span>(y_vals) <span class="sc">/</span> <span class="dv">2</span>,</span>
<span id="cb33-35"><a href="probs.html#cb33-35" tabindex="-1"></a>           <span class="at">label =</span> <span class="fu">paste</span>(<span class="st">&quot;E(X) =&quot;</span>, mu), <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-36"><a href="probs.html#cb33-36" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> a, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-37"><a href="probs.html#cb33-37" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> b, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-38"><a href="probs.html#cb33-38" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> a <span class="sc">-</span> <span class="fl">0.2</span>, <span class="at">y =</span> <span class="fu">max</span>(y_vals) <span class="sc">/</span> <span class="dv">4</span>,</span>
<span id="cb33-39"><a href="probs.html#cb33-39" tabindex="-1"></a>           <span class="at">label =</span> <span class="fu">paste</span>(<span class="st">&quot;a =&quot;</span>, a), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-40"><a href="probs.html#cb33-40" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> b <span class="sc">+</span> <span class="fl">0.2</span>, <span class="at">y =</span> <span class="fu">max</span>(y_vals) <span class="sc">/</span> <span class="dv">4</span>,</span>
<span id="cb33-41"><a href="probs.html#cb33-41" tabindex="-1"></a>           <span class="at">label =</span> <span class="fu">paste</span>(<span class="st">&quot;b =&quot;</span>, b), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-42"><a href="probs.html#cb33-42" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Here, like in any other “nice” continuous distribution, the area under the curve is 1:</p>
<p><span class="math display">\[\int_{-\infty}^{\infty} f(x) dx = 1.\]</span></p>
<p>The probability of a single point is zero (<span class="math inline">\(\mathbb{P}(\{ x_i \}) = 0\)</span>).
In any continuous distribution, we use the area under the curve to calculate probabilities. The probability of <span class="math inline">\(X\)</span> being between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the area under the curve (blue shade) between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math inline">\(\mathbb{P}(X \in (a,b))\)</span>. Note that the area over a single point would be zero and therefore the probability of a single point is zero.</p>
<p>The graph above is called a <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (PDF).
Over every point, we express the probability by the height of the curve.
See <a href="descriptive_stats.html#exercise3_descriptive_stats">exercise 3</a> in the next chapter for a practical example for what we will use this in research.</p>
<p><strong>Expectation <span class="math inline">\(\mathbb{E}(X)\)</span> of a continuous random variable</strong>: The expectation of a continuous random variable <span class="math inline">\(X\)</span> is defined as:</p>
<p><span class="math display">\[\mu = \mathbb{E}(X) = \int x \cdot f(x) dx,\]</span></p>
<p>where <span class="math inline">\(x\)</span> are the possible values of <span class="math inline">\(X\)</span> and <span class="math inline">\(f(x)\)</span> is the probability density function of <span class="math inline">\(X\)</span>.</p>
<p><strong>Variance of a continuous random variable</strong>: The variance of a continuous random variable <span class="math inline">\(X\)</span> is defined as:</p>
<p><span class="math display">\[\mathbb{V}ar(X) = \mathbb{E} \{ (\mathbb{E}(X) - X)^2 \} = \int (\mathbb{E}(X) - x)^2 f(x) dx.\]</span></p>
<p>A more natural interpretation of variability is the <strong>standard deviation</strong>:</p>
<p><span class="math display">\[\sigma = \sqrt{\mathbb{V}ar(X)},\]</span></p>
<p>since it’s on the same scale as X.</p>
<p><strong>Example:</strong> Normally distributed Ages of ZHAW students: <span class="math inline">\(\mu = 24\)</span>, <span class="math inline">\(\sigma = 3\)</span>. For the normal distribution,
this means that approx. 68% of the students are between <span class="math inline">\((24-3=)21\)</span> and <span class="math inline">\((24+3=)27\)</span> years old.
You might want to keep <a href="https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Standard_deviation_diagram_micro.svg">this picture</a>
or <a href="https://mathbitsnotebook.com/Algebra2/Statistics/normalstandard.jpg">this one</a> in mind.
So, with just the parameters we have instant information where the values are and where new values
drawn from the same distribution are likely to be.</p>
</div>
</div>
<div id="prominent_probability_distributions_in_health_sciences" class="section level3 hasAnchor" number="2.2.9">
<h3><span class="header-section-number">2.2.9</span> Examples of prominent probability distributions used in health sciences<a href="probs.html#prominent_probability_distributions_in_health_sciences" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first 2 are absolutely essential.</p>
<ul>
<li><p>The most important one is, as mentioned above, the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>.
It is often used to model the distribution of many variables in health sciences, e.g., blood pressure, weight, height, etc.
Normality is also a common assumption in many statistical tests and models. This is the reason why you will find many statements like
“we have checked normality using the Shapiro-Wilk test” (Which I would not recommend) in scientific articles.
Normal distribution theory is very aesthetic and one is sometimes lead to believe that this is the <em>normal</em> state of nature, which is
not the case. See also the <a href="https://en.wikipedia.org/wiki/Normal_distribution#History">history</a> of the normal distribution.
A common use of the normal distribution is in <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>,
where the errors and the conditional distribution of the modeled variable are assumed to be normally distributed.
We will deal with this in QM2.</p></li>
<li><p>The <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a> (<span class="math inline">\(X \sim B(n, p)\)</span>) is used to model the number of successes in a fixed number of trials.
For example, the number of patients that respond to a therapy in a fixed number of patients.
A special case of it is the <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>,
which is used to model a single trial with two outcomes (throw the coin once; <span class="math inline">\(X \sim B(1,p)\)</span>).</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Logistic_distribution">Logistic Distribution</a>. Underpins logistic regression models,
which are used to predict binary outcomes (e.g., the presence or absence of a disease).</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>. Used to model the number of events in a fixed interval of time or space.
For example, the number of patients arriving at an emergency department in a fixed time interval.
Maybe you want to watch this <a href="https://www.youtube.com/watch?v=jmqZG6roVqU&amp;ab_channel=jbstatistics">video</a>.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a>.
For instance used in survival analysis to model the time until an event (e.g. refrigerator stops working) occurs.
This <a href="https://www.youtube.com/watch?v=2kg1O0j1J9c&amp;ab_channel=zedstatistics">video</a> might be interesting.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student’s <span class="math inline">\(t\)</span>-distribution</a> (small “t” please) generalizes
the standard normal distribution. Like the latter, it is symmetric around zero and bell-shaped, but
has <a href="https://en.wikipedia.org/wiki/Fat-tailed_distribution">fatter tails</a> (compared to the normal distribution), i.e.,
“extreme” values are more likely. It is a very well known distribution underlying the <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">t-test</a>.
See <a href="probs.html#exercise12">Exercise 12</a> for a practical example.
This <a href="https://www.youtube.com/watch?v=Uv6nGIgZMVw&amp;ab_channel=jbstatistics">video</a> might be interesting.</p></li>
</ul>
<p>There are infinitely (!) many more <a href="https://en.wikipedia.org/wiki/List_of_probability_distributions">distributions</a>.</p>
<p><strong>Our goal is to learn</strong>: How can we describe (the distribution of) what we see in our data?
How can we make predictions? How can we make decisions based on our data?
Probability theory and statistics are (for us) a very large tool box to answer these questions.
They are unfortunately not magic and cannot turn uncertainty into certainty.</p>
</div>
</div>
<div id="exercises" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Exercises<a href="probs.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Difficulty levels of exercises:
<strong>E</strong>: easy,
<strong>M</strong>: intermediate,
<strong>D</strong>: difficult</p>
<div id="exercise1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> [M] Exercise 1 - Throwing a die very often<a href="probs.html#exercise1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Use your favourite large language model (LLM) to create an R-Script to simulate throwing a fair die 1000 times.</li>
<li>Try to run the script. If it does not run, try to debug it using the LLM.</li>
<li>Once, the script runs, let the LLM explain the code and outputs (“Please explain this script in detail…”).</li>
<li>Plot the frequency of each number (1-6) (after 1000 throws) and compare it to the theoretical probability of getting each number (<span class="math inline">\(\frac{1}{6}\)</span>).</li>
<li>Plot the relative frequency of 3s on the y-axis and the number of throws on the x-axis. This should give a converging pattern towards <span class="math inline">\(y=\frac{1}{6}\)</span>.</li>
<li>Which law of probability theory is illustrated by this simulation?</li>
</ul>
</div>
<div id="exercise2" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> [D] Exercise 2 - Bayes-teaser<a href="probs.html#exercise2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use Bayes’ theorem to calculate the posterior probability of the therapy’s effectiveness in the physiotherapy example above (<a href="probs.html#example1_physio">Example</a>).
For simplicity, let’s just test two <span class="math inline">\(\theta\)</span>-values: 0.3 (as in the previous study) and 0.4. We assign 50% in the prior knowledge that the parameter
<span class="math inline">\(\theta=0.3\)</span>, and 50% to <span class="math inline">\(\theta=0.4\)</span> since we trust our colleagues as well.</p>
</div>
<div id="exercise3" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> [E] Exercise 3 - Find journals<a href="probs.html#exercise3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Note: This is among the most important exercises of the course:</strong>
Use Google or your favourite search engine to find scientific journals in <em>your</em> field (physiotherapy, midwifery, nursing, etc.).
Look at the latest articles. We are interested in articles that used statistics (no qualitative studies).</p>
<ul>
<li>What was the research question? What where they trying to find out/confirm? Write down at least 10 research questions!</li>
<li>Which statistical <em>methods</em> were used? Write down at least 10 methods!</li>
<li>Was prior/external knowledge - before the actual model was estimated - used in any of the analysis?</li>
<li>Where the results presented in a dichotomous way; meaning, was there a “significant”/“non-significant” result or not?</li>
</ul>
</div>
<div id="exercise4" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> [M] Exercise 4 - Independent and disjoint<a href="probs.html#exercise4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Look at the definitions above and try to come up with examples for independent and disjoint events in your field of study.</p>
<ul>
<li>Is this possible?</li>
<li>Why or why not?</li>
<li>What would that imply?</li>
<li>Draw a Venn diagram if possible!</li>
</ul>
</div>
<div id="exercise5" class="section level3 hasAnchor" number="2.3.5">
<h3><span class="header-section-number">2.3.5</span> [M] Exercise 5 - Variance<a href="probs.html#exercise5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Simulate the number of positive results (found an effect even though there is none) in our 1000-researcher-experiment under the assumption that the therapy is not working at all (<span class="math inline">\(p=0.04\)</span>).</li>
<li>Do this experiment in <img src="images/Rlogo.png" height="20px"/> 10,000 times and visualize the results in a histogram.</li>
<li>How often do you get 65 or more positive results? How often do you get 15 or less positive results?</li>
<li>Can you find the limits of a 90% interval around the mean (of 40) - using the so-called <a href="https://en.wikipedia.org/wiki/Quantile">quantiles</a> - for the number of positive results?</li>
<li>What is the theoretical variance for our experiment?</li>
<li>How can you estimate this theoretical (and in reality: unknown) variance from the 10,000 simulations?</li>
</ul>
</div>
<div id="exercise6" class="section level3 hasAnchor" number="2.3.6">
<h3><span class="header-section-number">2.3.6</span> [E] Exercise 6 - Three researchers<a href="probs.html#exercise6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Above in <a href="probs.html#addition_of_probabilities">Addition of probabilites</a> we went through in detail the case of 2 researchers finding an effect.
Let’s now consider 3 researchers simulatenously conducting the experiment.</p>
<ul>
<li>What does the event space <span class="math inline">\(\Omega\)</span> look like?</li>
<li>Which elementary events are in the set of all possible outcomes of our 3-researcher experiment and how many are there?</li>
<li>Draw the corresponding binary tree for this experiment.</li>
<li>Which elementary events are in the following event: “Researcher 3 finds a positive effect”?</li>
<li>Are the events “only researcher 1 finds an effect” and “only researcher 3 finds an effect” disjoint and/or independent?</li>
</ul>
</div>
<div id="exercise7" class="section level3 hasAnchor" number="2.3.7">
<h3><span class="header-section-number">2.3.7</span> [E] Exercise 7 - Conditional probability<a href="probs.html#exercise7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider again the 2 reasearcher situation from above (<a href="probs.html#addition_of_probabilities">Addition of probabilites</a>).</p>
<p><span class="math inline">\(\Omega = \{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \}\)</span>.</p>
<ul>
<li>What is the probability that researcher 1 finds an effect given that researcher 2 found an effect?</li>
</ul>
</div>
<div id="exercise8" class="section level3 hasAnchor" number="2.3.8">
<h3><span class="header-section-number">2.3.8</span> [E] Exercise 8 - Invent a discrete probability distribution<a href="probs.html#exercise8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Invent your own discrete probability distribution.</li>
<li>What is the expected value of your distribution?</li>
<li>What is the variance of your distribution?</li>
<li>Think of something in the real world that could be modeled by your distribution.</li>
</ul>
</div>
<div id="exercise9" class="section level3 hasAnchor" number="2.3.9">
<h3><span class="header-section-number">2.3.9</span> [E] Exercise 9 - Continuous probability distributions<a href="probs.html#exercise9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Invent your own continuous probability distribution.</li>
<li>What is the expected value of your distribution?</li>
<li>What is the variance of your distribution?</li>
<li>Think of something in the real world that could be modeled by your distribution.</li>
</ul>
<p>Hint: You can use simple shapes for the densitiy function defined by lines. And you can use simulation to answer questions about expected value and variance.</p>
</div>
<div id="exercise10" class="section level3 hasAnchor" number="2.3.10">
<h3><span class="header-section-number">2.3.10</span> [M] Exercise 10 - MSc-ZHAW-distribution<a href="probs.html#exercise10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Create sufficiently many random numbers (sample) from the MSc-ZHAW-distribution (see <a href="probs.html#discrete_probability_distributions">above</a>)
and see if you can produce values outside of <span class="math inline">\(\pm 6\)</span>.</li>
<li>What is the mode of this distribution and how could we estimate it from the sample?</li>
<li>What is the <a href="https://en.wikipedia.org/wiki/Interquartile_range">interquantile range</a> of this distribution and how could we estimate it from the sample?</li>
</ul>
</div>
<div id="exercise11" class="section level3 hasAnchor" number="2.3.11">
<h3><span class="header-section-number">2.3.11</span> [M] Exercise 11 - Independence and disjointness for dice events<a href="probs.html#exercise11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Find examples of dice events when throwing a die once that are:</p>
<ul>
<li>Not independent and not disjoint.</li>
<li>Not independent but disjoint.</li>
<li>Independent but not disjoint.</li>
</ul>
</div>
<div id="exercise12" class="section level3 hasAnchor" number="2.3.12">
<h3><span class="header-section-number">2.3.12</span> [D] Exercise 12 - Student’s <span class="math inline">\(t\)</span>-distribution<a href="probs.html#exercise12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s look at a <a href="https://www.sciencedirect.com/science/article/pii/S0954611112002958#aep-abstract-id3">paper</a>,
where the <span class="math inline">\(t\)</span>-distribution is used (in the background). The aim of the study was to assess the efficacy of pulmonary
rehabilitation in addition to regular chest physiotherapy in non cystic fibrosis bronchiectasis.
Table 1 describes the patient characteristics in both groups.
Table 2 shows the primary endpoint (incremental shuttle walk test - ISWT) at baseline and follow-up time points.
<a href="https://ars.els-cdn.com/content/image/1-s2.0-S0954611112002958-gr2_lrg.jpg">Figure 2</a> shows the outcomes
at baseline, 8 weeks and 20 weeks for both groups.
They want to find out if the ISWT is different between the two groups.
(Note, that an arbitrary threshold for the <span class="math inline">\(p\)</span>-value of 0.05 is used to decide if the groups
are “significantly” different. One should <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">avoid</a> these formulations.
There is no reason not to use a different threshold (like 4.3%).)
<strong>The standardized difference of the group means is <span class="math inline">\(t\)</span>-distributed</strong>. This case is a bit more complex than the simple ones,
since we have different sample sizes (15 vs. 12) and different variances in the groups. The statistics software will take care of this
and use the so-called <a href="https://en.wikipedia.org/wiki/Welch%27s_t-test">Welch’s <span class="math inline">\(t\)</span>-test</a>.</p>
<ul>
<li>What do you think about the baseline values for ISWT in the two groups?</li>
<li>What is the number in brackets next to the ISWT-values?</li>
<li>According to the article, the data is normally distributed.
Draw 3 probability density functions of normal distributions in one graph with the respective parameters
for baseline, 8 weeks and 20 weeks for both groups. Make two graphs, one for each group.</li>
<li>According to the text, <a href="https://ars.els-cdn.com/content/image/1-s2.0-S0954611112002958-gr2_lrg.jpg">Figure 2</a> shows the means
<span class="math inline">\(\pm\)</span> standard errors (<span class="math inline">\(SE = \frac{s}{\sqrt{n}}\)</span>) of the ISWT at baseline, 8 weeks and 20 weeks for both groups. Look at Figure 2, a.
Does this match the description for instance at 8 weeks in the acappella+pulmonary group? Do the bars make sense?</li>
<li>Now, let’s simulate the differences at week 8 (ISWT) using the parameters given: Group sizes, 15 and 12,
means (<span class="math inline">\(338.7\)</span> and <span class="math inline">\(344.2\)</span>) and standard deviations (<span class="math inline">\(42.2\)</span> and <span class="math inline">\(115.5\)</span>).
Draw a histogram of the simulated differences.
Calculate the 1.5% and 98.5% quantiles of the differences.</li>
</ul>
</div>
</div>
<div id="solutions" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Solutions<a href="probs.html#solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Solutions for this chapter can be found <a href="https://github.com/jdegenfellner/Script_QM1_ZHAW/tree/main/Solutions_to_Exercises/2_Probability">here</a>.</p>
</div>
<div id="sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Sample exam questions for this chapter (in German since exam is in German)<a href="probs.html#sample-exam-questions-for-this-chapter-in-german-since-exam-is-in-german" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this section, no solutions are provided.</p>
<div id="question-1---independence-and-disjointness" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Question 1 - Independence and disjointness<a href="probs.html#question-1---independence-and-disjointness" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Wir werfen einen fairen Würfel einmal und betrachten die Augenzahl. Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)?</p>
<ul>
<li><p>Die Wahrscheinlichkeit, eine gerade Zahl zu werfen, ist <strong>unabhängig</strong> von der Wahrscheinlichkeit, eine Zahl größer als 3 zu werfen.</p></li>
<li><p>Die Ereignisse “gerade Zahl” und “ungerade Zahl” sind <strong>disjunkt</strong>.</p></li>
<li><p>Die Ereignisse “Zahl größer als 3” und “Zahl kleiner als 4” sind <strong>unabhängig</strong>.</p></li>
<li><p>Die Wahrscheinlichkeit, eine Zahl größer als 3 zu werfen, ist <strong>unabhängig</strong> (independent) <strong>und</strong>
<strong>disjunkt</strong> (disjoint) von der Wahrscheinlichkeit, eine Zahl kleiner als 4 zu werfen.</p></li>
</ul>
</div>
<div id="frage-2---bedingte-wahrscheinlichkeit" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Frage 2 - Bedingte Wahrscheinlichkeit<a href="probs.html#frage-2---bedingte-wahrscheinlichkeit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ein medizinischer Test wird verwendet, um eine bestimmte Krankheit zu erkennen. Der Test hat folgende Eigenschaften:<br />
- <strong>Sensitivität</strong>: 95%<br />
- <strong>Spezifität</strong>: 90%<br />
- Die Prävalenz der Krankheit in der Bevölkerung beträgt <strong>2%</strong>.</p>
<p>Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)?</p>
<ul>
<li>Die Wahrscheinlichkeit, dass der Test positiv ist, wenn die Person die Krankheit hat, beträgt 95%.</li>
<li>Die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person die Krankheit nicht hat, beträgt 90%.</li>
<li>Die Wahrscheinlichkeit, dass der Test positiv ist, wenn die Person die Krankheit nicht hat, beträgt 10%.</li>
<li>Die Wahrscheinlichkeit, dass die Person die Krankheit hat, wenn der Test positiv ist, beträgt <span class="math inline">\(0.1623932\)</span>.</li>
</ul>
</div>
<div id="frage-3---erwartungswert-und-varianz" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Frage 3 - Erwartungswert und Varianz<a href="probs.html#frage-3---erwartungswert-und-varianz" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Die diskrete Wahrscheinlichkeitsverteilung einer Zufallsvariablen <span class="math inline">\(X\)</span> ist gegeben durch:
<span class="math display">\[
\begin{array}{|c|c|}
\hline
x &amp; P(X = x) \\
\hline
1 &amp; 0.2 \\
2 &amp; 0.3 \\
3 &amp; 0.4 \\
4 &amp; 0.1 \\
\hline
\end{array}
\]</span></p>
<p>Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)?</p>
<ul>
<li>Der Erwartungswert von <span class="math inline">\(X\)</span> beträgt 2.6.</li>
<li>Die Varianz von <span class="math inline">\(X\)</span> beträgt 1.3.</li>
<li>In einer Wette, welche Ausprägung von <span class="math inline">\(X\)</span> beim nächsten Ziehen kommt,
würde man hier auf den Erwartungswert <span class="math inline">\(\mathbb{E}(X)\)</span> tippen.</li>
<li>Die Wahrscheinlichkeit für das Ereignis, dass <span class="math inline">\(X\)</span> eine gerade Zahl ist, ist 50%.</li>
</ul>
</div>
<div id="frage-4---dichtefunktion" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Frage 4 - Dichtefunktion<a href="probs.html#frage-4---dichtefunktion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Es sei folgende Dichtefunktion der stetigen Zufallsvariablen <span class="math inline">\(X\)</span> gegeben:
<img src="_main_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)?</p>
<ul>
<li>Die Fläche unter der Dichtefunktion beträgt 1, falls die Spitze des Dreiecks bei Höhe <span class="math inline">\(y=2\)</span> liegt.</li>
<li>Modus, Median und Erwartungswert sind identisch.</li>
<li><span class="math inline">\(\mathbb{P}(X \in (0,0.25)) = \frac{1}{8}\)</span>.</li>
<li><span class="math inline">\(\mathbb{P}(X = 0.5) = 0\)</span>.</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="descriptive_stats.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jdegenfellner/Script_QM1_ZHAW/edit/main/02-probs.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"favicon": "favicon.ico"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
