[["index.html", "Quantitative Methods 1, ZHAW Chapter 1 Introduction 1.1 Books I can (highly) recommend: 1.2 1.3 Additional Tools 1.4 Workflow suggestion 1.5 Orientation for the course and script 1.6 Warning of incompleteness", " Quantitative Methods 1, ZHAW Jürgen Degenfellner 2024-12-12 Chapter 1 Introduction This script is a collection of notes and exercises for the course “Quantitative Methods 1” (Codename for: Statistical foundations) at ZHAW in Winterthur, Switzerland. It is permanently evolving and the github repository is public. The vision is to write this script in collaboration with students. If you find any errors, have suggestions, or want to contribute, feel free to contact me. Which (online) content (video, book, blog…) helped you understand the topic at hand better? We should link those resources in the script! The goal of this script is to provide a starting point for further reading and learning. It should function as an initial start to get you going. We are all learners. Feel free to use any content for your own purposes. At the end of each chapter, you will find a list of exercises. Last year’s exam will be uploaded in time in the respective Moodle folder to help you prepare. The most important lessons you’ll learn are not part of the final exam: Intellectual honesty and humility. A lot of content relevant for this course can be found in the ZHAW_teaching folder, which contains many R scripts (among other stuff). We will focus a lot on descriptive statistics and basic concepts as it is a very important part of understanding data. Statistical modeling will be covered later. One thing to consider for health sciences with respect to quantitative methods is the following: We do not have to use quantitative methods for each and every question arising in applied research (physiotherapy, midwifery, nursing, occupational therapy). Small sample sizes (e.g., n=10) often do not warrant the use of inferential statistics or estimation or at least make it considerably harder to answer the question at hand. If one decides to answer questions in a statistical way, we are bound to the rules of the game. GPT4o and Github Copilot where used for writing this script. 1.1 Books I can (highly) recommend: (Free, looks nice at first glance) Introduction to Probability Statistical Rethinking, YouTube-Playlist: Statistical Rethinking 2023 (Free) Understanding Regression Analysis: A Conditional Distribution Approach Data Analysis Using Regression and Multilevel/Hierarchical Models (Free) Doing Bayesian Data Analysis Applied Regression Analysis and Generalized Linear Models These books are well-written, approachable, and not overly technical. For more theoretically advanced approaches, I can recommend: (Free) Bayesian Data Analysis (Free) Elements of Statistical Learning (Free) Unterstanding Advanced Statistical Methods 1.2 In this course, we use R as our main tool for data analysis. R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. We are following a two-tier approach to learning R. The first tier jumps right into solving exercises and problems with R using GPT. This anticipates the capacities of the software without knowing the details yet. It is not expected that you understand every detail of the codes GPT might suggest. This comes with time. The second tier is a more detailed approach to R, where we learn the basics of the language and how to use it effectively. This part will be done in extensive recorded eLearning sessions over the course. You may want to give Hadley Wickham’s book a try: (Free) R for Data Science Especially, the chapter on exploratory data analysis is very helpful. This free book seems also very helpful for beginners: (Free) R for non-programmers R4NP Further resources: (Example) Introduction to R In this video, many basic commands are explained. There are many more R introductions. My tip: Watch the beginning of a few different ones and see which explanations work best for you individually. Overview of introductory resources 1.3 Additional Tools Currently I use a combination of Github Copilot (paid), GPT4o (paid), and RStudio for writing code and this script. You can use Github Copilot directly in RStudio to make code suggestions, which is often increasing productivity. I can highly recommend using these tools in combination. GPT4o can process images as well, which is a game changer. As far as I know, there is no self-debugging version of GPT4o in combination with RStudio available yet (in the style of AutoGPT or similar approaches). Maybe, we’ll work on that soon as a side project. For the more technically inclined among you, you can also use VSCode with the Github Copilot extension to write your R-Code. Large Language Models (LLM) like GPT4o enable you to write/adapt code using natural language. Among other tasks, they help you create complicated, aesthetic plots. Very often, debugging attempts get stuck. They are far from perfect yet, but an impressive feat of engineering. 1.4 Workflow suggestion You could use RStudio to write and execute your code. In the global options of RStudio (below), you can add your github copilot account. Additionally, you can have your GPT4o(1) window open and copy-paste images of your plots or code in order to achieve your current objective. 1.5 Orientation for the course and script Throughout the script it is important to keep a mental mind map of the topics we are covering and why. Typically, we start with a question like “Is this therapy more effective than usual care”, then we collect data, summarize it, and then we try to answer the question. Often, we already have the data (a so-called convenience sample) and we try to answer a question with it (exploratively). By answering the question, we make an inference about the population respectively the process generating the data. So we want to conclude something about the population from the sample in front of us. When we only describe the data to better understand it, we are in the realm of descriptive statistics. When we try to make an inference about the population, we are in the realm of inferential statistics. Simply put, we want to find out if, for example, the treatment effect we are seeing in the sample is also present in the population or if what we are seeing was just by chance. We also add measures of uncertainty (for instance credible or confidence intervals) to our estimates. We can do this inference in two ways: Frequentist approach using null hypothesis significance testing (NHST), or by using the Bayesian approach (Bayesian inference). 1.6 Warning of incompleteness This script is not static but a continuously evolving document. It represents a snapshot of our understanding at the time of writing. Many of the topics touched upon or mentioned briefly here are vast research fields in their own right, and it is beyond the scope of this script to cover them comprehensively. Our goal, however, is to create a resource that aligns closely with the needs of health sciences and reflects our needs in applied research. While it may not be exhaustive, we aim for it to be both practical and relevant, serving as a tailored guide for our specific context. "],["probs.html", "Chapter 2 Probability 2.1 Frequentist vs. Bayesian statistics 2.2 Foundations of probability theory 2.3 Exercises 2.4 Solutions 2.5 Sample exam questions for this chapter (in German since exam is in German)", " Chapter 2 Probability Probability is a measure of the likelihood that an event will occur. Probability is quantified as a number between 0 and 1 (or 0 to 100%), where 0 indicates impossibility and 1 indicates certainty, although we will see later that a probability of 0 does not necessarily mean that such an event can never occur. The higher the probability of an event, the more likely it is that the event will occur. Why is probability important in our field of study (applied health sciences)? Quantative research methods (often a code name for statistics) use probability theory to make statements about a larger population or a data generating process (DGP), as it should be more appropriately called. In observational studies, we often make statements about associations between variables. In experimental studies (e.g., a randomized controlled trial), we often try to make statements about the effect of an intervention on a certain outcome - for instance if a therapy lowers pain by at least 1 point better compared to usual therapies. Probability theory has its roots in gambling and betting. Blaise Pascal wrote a letter to Pierre de Fermat in 1654 when a French essayist Antoine Gombaud, intrigued by gambling, sought to solve “the problem of points,” first posed by Luca Paccioli in 1494. The problem asked how to fairly divide the winnings if a game is interrupted before its conclusion. Gombaud approached mathematician Blaise Pascal, who collaborated with Pierre de Fermat. Together, they laid the groundwork for modern probability theory. Fermat’s method involved listing all possible outcomes and calculating each player’s chance of winning, while Pascal developed a backward induction algorithm to assign probabilities. Their work revolutionized mathematics and influenced fields like economics and actuarial science. Philosophically speaking, we could distinguish between two flavors of probability: Probabilities for events that are repeatable respectively have already happened, and probabilities for events that haven’t happened yet. An example for a repeatable event is getting a 6 when throwing a fair die. We can do this experiment right now by fetching a die and throwing it. An example for the latter is the probability of a patient dying within the next 5 years after a certain diagnosis. It is hard to argue that this experiment would be repeatable under (almost) identical conditions since every patient is different whereas the dice are typically much more similar. Here, we could at least put forward that other similar patients have a certain proportion of dying within 5 years. There are of course events that have not happend ever before, like the creation of artificial general intelligence (AGI). Nevertheless, one can still try to assign probabilities when such an event would happen. 2.1 Frequentist vs. Bayesian statistics There are two main schools of thought in statistics: Frequentist and Bayesian. Often one hears that there is a “war” between the two. It is not our place to say which one is better. Both have their strengths and weaknesses and are used in different contexts. I would consider the rapant misuse of \\(p\\)-values and the cookbook-like application of frequentist statistics as a weakness of this approach (in its widely used form at least). Of course, this is not the method’s fault but the fault of the user. Bayesian statistics is often considered more intuitive and flexible. It is also more computationally demanding and requires prior knowledge which is argued to be subjective. Computation time is sometimes still an issue in comparison for instance in regression modelling when using an end user laptop. It is also argued that for large sample size frequentist and Bayesian statistics converge to the same result. There are very smart proponents on both sides and we will try to use and contrast both techniques throughout this script whenever convenient. Especially one of the early eminent statisticians, Ronald Fisher, was an oponent of Bayesian statistics, or as he called it: “inverse probability”. The only thing we are interested in is the practical application of both methods in the field of applied health sciences. How well can we describe data and make predictions, how well can we learn from data in our field? 2.1.1 Frequentist statistics Frequentist statistics is based on the idea that probability is the long-run frequency of events. For instance, if I throw a fair die 1000 times, the frequency of getting a 3 is (approximately) \\(\\frac{1}{6}\\). In the limit, if I throw the die infinitely many times, the frequency of getting a 3 will converge to \\(\\frac{1}{6}\\). In mathematical notation, we would write \\[ \\mathbb{P}(\\text{getting a 3}) = \\lim_{n \\to \\infty} \\frac{\\text{Number of 3s in } n \\text{ throws}}{n} = \\frac{1}{6}, \\] where \\(\\mathbb{P}\\) is the probability measure which we will define more formally later (see Exercise 1). More genereally, in frequentist statistics, we are looking for a fixed but unknown parameter from an underlying data generating process (DGP). In the dice example, the process of repeatedly throwing the die is the data generating process. Basically, we could estimate the parameter of interest arbitrarily well by reapeated drawing from the DGP if we had enough data. Example: Throw your (fair or unfair) die often enough and you will get a good estimate of the probability of getting a 3. Example: We could try to estimate the mean birth weight of all babies from smoking parents born in Switzerland in 2022. We would draw a (random) sample of birthweights and calculate the mean. With a sample large enough, we could estimate this parameter fairly well. With all birthweights, we would know the true mean of the population of interest (for that year alone). 2.1.2 Bayesian statistics Bayesian statistics, on the other hand, is based on the idea that probability is a measure of our uncertainty about an event or a parameter. Here, we use prior (i.e., before/outside of our experiment) knowledge about a parameter and update this knowledge with new data using the famous Bayes’ theorem: \\[ p(\\theta | \\text{data}) = \\frac{p(\\text{data} | \\theta) \\cdot p(\\theta)}{p(\\text{data})}, \\] where: \\(p(\\theta | \\text{data})\\) is the posterior probability: the updated probability of the parameter \\(\\theta\\) given the observed data. \\(p(\\text{data} | \\theta)\\) is the likelihood: the probability of observing the data given a certain value of the parameter \\(\\theta\\). \\(p(\\theta)\\) is the prior probability: the initial belief about the parameter \\(\\theta\\) before seeing the data. \\(p(\\text{data})\\) is the marginal likelihood or evidence: the probability of observing the data under all possible parameter values. 2.1.2.1 Example in applied health sciences (physiotherapy) Suppose you’re a physiotherapist trying to estimate the probability that a new therapy improves the mobility of patients with chronic back pain (Improvement Yes/No). You already have some prior knowledge (based on previous studies or expert opinions) that suggests the therapy works for 30% of patients. This is your prior knowledge: \\(\\theta = 0.30\\), where \\(\\theta\\) is the probability that the therapy is effective. Your colleagues are not convinced and argue that the probability is 40%. Now, you run a small trial with 50 patients and observe that 22 of them showed a clinically relevant improvement in mobility (self-reported from the patient). This new data (the result of the trial) updates your belief about the effectiveness of the therapy. Using Bayes’ theorem (Exercise 2), you combine the prior knowledge \\(\\theta = 0.30\\) with the likelihood of the new data \\(p(\\text{data} | \\theta)\\), and you calculate the posterior probability, \\(p(\\theta | \\text{data})\\), which reflects your updated belief about the effectiveness of the therapy after observing the trial data. We could assign the probability of \\(\\theta = 0.3\\) or \\(\\theta = 0.4\\) equally: \\(p(\\theta = 0.3) = p(\\theta = 0.4) = 0.5.\\) Bayesian analysis allows you to update your estimates as new evidence becomes available, providing a flexible framework for decision-making in health sciences. 2.2 Foundations of probability theory We need to know some basic concepts of probability theory in order to dive in deeper. We will try to introduce them playfully and find formality as we go along. As stated above, in the frequentist sense, we are interested in the long-run frequency of events. How often does an event occur if we repeat the random experiment many times? Eureka? Let’s imagine we are in a research department with 1000 researchers all trying to answer the same question: Does the new physiotherapy work (e.g., reduce pain by 1 point better than the usual treatment)? Let’s assume (unrealistically) that they are all working on this one question and they are not talking about their experiments or their research methodology to each other (assumption of independence). The statistician in the department has calculated (due to the variability of such treatment effects in the relevant population and theoretical considerations) that even under the assumption of the therapy is not working at all - which we will assume for the time being - , one would see an effect just by chance in 4% of the study results. What would be considered a discovery under these cicumstances? We now conduct an experiment. All 1000 researchers are conducting a study with 50 patients to answer the same question. This is our random experiment (instead of throwing dice). Instead of throwing a fair die, we do a round of “research” with 1000 researchers. You as observer give the assignment to the researchers and come back as soon as all 1000 researeches have finished their experiments. Again, the are not taking to each other and we can (unrealistically) assume that their results will be not influenced by each other. Now we could ask different questions: 2.2.1 Questions about the 1000 researcher-experiment (among many others): If you had to bet, how many experiments showed a treatment effect if you assume that the therapy is not working at all? If you get 137 results showing a treatment effect, would you be surprised? Would you reject the assumption, that the therapy is not working at all? Why? How many experiments (would you expect) showed a treatment effect if you assume that the therapy is “working” (positive result by chance) in 12% (instead of 4%) of the patients? Assuming that you have 47 results showing a treatment effect and your marketing lead is asking you to write a press release stating that 47 out of 50 studies showed a treatment effect. What is the problem? Assuming one very motivated researcher has tested 65 (secondary) hypotheses in her experiments and found 4 results that are difficult to explain by chance alone. What is the problem? Suppose there are many large research departments in the world with 1000 researchers. How strongly would the number of positive results vary between these large departments? We will try to answer these questions below. First, it seems intuitive that Probability within an experiment should add up if the events are disjoint. The event \\(A_1=\\) “only researcher 45 gets a positive result” and \\(A_2=\\) “only researcher 897 gets a positive result” are mutually exclusive. If only researcher 45 finds an effect, then researcher 897 does not find an effect and vice versa. They cannot happen at the same time within that one experiment. Hence, the two events are said to be disjoint. If we add up the probabilities of all mutually exclusive events, we should get 1, or 100%. We say that the probability of all elementary events (called \\(\\omega\\)) sums to 1. Let’s look at a Venn diagram to illustrate the concept of being mutually exclusive (disjoint). Again, this refers to being mutually exclusive within our 1000-researcher experiment. Both events cannot happen at the same time in this context, so we assign \\(0\\) to the event that both occur simultaneously: \\(\\mathbb{P}(A_1 \\cap A_2)=0\\). The \\(\\cap\\)-Symbol refers to all elementary events that are in both sets. In our case we have the sets \\[A_1 = \\{ (\\dots ,R_{45} = pos, \\dots ,R_{897} = neg, \\dots) \\}\\] \\[and\\] \\[A_2 = \\{ (\\dots ,R_{45} = neg, \\dots , R_{897} = pos, \\dots) \\}.\\] An example of non-disjoint events (within our 1000-researcher experiment) would be the event \\(A_1=\\) “only researcher 45 gets a positive result” and the event \\(A_3=\\) “only researcher 45 or only reasearcher 67 gets a positive result”. Which researchers got a positive result in both events? The answer is: Researcher 45. Hence, the two events are said to be non-mutually exclusive. We can’t just add up the probabilities (of events \\(A_1\\) and \\(A_3\\)) here, since we would count the probability of researcher 45 twice. The sets look like this: \\[A_1 = \\{ (\\dots ,R_{45} = pos, \\dots) \\}\\] \\[and\\] \\[A_3 = \\{ (\\dots ,R_{45} = pos, \\dots), (\\dots ,R_{67} = pos, \\dots) \\}.\\] How many elementary events are in the set of all possible outcomes of our 1000-researcher experiment? For every researcher, there are two possible outcomes: positive or negative result. Hence, we have \\(2 \\cdot 2 \\cdot 2 \\cdots = 2^{1000}\\) elementary events in our set of all possible outcomes. This is a very large number (\\(\\sim 10^{300}\\)) - more than there are particles in the universe (\\(\\sim 10^{80}\\)). We call the set of all elementary events \\(\\Omega\\) (the Greek letter Omega): \\[\\Omega = \\{ \\omega_1, \\omega_2, \\cdots, \\omega_{2^{1000}} \\}.\\] Note, that we collect elementary events to form events like we just did for event \\(A_3\\). Note, that the \\(2^{1000}\\) elementary events in the 1000 researcher experiment are also disjoint. Why? For every elementary event, certain researchers found something and others did not. The combinations are all different from each other. Hence, all the elementary events cannot happen at the same time within that one experiment. All of them are disjoint. The probability of the event “” (nothing occurred) should be zero (\\(\\mathbb{P}(\\emptyset)=0\\)), were “” denotes the event that no researcher gets a positive or negative result ( = \\(\\emptyset\\), the so-called empty set). This is impossible due to the design of the experiment. We would therefore define this probability as zero and (if we can count the number of different outcomes) this event can indeed never happen. Obviously, the probability of an event should at a mininum be zero and at a maximum be one: \\[0 \\le \\mathbb{P}(A) \\le 1.\\] 2.2.2 Axioms of probability theory We can summarize these informally stated properties more formally (Kolmogorov’s axioms): \\[\\begin{align} 1. &amp;\\ \\mathbb{P}(\\emptyset) = 0 \\text{: Probability of the &quot;impossible&quot; event should be zero.}\\\\ 2. &amp;\\ \\mathbb{P}(\\Omega) = 1 \\text{: Probability, that any outcome occurs in our random experiment.}\\\\ 3. &amp;\\ \\text{If } A_1, A_2,... \\text{ pairwise disjoint: } \\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mathbb{P}(A_i) \\end{align}\\] The \\(\\infty\\)-symbol in axiom 3 comes into play if we are dealing with (potentially) infinitely many events. For instance, we could ask for the number of researchers we need to look at until we see the first positive result (geometric distribution). We could find the first positive result in the first researcher, or the second, etc. There is no upper limit. As concrete example for law 3 in our example, we can put the following: \\[\\begin{align} \\scriptsize \\mathbb{P}(\\text{&quot;(only) researchers 34, 56 and 777 get a pos. result&quot; or &quot;(only) researchers 1 and 5 get a pos. results&quot;}) =\\\\ \\scriptsize \\mathbb{P}(\\text{&quot;(only) researchers 34, 56 and 777 get a pos. result&quot;}) + \\mathbb{P}(\\text{&quot;(only) researchers 1 and 5 get a pos. results&quot;}) \\end{align}\\] Since the researchers are working independently from each other, we can simply multiply the probabilities of their individual positive or negative results in our larger 1000-researcher experiment. For example, for the first probability there are exactly 3 positive results (=effect found) and 997 negative results (=no effect found). This can be calculated as: \\(0.04 \\cdot 0.04 \\cdot 0.04 \\cdot \\underbrace{0.96 \\cdots 0.96}_{\\text{997 times}} = 0.04^3 \\cdot 0.96^{997}\\), which yields a very small number (\\(1.350826 \\cdot 10^{-22}\\)) since we are fixating on specific researchers to find the effect. If we relax the question to the number of researchers that find an effect, we get much larger numbers. We say, the number \\(X\\) of positive results under \\(H_0\\) (there is no true effect)) for a positive effect is binomially distributed: \\(X \\sim Bin(n=1000, p=0.04)\\). The YouTube-channel 3Blue1Brown is highly recommended in general. You should watch this video on the binomial distribution to get a clearer picture. This video from KhanAcademy could also be helpful. In our example, the probability that exactly 3 researchers find an effect is \\(\\binom{1000}{3} \\cdot 0.04^3 \\cdot 0.96^{997} = 2.244627 \\cdot 10^{-14}\\). Still small, but much higher than before. Of course, the commands in R can be found easily via Google or your favourite large language model (LLM): “Give me the commands for the binomial distribution in R and a nice example too”. Note that the sum of all elementary events (all possible outcomes) indeed adds up to 1 in our 1000-researcher-experiment: \\(\\sum_{i=0}^{1000} \\binom{1000}{i}0.04^i 0.96^{1000-i} = 1\\) sum(dbinom(0:1000, prob = 0.04, size = 1000)) ## [1] 1 sum(dbinom(0:80, prob = 0.04, size = 1000)) ## [1] 1 # = 1 since the other probabilities are very small As we will see later, axiom 1 above does not mean, that the event can never occur. For every continuous random variable (e.g. with a normal or a uniform distribution), the probability of a single point is zero. This video could help. Axiom 2 is always true. Some result has to occur in our random experiment. What is \\(\\Omega\\) again? In our countable case of researchers, \\(\\Omega = \\{ \\omega_1, \\omega_2, \\cdots, \\omega_{2^{1000}} \\}\\) would be the set of all possible outcomes if we let 1000 researchers conduct the experiment. Each researcher can either find an effect or not. Hence, we have \\(2^{1000}\\) possible outcomes of our 1000-researcher experiment. This is a very large number. Adding up all these probabilities would sum to 1 according to axiom 3. Combining different elementary events \\(\\omega\\) from the whole collection of possible outcomes \\(\\Omega\\) gives us “events” like the ones we used above (\\(A_1, A_2, A_3\\)). Note that there is a difference between the elementary experiment of the individual researcher (finding an effect or not) and the whole experiment of 1000 researchers we are looking at (simultaneously). Do not make the mistake to add the single probabilities of finding an effect (under \\(H_0\\)) of 0.04 to get the probability of finding an effect in the whole experiment: This would result in: \\(1000 \\cdot 0.04 = 40 &gt; 1\\), which is hardly a probability anymore. This leads us to the concept of independence of events. 2.2.3 Independence of events Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the occurrence of the other event. In plain English, the probability of event \\(A\\) happening is the same whether event \\(B\\) happens or not. Mathematically, we can write this as: \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\] or equivalently: \\[\\mathbb{P}(A | B) = \\mathbb{P}(A).\\] A simple example in our context: The probability of researcher 45 finding an effect (event A) is the same whether researcher 67 finds an effect (event B) or not since they are not communicating with each other. This is the reason why we just multiplied the probabilities of the individual researchers finding an effect/not finding an effect to get the probability of the whole elementary event of the 1000-researcher experiment. Above, we used the very important concept of conditional probability. The probability of event \\(A\\) given that event \\(B\\) has occurred (not necessarily chronologically different!) is denoted as \\[\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}.\\] This video explains it well. For the probability of event A, we are now only interested in the light-grey area with respect to the whole area of event B since event B is our reference frame now (as opposed to the whole space \\(\\Omega\\) before). Note that even if the probability of event A changes when B has happend, B could still have no causal effect on A. They could have a common cause, for instance. Example in our context: Let’s assume researchers 45 and 67 would not be independent. We would for instance find that the probability of 45 is higher than 4% if we knew that 67 found the effect. This does not necessarily mean that researcher 67 causes researcher 45 to find an effect. It could might as well be that their statistical training was very similar and they both made the same mistake in their analysis. 2.2.4 Difference between independence and disjointness There are four possible scenarios when considering two events: Example 1: disjoint but not independent Event A: Patient receives treatment A. Event B: Patient receives treatment B (or is in the control group). These two events, A and B, are disjoint because a patient cannot receive both treatments at the same time. If a patient receives treatment A, he/she cannot receive treatment B (and vice versa), meaning the events cannot occur together in this setting. Thus, \\(P(A \\cap B) = 0\\). However, these events are not independent, because the probability of receiving one treatment depends on not receiving the other. In this setup, if the patient received treatment A, the probability of receiving treatment B is zero: \\(\\mathbb{P}(B|A) = 0\\). The probability of the patient receiving therapy B could be 50% (if randomized): \\(\\mathbb{P}(B) = 0.5\\). Hence, they are dependent. Example 2: independent but not disjoint Event A: The patient shows a treatment effect during a study. Event B: The patient wins the lottery during the study. These two events are independent because the probability of a patient showing a treatment effect is not influenced by whether they win the lottery or not (at least if we assume that lottery participants do not have different properties compared to non-lottery particiants that are conducive to showing a treatment effect). Also, the probability of winning the lottery is not influenced by whether the patient shows a treatment effect or not. We would probably see a surge in volunteers in our studies. The events are unrelated: one depends on the treatment, while the other is purely a matter of luck. However, these events are not disjoint because both can happen at the same time. A patient could experience the treatment effect and also win the lottery during the study. Thus, \\(P(A \\cap B) \\neq 0\\) , meaning both events can occur together. Example 3: neither independent nor disjoint Event A: The patient shows a treatment effect during a study. Event B: The patient is a heavily motivated and self-sufficient. These two events are neither independent nor disjoint. The patient’s motivation could influence the treatment effect (if for instance home exercises are part of the therapy), making the events dependent. However, the patient’s motivation is not mutually exclusive with the treatment effect: The patient can be heavily motivated and show a treatment effect at the same time. Hence, the events are not disjoint either. They can occur together. Example 4: independent and disjoint See Exercise 4. 2.2.5 Answers to questions about the 1000 researcher-experiment (among many others): Maybe, we can already answer some of the questions from above using what we have learned so far. For the first question we would probably bet on the maximum probability of the binomial distribution. The number of positive experiments out of \\(1000\\) has to be between \\(0\\) and \\(1000\\). Each one of them has \\(0.04\\) probability of happening. With R, we quickly calculate the maximum probability: # Calculate the maximum probability using binomial distribution n &lt;- 1000 # number of researchers p &lt;- 0.04 # probability of a treatment effect for each researcher # Calculate the probabilities for each possible number of positive results probs &lt;- dbinom(0:n, size = n, prob = p) # Find the number of experiments with the highest probability # index of the maximum probability starting with 1 max_prob_number &lt;- which.max(probs) # Show the result max_prob_number - 1 # since we started with 0 ## [1] 40 # 40 is the most likely number of positive results dbinom(39:41, size = 1000, prob = 0.04) ## [1] 0.06417798 0.06424483 0.06267788 Now, let’s visualize the binomial distribution for this case using base R syntax: # Plot the binomial distribution plot(0:n, probs, type = &quot;h&quot;, lwd = 2, col = &quot;blue&quot;, xlab = &quot;Number of positive results&quot;, ylab = &quot;Probability&quot;, xlim = c(0, 100), main = &quot;Binomial distribution for treatment effect (yes/no) in 1000 researchers&quot;) abline(v = max_prob_number - 1, col = &quot;red&quot;, lwd = 2, lty = 2) Note, this form of distribution looks like a bell curve, aka a normal distribution, probably the most important distribution in statistics. One can show formally that the binomial distribution converges to the normal distribution under certain conditions. So, if we would only have one shot to predict the number of researches reporting a treatment effect under the assumption that no treatment exists, we would bet on 40. This guess would also not be too bad considering the whole range (0 to 1000) since we can expect the number of successes above, let’s say, 65 and below, let’s say, 15 to be very unlikely. sum(dbinom(0:14, size = 1000, prob = 0.04)) # prob of 14 or less ## [1] 1.384829e-06 sum(dbinom(66:1000, size = 1000, prob = 0.04)) # prob of 66 or more ## [1] 7.160623e-05 sum(dbinom(15:65, size = 1000, prob = 0.04)) # prob of 15 to 65 ## [1] 0.999927 We can easily draw from a binomial distribution in R. We now do the 1000-researcher experiment 10000 times and look at the histogram of the number of positive results: library(ggplot2) # Simulate data data &lt;- rbinom(10000, size = 1000, prob = 0.04) ggplot(data.frame(value = data), aes(x = as.factor(value))) + geom_bar(color = &quot;blue&quot;, fill = &quot;blue&quot;, width = 0.8) + geom_vline(xintercept = as.character(1000 * 0.04), linetype = &quot;dashed&quot;, color = &quot;red&quot;) + labs(title = &quot;Bar Plot of 10000 1000-researcher Experiments&quot;, x = &quot;Number of Effects Found&quot;, y = &quot;Count&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) As you can see, the realized number of found effects matches well the theoretical probabilites given by the binomial distribution. Note, that not necessarily 40 is the most often found number of effects among 1000 researchers, but we are very close. The second question above asked about observing 137 positively reporting researchers. We can calculate the probability of observing 137 or more positive results using the binomial distribution (plug into the formula): \\(\\mathbb{P}(\\text{observing 137 or more}) = \\sum_{i=137}^{1000} \\binom{1000}{i}0.04^i (1-0.04)^{1000-i}\\): # Calculate the probability of observing 137 or more positive results # (using the complement rule) 1 - sum(dbinom(0:136, size = 1000, prob = 0.04)) ## [1] 5.551115e-16 # Compare to winning the Swiss lottery (1 / 31474716) / (1 - sum(dbinom(0:136, size = 1000, prob = 0.04))) ## [1] 57234507 57 million times less likely than winning the Swiss lottery. If this event would happen, we would probably reject the assumption that the therapy is not working at all. In the calculation above, we used the complement rule to calculate the probability of observing 137 or more positive results: \\(1 - \\mathbb{P}(\\text{observing 136 or less})\\). In general, for an event \\(A\\): \\[\\mathbb{P}(A^C) = 1 - \\mathbb{P}(A),\\] library(ggplot2) plot &lt;- ggplot() + geom_rect(aes(xmin = -5, xmax = 5, ymin = -5, ymax = 5), fill = &quot;lightgray&quot;, color = &quot;black&quot;, linewidth = 1) + geom_point(aes(x = 0, y = 0), color = &quot;black&quot;, size = 60, shape = 21, fill = &quot;skyblue&quot;) + annotate(&quot;text&quot;, x = 0, y = 0, label = &quot;A&quot;, size = 6) + annotate(&quot;text&quot;, x = -4.5, y = 4.5, label = &quot;Ω&quot;, size = 6, hjust = 0) + annotate(&quot;text&quot;, x = -3, y = -3, label = &quot;A^C&quot;, parse = TRUE, size = 6) + coord_fixed() + theme_void() print(plot) where \\(A^C\\) comprises all elementary events that are not in \\(A\\). In our case, the compliment of observing 136 or less is observing 137 or more and vice versa: \\(\\mathbb{P}(0, \\dots, 136) = 1 - \\mathbb{P}((0, \\dots, 136)^C) = 1 - \\mathbb{P}(137, \\dots, 1000)\\). The third question above asked about the expected number of positive results if the therapy is working in 12% of the patients. As you can probably guess by now: We would guess \\(1000 \\times 0.12 = 120\\) positive results. This is the so-called expected value \\(\\mathbb{E}(X)\\) of the binomial distribution. It is not always the maximum probability (the so-called mode) of the distribution though: Consider a binomial distribution \\(\\text{Bin}(10, 0.77)\\): The mean is \\(\\mathbb{E}(X) = 10 \\times 0.77 = 7.7\\). This number is not an integer and we can therefore not calculate the density at this point. The mode is 8: library(tidyverse) data.frame(x = 0:10, p_x = dbinom(0:10, 10, 0.77)) %&gt;% filter(x %in% c(7, 8, 9)) ## x p_x ## 1 7 0.2343149 ## 2 8 0.2941670 ## 3 9 0.2188489 The fourth question above asked about the problem of writing a press release stating that 47 out of 50 studies showed a treatment effect. Well, this would be scientific fraud and a case of survivorship bias. You only look at the studies that showed a treatment effect and ignore the ones that did not or you restrict the number of studies to a certain number lower than the true number. This is also relevant in finance. You may want to read this excellent article by John Ioannidis for a humbling big-picture of how relevant published results can be. The fifth question above asked about the problem of multiple testing and related to the previous question. If you test many hypotheses, you will find some “significant” results by chance alone. One could also call the practice of testing many hypotheses to find “significant” ones \\(p\\)-hacking. This should be absolutely avoided. Unfornately, it is still common practice in many fields. Often it happens unconsciously. Example: If you test 100 hypotheses simultaneously at a significance level of 4%, you would expect 4 “significant” results by chance alone. If you report those 4 results as legitimate finding, you are p-hacking. When reading a scientific article, watch out for large amounts of \\(p\\)-values and their (over-)interpretation as “significant” (relevant) or “non-significant” (not relevant). This article is recommendable to get away from a too strict dichotomous interpretation of research results. The sixth question above asked about the variation of positive results between large research departments. This demands the very important concept of variance: The expected quadratic deviation from the mean: \\(\\mathbb{V}ar(X) = \\mathbb{E} \\{ (\\mathbb{E}(X) - X)^2 \\}\\). In simple terms: How much does the number of positive results vary around the mean of 40 on average? See also Exercise 5. Maybe this video helps as well. 2.2.6 Addition of probabilities Above in axiom 3, we stated that the probability of the union of pairwise disjoint events is the sum of the probabilities of the individual events. What if the events are not disjoint? For simplicity, let’s consider only 2 researchers (doing 2 parallel experiments) and define event \\(A_1\\) as “researcher 1 finds an effect” and \\(A_2\\) as “researcher 2 finds an effect”. What is the probability that at least one of the researchers finds an effect? Our event space \\(\\Omega = \\{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \\}.\\) \\(\\sum_{\\omega_i} \\mathbb{P}(\\omega_i) = 0.04^2 + 0.04 \\times 0.96 + 0.96 \\times 0.04 + 0.96^2 = 1\\) \\(A_1 \\cup A_2 = \\{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos)\\}\\) \\(A_1 = \\{ (R1pos, R2pos), (R1pos, R2neg)\\}\\) \\(A_2 = \\{ (R1pos, R2pos), (R1neg, R2pos)\\}\\) \\(\\mathbb{P}(A_1) = 0.04^2 + 0.04 \\times 0.96\\) (First researcher finds an effect or both find an effect) \\(\\mathbb{P}(A_2) = 0.04^2 + 0.96 \\times 0.04\\) (Second researcher finds an effect or both find an effect) In, general, we can write the probability of the union of two events as: \\(\\mathbb{P}(A_1 \\cup A_2) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) - \\mathbb{P}(A_1 \\cap A_2)\\) Put in the values: \\(0.04^2 + 0.04 \\times 0.96 +\\) \\(0.04^2 + 0.96 \\times 0.04 - 0.04^2=\\) \\(0.04^2 + 0.04 \\times 0.96 + 0.96 \\times 0.04. = 0.0784\\). Or simpler with the complement rule: \\(\\mathbb{P}(A_1 \\cup A_2) = 1 - \\mathbb{P}(\\text{neither }A_1 \\text{ nor }A_2) = 1-0.96^2 = 0.0784\\). See also Exercise 6. Here is another helpful depiction of the situation: So, the probability of at least one researcher finding an effect is the sum of the probabilities of the individual researchers finding an effect minus the probability of both finding an effect, which is the same as that both or exactly one of them finds an effect. We can also visualize the 4 disjoint elementary events \\[\\Omega = \\{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \\}\\] in a Venn diagram. The probabilites of these 4 events in the event space \\(\\Omega\\) must add up to 1 since they are disjoint and one of them has to happen. There is no “room” left. 2.2.7 Probabilities for health science We have learned a lot so far: The axioms of probability theory, the difference between independence and disjointness, and the addition of probabilities. How does probability theory fit into the big picture of statistics for health sciences? In many health-related studies, we want to perform one or more of the following tasks: Estimate proportions (e.g., the proportion of patients with lower back pain. How big is the problem from a public health perspective?), Test hypotheses (e.g., whether a new therapy is superior to the standard therapy. How sure can we be that the new therapy is better? What is the probability that the treatment effect is between x and y points on some scale?), Estimate therapy effects (e.g., the effect of a new therapy on pain reduction: How many points does the pain decrease? How is the pain reduction distributed? Are there outliers and why? Are there participants that to not benefit from the therapy?) In all such cases, probability theory is the established tool to answer questions that are afflicted with uncertainty. Would there be no variation in results/effects, we would probably argue differently. In our world, probability theory is the tool to quantify uncertainty. We can always ask ourselves: Where is this entity (proportion, effect, etc.) with which frequency/probability? 2.2.8 Discrete vs. continuous probability distributions As one of the most prominent examples of a discrete distribution, we have already seen the binomial distribution in our 1000-researcher-experiment. A special case of it is the Bernoulli distribution, where you only throw the coin once or let one researcher conduct the experiment. As an example of a continuous distribution we have mentioned the normal distribution above. It is the most important distribution in statistics for reasons that become increasingly clear as we go along. One of them is the central limit theorem which we have already mentioned in the introduction slides. Feel free to watch this video. The theorem states that, under appropriate conditions, the distribution of a normalized version of the sample mean \\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\] converges to a standard normal distribution. By this theorem, we can link any distribution to the normal distribution. Discrete or continuous, the goal is the same: We want to now where the realization of my random variable lands with what probability when I do the experiment. How often will I get heads? How often will the researcher find an effect? With what probability will I get a pain-score reduction of at least 1 point in this patient in front of me given his/her characteristics and history? When looking at ZHAW students, female, soccer lovers; what kind of hourly intense sports activity can I expect and does that differ from other groups? 2.2.8.1 Discrete probability distrubtions are used when we can count the outcomes, which includes infinitely many. Some examples of discrete probability distributions are: Bernoulli distribution: A single trial with two outcomes (e.g., find an effect or do not find an effect). Binomial distribution: The number of successes in a fixed number of trials (e.g., the number of false effects found among 1000 researchers). Poisson distribution: The number of events in a fixed interval of time or space. Geometric distribution: The number of trials until the first success. This number has no upper limit. We always assign probabilites to the countable outcomes of these distributions, like in the example of the binomial distribution when we throw the dice 20 times and are interested in the number of 3s: # Define parameters for the binomial distribution x_values &lt;- 0:20 probabilities &lt;- dbinom(x_values, size = 20, prob = 1 / 6) # Plot the binomial distribution with styling plot(x_values, probabilities, type = &quot;h&quot;, lwd = 2, col = &quot;blue&quot;, xlab = &quot;X&quot;, ylab = &quot;Probability&quot;, main = &quot;Binomial Probability Distribution (n = 20, p = 1/6)&quot;) # Add points for clarity points(x_values, probabilities, pch = 19, col = &quot;red&quot;) Each outcome has a probability \\(&gt;0\\) assigned to it. The sum of all probabilities is 1: \\(\\sum_{i \\in \\text{Possible outcomes}} \\mathbb{P}(X=i) = 1\\). For every event, we just add the probabilities of the elementary outcomes that are in the event: \\(\\mathbb{P}(X \\in (3,8,9,14)) = \\mathbb{P}(X = 3) + \\mathbb{P}(X = 8) + \\mathbb{P}(X = 9) + \\mathbb{P}(X = 14)\\). This principle is true for all discrete probability distributions. Rather simple and elegant: \\[\\sum_{i} \\mathbb{P}(X = x_i) = 1,\\] where \\(X\\) ist the random variable (which takes values \\(x_i\\) when the random experiment is conducted) and \\(x_i\\) are the possible outcomes of \\(X\\). We could invent our own discrete probability distribution instantly (see also Exercise 8), we’ll call it the MSc-ZHAW-distribution: Let \\(X \\in \\mathbb{Z}\\). Every whole number gets the following probability: \\(\\mathbb{P}(X=0) = 0.1\\) and for \\(x_i \\neq 0\\): \\(\\mathbb{P}(X = x_i) = 0.2^{|x_i|}\\). The sum of all probabilities is: \\(\\sum_{x_i \\in \\mathbb{Z}} \\mathbb{P}(X=x_i) = \\mathbb{P}(X=0) + 2 \\cdot \\sum_{i \\in \\mathbb{N}} 0.2^i = 0.1 + 2 \\cdot \\frac{0.2}{1-0.2} = 0.6\\). Hence, we need to divide every probability by 0.6 to get in sum 1. The final definition is then: \\(\\mathbb{P}(X=0) = \\frac{1}{6}\\) and for \\(x_i \\neq 0\\): \\(\\mathbb{P}(X = x_i) = \\frac{5}{3} 0.2^{|x_i|}\\). # Define the probability function P &lt;- function(X) { if (X == 0) { return(1 / 6) } else { return((5 / 3) * (0.2^abs(X))) } } # Create a sequence of X values from -10 to 10 x_values &lt;- -10:10 # Compute the probabilities for each X value probabilities &lt;- sapply(x_values, P) # Plot the probabilities plot(x_values, probabilities, type = &quot;h&quot;, lwd = 2, col = &quot;blue&quot;, xlab = &quot;X&quot;, ylab = &quot;Probability&quot;, main = &quot;MSc-ZHAW Probability Distribution of X from -10 to 10&quot;) # Add points for clarity points(x_values, probabilities, pch = 19, col = &quot;red&quot;) # Check if it sums to 1 (approximately): x_values &lt;- -1000:1000 sum(sapply(x_values, P)) ## [1] 1 Deviations from zero (\\(\\pm 1\\)) are highly likely with this distribution. The probability of \\(X=0\\) is also rather high with \\(\\frac{1}{6}\\). Larger deviations from zero are less likely and go exponentially towards zero (very fast). So we would expect almost never to see values outside of \\(\\pm 10\\). This does of course not mean that we will never see them. Do the experiment often enough and you will see them with probability 1 (see Exercise 10). x_values &lt;- setdiff(-1000:1000, -10:10) # exclude values from -10 to 10 sum(sapply(x_values, P)) ## [1] 8.533333e-08 Expectation \\(\\mathbb{E}(X)\\) of a discrete random variable: The expectation of a discrete random variable \\(X\\) is defined as: \\[\\mu = \\mathbb{E}(X) = \\sum x_i \\cdot \\mathbb{P}(X = x_i),\\] a weighted sum of possible values \\(x_i\\) with their respsective probabilities \\(\\mathbb{P}(X = x_i)\\). The term “expectation” is probably somewhat misleading. It is not necessarily the value we “expect to see” when we do the experiment. For instance, the expected value of a Bernoulli distribution is: \\(\\mu = \\mathbb{E}(X) = 0 \\cdot (1-p) + 1 \\cdot p = p\\), which could be \\(0.5\\). The individual outcomes are \\(0\\) and \\(1\\), and not \\(0.5\\). But \\(0.5\\) would be the mean of the outcomes of many experiments. The expectation can be interpreted as the center of mass of the distribution. It is the value that the distribution “balances” around. Maybe this video helps too. The cool thing is that we can learn the true (but unknown) expectation of a distribution by the sample mean. The more samples we collect, the closer we will be. This is (roughly) the statement of the law of large numbers: \\[\\bar{X}_n \\rightarrow \\mu = \\mathbb{E}(X) \\quad \\text{as} \\quad n \\rightarrow \\infty.\\] See here for an animated example of this law. Remember: The sample mean \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) is a (really good) estimator for the expectation \\(\\mu = \\mathbb{E}(X)\\) of a distribution. This is true for discrete and continuous distributions. The variance of a discrete random variable is defined as: \\[\\mathbb{V}ar(X) = \\mathbb{E} \\{ (\\mathbb{E}(X) - X)^2 \\} = \\sum_i (\\mathbb{E}(X) - x_i)^2 \\mathbb{P}(X = x_i),\\] the expected squared deviation from the mean. It is a measure of how much the values of the random variable differ from the mean. \\((\\mathbb{E}(X) - x_i)^2\\) quantifies the deviation from the mean. We weight this deviation with the probability of such a deviation happening. So a large deviation results only in a large variance if it is likely to happen. Remember: The sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) is a so-called estimator for the variance \\(\\mathbb{V}ar(X)\\) of a distribution. This is true for discrete and continuous distributions. A more natural interpretation of variability is the standard deviation: \\[\\sigma = \\sqrt{\\mathbb{V}ar(X)},\\] since it’s on the same scale as X (e.g. \\(m^2\\) or \\(kg\\)). 2.2.8.2 Continuous probability distributions are used when we cannot count the outcomes. The most famous continuous probability distribution is the normal distribution. This video about probability distributions in general might be helpful. # Load necessary library if (!require(pacman)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load(ggplot2) # Installs and loads the package at the same time # Define parameters for the normal distribution mu &lt;- 0 # Mean sigma &lt;- 1 # Standard deviation # Define the limits for the area to be shaded a &lt;- -2 # Lower bound b &lt;- -1 # Upper bound # Create a sequence of x values to evaluate the PDF x_vals &lt;- seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 1000) # Compute the corresponding density values using dnorm y_vals &lt;- dnorm(x_vals, mean = mu, sd = sigma) # Create a data frame for plotting df &lt;- data.frame(x = x_vals, density = y_vals) # Create a subset of the data for shading the area between a and b df_shaded &lt;- df[df$x &gt;= a &amp; df$x &lt;= b, ] # Plot the normal density and shade the area between a and b ggplot(df, aes(x = x, y = density)) + geom_line(color = &quot;blue&quot;, linewidth = 1) + # Use linewidth instead of size geom_ribbon(data = df_shaded, aes(ymin = 0, ymax = density), fill = &quot;blue&quot;, alpha = 0.3) + # Shaded area ggtitle(paste(&quot;Standard Normal Distribution: N(&quot;, mu, &quot;, &quot;, sigma^2, &quot;)&quot;, sep = &quot;&quot;)) + xlab(&quot;X&quot;) + ylab(&quot;Density&quot;) + theme_minimal() + geom_vline(xintercept = mu, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, x = mu + 0.2, y = max(y_vals) / 2, label = paste(&quot;E(X) =&quot;, mu), color = &quot;red&quot;) + geom_vline(xintercept = a, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = b, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, x = a - 0.2, y = max(y_vals) / 4, label = paste(&quot;a =&quot;, a), color = &quot;black&quot;) + annotate(&quot;text&quot;, x = b + 0.2, y = max(y_vals) / 4, label = paste(&quot;b =&quot;, b), color = &quot;black&quot;) + theme(plot.title = element_text(hjust = 0.5)) Here, like in any other “nice” continuous distribution, the area under the curve is 1: \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1.\\] The probability of a single point is zero (\\(\\mathbb{P}(\\{ x_i \\}) = 0\\)). In any continuous distribution, we use the area under the curve to calculate probabilities. The probability of \\(X\\) being between \\(a\\) and \\(b\\) is the area under the curve (blue shade) between \\(a\\) and \\(b\\): \\(\\mathbb{P}(X \\in (a,b))\\). Note that the area over a single point would be zero and therefore the probability of a single point is zero. The graph above is called a probability density function (PDF). Over every point, we express the probability by the height of the curve. See exercise 3 in the next chapter for a practical example for what we will use this in research. Expectation \\(\\mathbb{E}(X)\\) of a continuous random variable: The expectation of a continuous random variable \\(X\\) is defined as: \\[\\mu = \\mathbb{E}(X) = \\int x \\cdot f(x) dx,\\] where \\(x\\) are the possible values of \\(X\\) and \\(f(x)\\) is the probability density function of \\(X\\). Variance of a continuous random variable: The variance of a continuous random variable \\(X\\) is defined as: \\[\\mathbb{V}ar(X) = \\mathbb{E} \\{ (\\mathbb{E}(X) - X)^2 \\} = \\int (\\mathbb{E}(X) - x)^2 f(x) dx.\\] A more natural interpretation of variability is the standard deviation: \\[\\sigma = \\sqrt{\\mathbb{V}ar(X)},\\] since it’s on the same scale as X. Example: Normally distributed Ages of ZHAW students: \\(\\mu = 24\\), \\(\\sigma = 3\\). For the normal distribution, this means that approx. 68% of the students are between \\((24-3=)21\\) and \\((24+3=)27\\) years old. You might want to keep this picture or this one in mind. So, with just the parameters we have instant information where the values are and where new values drawn from the same distribution are likely to be. 2.2.9 Examples of prominent probability distributions used in health sciences The first 2 are absolutely essential. The most important one is, as mentioned above, the normal distribution. It is often used to model the distribution of many variables in health sciences, e.g., blood pressure, weight, height, etc. Normality is also a common assumption in many statistical tests and models. This is the reason why you will find many statements like “we have checked normality using the Shapiro-Wilk test” (Which I would not recommend) in scientific articles. Normal distribution theory is very aesthetic and one is sometimes lead to believe that this is the normal state of nature, which is not the case. See also the history of the normal distribution. A common use of the normal distribution is in linear regression, where the errors and the conditional distribution of the modeled variable are assumed to be normally distributed. We will deal with this in QM2. The binomial distribution (\\(X \\sim B(n, p)\\)) is used to model the number of successes in a fixed number of trials. For example, the number of patients that respond to a therapy in a fixed number of patients. A special case of it is the Bernoulli distribution, which is used to model a single trial with two outcomes (throw the coin once; \\(X \\sim B(1,p)\\)). Logistic Distribution. Underpins logistic regression models, which are used to predict binary outcomes (e.g., the presence or absence of a disease). Poisson distribution. Used to model the number of events in a fixed interval of time or space. For example, the number of patients arriving at an emergency department in a fixed time interval. Maybe you want to watch this video. Exponential distribution. For instance used in survival analysis to model the time until an event (e.g. refrigerator stops working) occurs. This video might be interesting. Student’s \\(t\\)-distribution (small “t” please) generalizes the standard normal distribution. Like the latter, it is symmetric around zero and bell-shaped, but has fatter tails (compared to the normal distribution), i.e., “extreme” values are more likely. It is a very well known distribution underlying the t-test. See Exercise 12 for a practical example. This video might be interesting. There are infinitely (!) many more distributions. Our goal is to learn: How can we describe (the distribution of) what we see in our data? How can we make predictions? How can we make decisions based on our data? Probability theory and statistics are (for us) a very large tool box to answer these questions. They are unfortunately not magic and cannot turn uncertainty into certainty. 2.3 Exercises Difficulty levels of exercises: E: easy, M: intermediate, D: difficult 2.3.1 [M] Exercise 1 - Throwing a die very often Use your favourite large language model (LLM) to create an R-Script to simulate throwing a fair die 1000 times. Try to run the script. If it does not run, try to debug it using the LLM. Once, the script runs, let the LLM explain the code and outputs (“Please explain this script in detail…”). Plot the frequency of each number (1-6) (after 1000 throws) and compare it to the theoretical probability of getting each number (\\(\\frac{1}{6}\\)). Plot the relative frequency of 3s on the y-axis and the number of throws on the x-axis. This should give a converging pattern towards \\(y=\\frac{1}{6}\\). Which law of probability theory is illustrated by this simulation? 2.3.2 [D] Exercise 2 - Bayes-teaser Use Bayes’ theorem to calculate the posterior probability of the therapy’s effectiveness in the physiotherapy example above (Example). For simplicity, let’s just test two \\(\\theta\\)-values: 0.3 (as in the previous study) and 0.4. We assign 50% in the prior knowledge that the parameter \\(\\theta=0.3\\), and 50% to \\(\\theta=0.4\\) since we trust our colleagues as well. 2.3.3 [E] Exercise 3 - Find journals Note: This is among the most important exercises of the course: Use Google or your favourite search engine to find scientific journals in your field (physiotherapy, midwifery, nursing, etc.). Look at the latest articles. We are interested in articles that used statistics (no qualitative studies). What was the research question? What where they trying to find out/confirm? Write down at least 10 research questions! Which statistical methods were used? Write down at least 10 methods! Was prior/external knowledge - before the actual model was estimated - used in any of the analysis? Where the results presented in a dichotomous way; meaning, was there a “significant”/“non-significant” result or not? 2.3.4 [M] Exercise 4 - Independent and disjoint Look at the definitions above and try to come up with examples for independent and disjoint events in your field of study. Is this possible? Why or why not? What would that imply? Draw a Venn diagram if possible! 2.3.5 [M] Exercise 5 - Variance Simulate the number of positive results (found an effect even though there is none) in our 1000-researcher-experiment under the assumption that the therapy is not working at all (\\(p=0.04\\)). Do this experiment in 10,000 times and visualize the results in a histogram. How often do you get 65 or more positive results? How often do you get 15 or less positive results? Can you find the limits of a 90% interval around the mean (of 40) - using the so-called quantiles - for the number of positive results? What is the theoretical variance for our experiment? How can you estimate this theoretical (and in reality: unknown) variance from the 10,000 simulations? 2.3.6 [E] Exercise 6 - Three researchers Above in Addition of probabilites we went through in detail the case of 2 researchers finding an effect. Let’s now consider 3 researchers simulatenously conducting the experiment. What does the event space \\(\\Omega\\) look like? Which elementary events are in the set of all possible outcomes of our 3-researcher experiment and how many are there? Draw the corresponding binary tree for this experiment. Which elementary events are in the following event: “Researcher 3 finds a positive effect”? Are the events “only researcher 1 finds an effect” and “only researcher 3 finds an effect” disjoint and/or independent? 2.3.7 [E] Exercise 7 - Conditional probability Let’s consider again the 2 reasearcher situation from above (Addition of probabilites). \\(\\Omega = \\{ (R1pos, R2pos), (R1pos, R2neg), (R1neg, R2pos), (R1neg, R2neg) \\}\\). What is the probability that researcher 1 finds an effect given that researcher 2 found an effect? 2.3.8 [E] Exercise 8 - Invent a discrete probability distribution Invent your own discrete probability distribution. What is the expected value of your distribution? What is the variance of your distribution? Think of something in the real world that could be modeled by your distribution. 2.3.9 [E] Exercise 9 - Continuous probability distributions Invent your own continuous probability distribution. What is the expected value of your distribution? What is the variance of your distribution? Think of something in the real world that could be modeled by your distribution. Hint: You can use simple shapes for the densitiy function defined by lines. And you can use simulation to answer questions about expected value and variance. 2.3.10 [M] Exercise 10 - MSc-ZHAW-distribution Create sufficiently many random numbers (sample) from the MSc-ZHAW-distribution (see above) and see if you can produce values outside of \\(\\pm 6\\). What is the mode of this distribution and how could we estimate it from the sample? What is the interquantile range of this distribution and how could we estimate it from the sample? 2.3.11 [M] Exercise 11 - Independence and disjointness for dice events Find examples of dice events when throwing a die once that are: Not independent and not disjoint. Not independent but disjoint. Independent but not disjoint. 2.3.12 [D] Exercise 12 - Student’s \\(t\\)-distribution Let’s look at a paper, where the \\(t\\)-distribution is used (in the background). The aim of the study was to assess the efficacy of pulmonary rehabilitation in addition to regular chest physiotherapy in non cystic fibrosis bronchiectasis. Table 1 describes the patient characteristics in both groups. Table 2 shows the primary endpoint (incremental shuttle walk test - ISWT) at baseline and follow-up time points. Figure 2 shows the outcomes at baseline, 8 weeks and 20 weeks for both groups. They want to find out if the ISWT is different between the two groups. (Note, that an arbitrary threshold for the \\(p\\)-value of 0.05 is used to decide if the groups are “significantly” different. One should avoid these formulations. There is no reason not to use a different threshold (like 4.3%).) The standardized difference of the group means is \\(t\\)-distributed. This case is a bit more complex than the simple ones, since we have different sample sizes (15 vs. 12) and different variances in the groups. The statistics software will take care of this and use the so-called Welch’s \\(t\\)-test. What do you think about the baseline values for ISWT in the two groups? What is the number in brackets next to the ISWT-values? According to the article, the data is normally distributed. Draw 3 probability density functions of normal distributions in one graph with the respective parameters for baseline, 8 weeks and 20 weeks for both groups. Make two graphs, one for each group. According to the text, Figure 2 shows the means \\(\\pm\\) standard errors (\\(SE = \\frac{s}{\\sqrt{n}}\\)) of the ISWT at baseline, 8 weeks and 20 weeks for both groups. Look at Figure 2, a. Does this match the description for instance at 8 weeks in the acappella+pulmonary group? Do the bars make sense? Now, let’s simulate the differences at week 8 (ISWT) using the parameters given: Group sizes, 15 and 12, means (\\(338.7\\) and \\(344.2\\)) and standard deviations (\\(42.2\\) and \\(115.5\\)). Draw a histogram of the simulated differences. Calculate the 1.5% and 98.5% quantiles of the differences. 2.4 Solutions Solutions for this chapter can be found here. 2.5 Sample exam questions for this chapter (in German since exam is in German) For this section, no solutions are provided. 2.5.1 Question 1 - Independence and disjointness Wir werfen einen fairen Würfel einmal und betrachten die Augenzahl. Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)? Die Wahrscheinlichkeit, eine gerade Zahl zu werfen, ist unabhängig von der Wahrscheinlichkeit, eine Zahl größer als 3 zu werfen. Die Ereignisse “gerade Zahl” und “ungerade Zahl” sind disjunkt. Die Ereignisse “Zahl größer als 3” und “Zahl kleiner als 4” sind unabhängig. Die Wahrscheinlichkeit, eine Zahl größer als 3 zu werfen, ist unabhängig (independent) und disjunkt (disjoint) von der Wahrscheinlichkeit, eine Zahl kleiner als 4 zu werfen. 2.5.2 Frage 2 - Bedingte Wahrscheinlichkeit Ein medizinischer Test wird verwendet, um eine bestimmte Krankheit zu erkennen. Der Test hat folgende Eigenschaften: - Sensitivität: 95% - Spezifität: 90% - Die Prävalenz der Krankheit in der Bevölkerung beträgt 2%. Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)? Die Wahrscheinlichkeit, dass der Test positiv ist, wenn die Person die Krankheit hat, beträgt 95%. Die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person die Krankheit nicht hat, beträgt 90%. Die Wahrscheinlichkeit, dass der Test positiv ist, wenn die Person die Krankheit nicht hat, beträgt 10%. Die Wahrscheinlichkeit, dass die Person die Krankheit hat, wenn der Test positiv ist, beträgt \\(0.1623932\\). 2.5.3 Frage 3 - Erwartungswert und Varianz Die diskrete Wahrscheinlichkeitsverteilung einer Zufallsvariablen \\(X\\) ist gegeben durch: \\[ \\begin{array}{|c|c|} \\hline x &amp; P(X = x) \\\\ \\hline 1 &amp; 0.2 \\\\ 2 &amp; 0.3 \\\\ 3 &amp; 0.4 \\\\ 4 &amp; 0.1 \\\\ \\hline \\end{array} \\] Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)? Der Erwartungswert von \\(X\\) beträgt 2.6. Die Varianz von \\(X\\) beträgt 1.3. In einer Wette, welche Ausprägung von \\(X\\) beim nächsten Ziehen kommt, würde man hier auf den Erwartungswert \\(\\mathbb{E}(X)\\) tippen. Die Wahrscheinlichkeit für das Ereignis, dass \\(X\\) eine gerade Zahl ist, ist 50%. 2.5.4 Frage 4 - Dichtefunktion Es sei folgende Dichtefunktion der stetigen Zufallsvariablen \\(X\\) gegeben: Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antwortoptionen)? Die Fläche unter der Dichtefunktion beträgt 1, falls die Spitze des Dreiecks bei Höhe \\(y=2\\) liegt. Modus, Median und Erwartungswert sind identisch. \\(\\mathbb{P}(X \\in (0,0.25)) = \\frac{1}{8}\\). \\(\\mathbb{P}(X = 0.5) = 0\\). "],["descriptive_stats.html", "Chapter 3 Descriptive statistics 3.1 Example: Descriptive statistics in health sciences 3.2 Univarate vs. bivariate statisics 3.3 The histogram 3.4 Q-Q Plots 3.5 Correlation 3.6 Exercises 3.7 Solutions 3.8 Sample exam questions for this chapter (in German since exam is in German)", " Chapter 3 Descriptive statistics There are a myriad sources (books, websites, videos) explaining the concepts of descriptive statistics. We do not need to reiterate everything here. You can go through these sources to get started: R for non-programmers Science direct Descriptive statistics The goal is to describe data in a meaningful and honest way. We summarize data to make them more easily digestable for us humans to answer questions like Where are the data points located? These questions are answered (at least attempted) by the location measures such as mean, median, and mode. How widely are they spread? How much do they vary? These questions are answered by the dispersion measures such as variance, standard deviation (root of the variance), interquantile range or just the range; or even Gini’s mean difference. Are there any outliers (rare data points that are far away from the rest (Westfall 2020, 405) and why? This is a bit more complicated. 3.1 Example: Descriptive statistics in health sciences These are birds-eye views on the data. Let’s look at a paper which was recently published in the Journal of Physiotherapy in order to get a running start: Patients with worse disability respond best to cognitive functional therapy for chronic low back pain: a pre-planned secondary analysis of a randomised trial (This should be open access.) The research question was “Do five baseline moderators identify patients with chronic low back pain who respond best to cognitive functional therapy (CFT) when compared with usual care?”. In Table 2 of the paper, the authors present the baseline characteristics of the patients stratified by the treatment group (ususal care vs CFT). We find absolute numbers, percentages, means, and standard deviations for the continuous variables, medians, and interquartile ranges for the ordinal variables. This should give an idea of the sample. In the population paradigm of statistics, we draw a sample from the population of interest and try to make inferences about the population. We want to learn more about the population respectively that data generating process (DGP) producing the data (Westfall 2020, 6–8). How did the data come about? Note that this sample varies everytime we draw from the population. We can either imagine an infinitely large population or a finite one (e.g. population of Switzerland). Often, variables in a study are (approximately) normally distributed. We can then efficiently summarize the variable with its mean and standard deviation (location and scale parameter) as is done in the paper for Age in years for instance. We do not want to present \\(p\\)-values in such an overview table since we merely describe data instead of making inferences about the population or the DGP. It is by no means given that age has to be normally distributed in our sample. We could easily have a sample with many young people and few elderly ones. This would result in a (positively) skewed distribution. Having many elderly people and few young ones would result in a negatively skewed distribution of course. 3.2 Univarate vs. bivariate statisics One can distinguish between univariate and bivariate statistics. In univariate statistics, we look at one variable at a time, for instance Age in the example above, where we could draw a boxplot or a histogram. Table 2 in the paper is a good example of univariate statistics. We are not so much interested in the relationship between variables. In bivariate statistics, we look at two variables simultaneously. An example could be a scatter plot of Age and Cognitive flexibility, where we would possible find a falling relationship (example in the wild, this would actually be a line plot, not a scatter plot). We are interested in the relationship between the two variables. How do they change (covary) together? Multivariate statistics is the next step, where we look at more than two variables at the same time. 3.3 The histogram One way to visualize the distribution of a continuous variable is the histogram. This video might be helpful to cover the basics. I recommend plotting the histogram with a boxplot below; this helps to visualize the raw data points as well. library(pacman) p_load(tidyverse) set.seed(4433) # to get the same plot every time # Generate normally distributed sample x &lt;- rnorm(10000, mean = 23.4, sd = 5.6) df &lt;- data.frame(values = x) p2 &lt;- ggplot(df, aes(x = values)) + geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7, color = &quot;darkgrey&quot;) + geom_density(aes(y = after_stat(density)), color = &quot;blue&quot;, linewidth = 1) + geom_boxplot(aes(y = -0.01, x = values), width = 0.02, position = position_nudge(y = -0.00)) + geom_point(aes(y = -0.01), position = position_jitter(width = 0.002, height = 0.01), size = 1, alpha = 0.05) + ggtitle(&quot;Histogram with density plot and boxplot below&quot;) + theme(plot.title = element_text(hjust = 0.5)) p2 Some researchers would discard the values below 10 and above 40 as outliers, but we know here that the data points are perfectly legitimate. One important thing we should be aware of in connection with small sample sizes is variability. Let’s create not \\(10000\\) samples of a normally distributed variable, but only \\(25\\) samples: # Generate normally distributed sample set.seed(1245) # to get the same plot every time x &lt;- rnorm(25, mean = 23.4, sd = 5.6) df &lt;- data.frame(values = x) p2 &lt;- ggplot(df, aes(x = values)) + geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7, color = &quot;darkgrey&quot;) + geom_density(aes(y = after_stat(density)), color = &quot;blue&quot;, linewidth = 1) + geom_boxplot(aes(y = -0.01, x = values), width = 0.02, position = position_nudge(y = -0.00)) + geom_point(aes(y = -0.01), position = position_jitter(width = 0.002, height = 0.01), size = 1, alpha = 0.05) + ggtitle(&quot;Histogram with density plot and boxplot below&quot;) + theme(plot.title = element_text(hjust = 0.5)) p2 We know (in this case) that these values come from a normal distribution with a mean of \\(23.4\\) and a standard deviation of \\(5.6\\). Let’s estimate the parameters (\\(\\mu\\), \\(\\sigma\\)) from the sample and use the often but not recommended Shapiro-wilk test for normality: # Estimate mean and standard deviation mean(x) # or ## [1] 20.6017 1 / length(x) * sum(x) ## [1] 20.6017 sd(x) # or ## [1] 4.622042 sqrt(1 / (length(x) - 1) * sum((x - mean(x))^2)) ## [1] 4.622042 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.95059, p-value = 0.2585 Firstly, the histogram looks rather differently when using a different seed. Secondly, we would probability not be able to tell if the data stems from a normal distribution. Thirdly, the sample mean and standard deviation are not all that bad estimators for the true (but unknown) mean and standard deviation. Nicely enough, the Shapiro test would not reject the null hypothesis of normality in this case. Let’s try the Shapiro test with a t-distribution with 3 degrees of freedom, which is not normal: set.seed(1245) # to get the same plot every time x &lt;- rt(25, df = 3) # random numbers from t-distribution df &lt;- data.frame(values = x) p2 &lt;- ggplot(df, aes(x = values)) + geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7, color = &quot;darkgrey&quot;) + geom_density(aes(y = after_stat(density)), color = &quot;blue&quot;, linewidth = 1) + geom_boxplot(aes(y = -0.01, x = values), width = 0.02, position = position_nudge(y = -0.00)) + geom_point(aes(y = -0.01), position = position_jitter(width = 0.002, height = 0.01), size = 1, alpha = 0.05) + ggtitle(&quot;Histogram with density plot and boxplot below&quot;) + theme(plot.title = element_text(hjust = 0.5)) p2 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.95827, p-value = 0.3811 Some researchers would argue, that the distribution is normal (since the \\(p\\)-value is rather large), but we know it is not. We will be cautious with such ugly rule of thumbs. We will instead use Q-Q plots and histograms to decide if a variable is normally distributed. 3.3.1 Example in the wild We will try to catch a histogram in the wild (= in a research paper): We find one here. Figure 1 shows a histogram of the the foot posture index (FPI-6) scores from the participants (3217 healthy children aged 3 to 15). “The FPI score may range from −12 (highly supinated) to +12 (highly pronated)”. There are a few things to note here: Acccording to the Methods-section: “Testing for normality using a Kolmogorov-Smirnov test, found non-normal distribution of all data…”. Especially the BMI could be asymmetrically distributed, or also age. Unfortunately, we do not have histograms or boxplots for these variables. Nevertheless, the authors present the mean and standard deviation for these variables which implies (when not reading the methods section) that the data is (at least sufficiently) normally distributed. Strictly speaking, an FPI score cannot be normally distributed since it takes discrete values which are bounded between -12 and 12 (normal distribution can take values between \\(-\\infty\\) and \\(\\infty\\)). But that should not be a problem with so many levels of an ordered categorical variable. It should be a sufficient approximation. “The FPI was analysed as continuous data, rather than as z-score data”. z-scores are standardized scores: \\[Z = \\frac{X - \\mu_X}{\\sigma_X}\\] Doing this for the FPI scores would give (FPI right, see Table 1): \\[z_i = \\frac{FPI_i - \\overline{FPI}}{SD(FPI)} = \\frac{FPI_i - 4.20}{3.00}\\] \\(\\overline{FPI}\\) ist the arithmetic mean of the FPI scores, \\(SD(FPI)\\) the standard deviation of the FPI scores. \\(\\mu_X\\) is the expectation of X and \\(\\sigma_X\\) the standard deviation of X. So we measure not in FPI units anymore, but in standard deviations from the mean which makes it easier to compare different variables with each other. If we would only center the data and define \\(FPI_{center} = FPI_i - \\overline{FPI}\\), we would interpret the values in FPI units, but the mean would be 0. Hence an \\(FPI_{center}\\)-value of 2 would mean that the FPI score is 2 units above the mean. A nice property of the normal distribution is: If X is normally distributed (\\(X \\sim N(\\mu, \\sigma\\))), then the z-scores are standard normally distributed (\\(Z \\sim N(0,1\\))). See Exercise 4. Of course both, z-scores as well as FPI scores are considered “continuous” (in this context). With so many obsvervations (\\(n=3217\\)), how would a truly normal distribution with the parameters \\(FPI \\sim N(4.20, 3.00)\\) look like? We can simulate this with the following code: set.seed(8345) # to get the same plot every time x &lt;- rnorm(3217, mean = 4.20, sd = 3.00) df &lt;- data.frame(values = x) p2 &lt;- ggplot(df, aes(x = values)) + geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7, color = &quot;darkgrey&quot;) + geom_density(aes(y = after_stat(density)), color = &quot;blue&quot;, linewidth = 1) + geom_boxplot(aes(y = -0.01, x = values), width = 0.02, position = position_nudge(y = -0.00)) + geom_point(aes(y = -0.01), position = position_jitter(width = 0.002, height = 0.01), size = 1, alpha = 0.05) + ggtitle(&quot;Histogram with density plot and boxplot below&quot;) + theme(plot.title = element_text(hjust = 0.5)) p2 Comparing these histograms (using different seed-values), we could assume that for truly normally distributed FPI scores, the histogram might look smoother than the one in Figure 1 in the paper. Especially the values around 6 seem to deviate from the normal distribution. 3.4 Q-Q Plots Another way to check for normality (or if the sample is distributed in another way) is the Q-Q plot. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. On the x-axis we plot the quantiles of the theoretical (for instance) normal distribution and on the y-axis the quantiles of the observed data. If the data is normally distributed, the points should lie on a straight line or at least sufficiently close to a confidence band. See exercise 9. Let’s look at multiple examples of normal Q-Q plots which demonstrate the concept: library(pacman) p_load(car) # Set seed for reproducibility set.seed(123) # 1. Sample from a normal distribution normal_sample &lt;- rnorm(100, mean = 45, sd = 6) qqPlot(normal_sample, main = &quot;Q-Q Plot: Normal Distribution&quot;, id = FALSE) # 2. Sample from an exponential distribution exp_sample &lt;- rexp(100, rate = 1) qqPlot(exp_sample, distribution = &quot;norm&quot;, main = &quot;Q-Q Plot: Exponential vs Normal&quot;, id = FALSE) # 3. Sample from a t-distribution with low degrees of freedom t_sample &lt;- rt(100, df = 2) qqPlot(t_sample, distribution = &quot;norm&quot;, main = &quot;Q-Q Plot: t-Distribution vs Normal&quot;, id = FALSE) The first plot shows a Q-Q plot of a sample from a normal distribution with \\(\\mu = 45\\) and \\(\\sigma = 6\\). The points lie on a straight line and are within a confidence band, indicating that the data is normally distributed. The second plot shows a Q-Q plot of a sample from an exponential distribution, which is obviously not normal for a rate (\\(\\lambda\\)) of 1. There are more observations to be expected in the lower range of x compared to normal for instance. The third plot shows a Q-Q plot of a sample from a Student’s \\(t\\)-distribution with 2 degrees of freedom, which has fater tails than the normal distribution. Hint: There are many R packages to display Q-Q plots, for instance ggpubr. 3.5 Correlation The correlation between two variables is a measure of the strength and direction of the linear relationship between them. It does not (directly) measure other kinds of relationships (for instance monotonic or polynomial). See also Anscombe’s quartet. You might play around with this online tool to get a better feeling what correlation is. Correlation is often denoted by the Greek letter \\[\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\] for the population parameter (the true but unknown) where: \\(\\text{Cov}(X, Y)\\) is the covariance between variables \\(X\\) and \\(Y\\), \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the population standard deviations of \\(X\\) and \\(Y\\), respectively. and \\[r = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\\] for the sample estimate. We can interpret the expression \\(\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\) Without loss of generality let’s assume that \\(\\text{Cov}(X, Y) &gt; 0\\). Assuming, one could change these quantities as one wishes: \\(\\rho\\) is near zero, if the covariance is near zero compared to \\(\\sigma_X \\sigma_Y\\). For fixed \\(\\sigma_X \\sigma_Y\\), the correlation is larger, the larger the covariance \\(\\text{Cov}(X, Y)\\) is. For fixed covariance \\(\\text{Cov}(X, Y)\\), the correlation is larger, the smaller \\(\\sigma_X\\) or \\(\\sigma_Y\\) (or their product) are. An interesting and rarely mentioned version of formula for the correlation coefficient is: \\[ r_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\bar{x}}{s_x} \\right) \\left( \\frac{y_i - \\bar{y}}{s_y} \\right) \\] Here, we see that the correlation coefficient is the average of the products of the z-scores of the two variables. This preserves the covariance interpretation of adding a positive or negative product to the sum if the obeservation is above or below the respective mean. Remember: We use \\(r\\) to estimate \\(\\rho\\) using the sample. The correlation coefficient can take values between -1 and 1. A value of 1 indicates a perfect positive linear relationship between the variables. A value of -1 indicates a perfect negative linear relationship between the variables. See here and here for a nice illustration. Study this in detail. A correlation of \\(\\pm 1\\) just means that the data points lie on a straight line. It does not say anything about the steepness of the line. You can watch these videos to get a better understanding of the concept: StatQuest - correlation StatQuest - covariance Animated correlation Correlation is an often (over-)used measure in research. It is important to note that correlation does not imply causation. For instance, chocolate consumption is positively correlated with the number of Nobel laureates in a country. This does - unfortunately - not mean that eating chocolate makes you smarter. One thing many of us think when they see a high correlation is that if we know the value of one variable, we can predict the value of the other variable. This is not necessarily true. This section explains why. Study the visualization here. It depicts the reduction of the prediction interval for Y given X as the correlation increases (assuming that the variables are jointly normally distributed). How much smaller is the interval where my next observation of Y will fall if I know the value of X? As you can see, this curve is relatively flat in the beginning, meaning, on an individual level, the correlation does not tell us much about the value of Y given X. Of course with correlation 1, we can predict the value of Y perfectly, but even with a correlation of 0.5 (which is considered high in many areas), the prediction interval for Y is only 13% smaller. One often reads that the two variables used for calculation of a Pearson correlation coefficient should be normally distributed. This is not necessary, at least not for descriptive purposes. The correlation coefficient is a measure of the linear relationship and we can think of an example with skewed data where the correlation coefficient is still meaningful: # Load required libraries library(ggplot2) library(ggExtra) # Set seed for reproducibility set.seed(1234) # Parameters n &lt;- 200 # Sample size beta &lt;- 0.7 # Desired beta # Generate left-skewed variable x (negative exponential distribution) x &lt;- -rexp(n, rate = 1) # Generate correlated left-skewed variable y error &lt;- rnorm(n, mean = 0, sd = 2) y &lt;- beta * x + error # Create a data frame data &lt;- data.frame(x = x, y = y) # Create scatterplot with marginal histograms and trendline p &lt;- ggplot(data, aes(x = x, y = y)) + geom_point(alpha = 0.5, color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(title = &quot;Scatterplot with Left-Skewed x and y and Trendline&quot;, x = &quot;Left-Skewed x&quot;, y = &quot;y&quot;) + theme_minimal() # Add marginal histograms using ggExtra ggExtra::ggMarginal(p, type = &quot;histogram&quot;, fill = &quot;lightblue&quot;, color = &quot;black&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; If X and Y are bivariate normally distributed, then one can use the variable \\(t = r \\sqrt{\\frac{n-2}{1-r^2}}\\) as test statistic for the null hypothesis (next chapter) \\(H_0: \\rho = 0\\). Note: It is not sufficient for X and Y to be individually normally distributed in order to be jointly normally distributed. 3.5.1 Example in the wild Let’s again visit the previous paper. In the results, correlations are presented in a dichotomous way with regards to \\(p\\)-values, which should be avoided. We will expand on this in the next chapter. For large sample sizes, even small (true) correlations will be “significant”, especially, if one decides to use an arbitrary threshold for the \\(p\\)-values like \\(0.05\\). There is not much information in this fact. I would argue that all \\(p\\)-values for such large sample sizes and larger correlations do not carry much information. See also Exercise 5. That was all nice, what could go wrong? (The sample) Correlation (coefficient) is based (see the formula above) on the artihmetic mean \\(\\bar{X}\\) (capital X since we are talking about the random variable which is realised when the \\(X_i\\) have materialized into the \\(x_i\\)) and sample standard deviation \\(s\\). The sample mean is not robust against outliers. A single large value can distort the mean arbitrarily much. Hence, the correlation coefficient is not robust against outliers. Let’s see this in action: # Load necessary libraries library(ggplot2) library(ggpubr) # Set seed for reproducibility set.seed(123) # Parameters n &lt;- 100 # Sample size rho &lt;- 0.75 # Desired correlation # Generate bivariate normal data with specified correlation x &lt;- rnorm(n) y &lt;- rho * x + sqrt(1 - rho^2) * rnorm(n) # see e.g. https://www.probabilitycourse.com/chapter5/5_3_2_bivariate_normal_dist.php # Store original data data_original &lt;- data.frame(x = x, y = y) # Function to create scatterplot with trend line and correlation create_plot &lt;- function(data, title) { ggplot(data, aes(x = x, y = y)) + geom_point(alpha = 0.6, color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + labs(title = title, subtitle = paste(&quot;Correlation:&quot;, round(cor(data$x, data$y), 2))) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) # Center the title } # Original scatterplot p_original &lt;- create_plot(data_original, &quot;Original Data&quot;) # Add outlier in x data_outlier_x &lt;- data_original data_outlier_x$x[n] &lt;- max(x) + 5 # Extreme value in x p_outlier_x &lt;- create_plot(data_outlier_x, &quot;Outlier in X&quot;) # Add outlier in both x and y data_outlier_xy &lt;- data_original data_outlier_xy$x[n] &lt;- max(x) + 5 # Extreme value in x data_outlier_xy$y[n] &lt;- max(y) + 5 # Extreme value in y p_outlier_xy &lt;- create_plot(data_outlier_xy, &quot;Outlier in Both X and Y&quot;) # Add outlier in y data_outlier_y &lt;- data_original data_outlier_y$y[n] &lt;- max(y) + 5 # Extreme value in y p_outlier_y &lt;- create_plot(data_outlier_y, &quot;Outlier in Y&quot;) # Arrange plots using ggarrange ggarrange(p_original, p_outlier_x, p_outlier_xy, p_outlier_y, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), ncol = 2, nrow = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; A: \\(r=0.71\\) for the original data. B: When we add an outlier in x, \\(r\\) decreases to \\(0.44\\) and the trendline is notably shifted. The reason for the smaller correlation is that the outlier is above the mean of the x values and below the mean of the y values and therefore adds negatively to the covariance which is the numberator in the formula for \\(r\\). The denominator is non-negative. C: When we add an outlier in both \\(x\\) and \\(y\\), \\(r\\) stays relatively stable. (At least in this case: We could also add, e.g., an outlier above the y mean and below the x mean.) The reason is that the outlier is above the mean of the x values and above the mean of the y values, so it adds positively to the covariance. Also, the outlier is in line with the trend of the data. We could ask, what happend in this data point? Was another scale used for instance? D: When we add an outlier in \\(y\\), the \\(r\\) decreases to 0.48. The reason for the smaller correlation is that the outlier is above the mean of the y values and below the mean of the x values and therefore adds negatively to the covariance. We could also ask why the trend lines (created with simple linear regression) change the way they do. We will come to this later in our courses. Please look at this really cool animation. This is the simulation of the regression line when an outlier with distance 8 to the center is introduced and wanders around the clock. 3.5.2 Spearman correlation The Pearson correlation coefficient is a measure of the linear relationship between two continuous variables. Spearman’s rank correlation coefficient, denoted by \\(\\rho\\), is a non-parametric measure of the monotonic relationship between two variables and can also be applied to ordered categorical variables (levels like “bad”, “medium”, “best”). It is defined as the Pearson correlation coefficient between the rank variables of the two variables. A rank is the position of an observation in an ordered list of observations. Since only the ranks are used, the Spearman correlation is less affected by outliers. \\[ r_s = \\rho\\left[\\mathrm{R}[X], \\mathrm{R}[Y]\\right] = \\frac{\\mathrm{cov}\\left[\\mathrm{R}[X], \\mathrm{R}[Y]\\right]} {\\sigma_{\\mathrm{R}[X]} \\sigma_{\\mathrm{R}[Y]}} \\] Interpretation: The sign of the Spearman correlation indicates the direction of association between X (the independent variable) and Y (the dependent variable). If Y tends to increase when X increases, the Spearman correlation coefficient is positive. If Y tends to decrease when X increases, the Spearman correlation coefficient is negative. Here is an example of a scatterplot depicting a non-linear relationship between two variables \\(X\\) and \\(Y\\). Spearman’s rank correlation coefficient equals 1 in this case, since the function is perfectly monotonically increasing. Pearson’s correlation coefficient would be \\(&lt;1\\) in this case and argueably not appropriate here. We can easily calculate both and compare them. If they differ noticably, we need to understand why. Here is an example of a Spearman correlation: set.seed(123) x &lt;- 1:7 y &lt;- round(2 * x + rt(7, df = 3)) plot(x, y, type = &quot;p&quot;, col = &quot;blue&quot;, lwd = 2) cor.test(x, y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 10.553, df = 5, p-value = 0.000132 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8553851 0.9969116 ## sample estimates: ## cor ## 0.9782797 cor.test(x, y, method = &quot;spearman&quot;) ## Warning in cor.test.default(x, y, method = &quot;spearman&quot;): Cannot compute exact ## p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 0.50225, p-value = 1.456e-05 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9910312 # by hand: rank(x) # already ordered ## [1] 1 2 3 4 5 6 7 rank(y) # order of y-values ## [1] 1.0 2.0 3.5 3.5 5.0 6.0 7.0 y ## [1] 1 3 6 6 12 14 15 cor(rank(x), rank(y), method = &quot;pearson&quot;) ## [1] 0.9910312 cor(x, y, method = &quot;spearman&quot;) ## [1] 0.9910312 The y-values contain two identical values (\\(6\\)), both get rank \\(3.5\\). These are so-called ties. 3.6 Exercises Difficulty levels of exercises: E: easy, M: intermediate, D: difficult 3.6.1 [D] Exercise 1 - Recreate table with fake data Create fake data for the study mentioned above in R. Recreate Table 2 of the paper mentioned above with fake data in R (using GPT, the R package gtsummary and other useful packages). This is rather helpful later on in your master thesis. Try to export the table to Excel and Word. 3.6.2 [E] Exercise 2 - Outliers and estimates Let’s assume we know that the Cognitive flexibility is normally distributed with a mean of 60 and a standard deviation of 7.4 (Table 1): \\(CognFlex \\sim N(60,7.4)\\). Draw a sample of 165 persons from this distribution and calculate the mean and standard deviation of the sample. How good is the estimate of the true (and in this case: known) mean and standard deviation? Let’s replace some of the data points with outliers. Change the score of 5 persons with to impossible CognFlex score of 100. Calculate the mean and standard deviation of the sample. How do the estimates change? When we try to estimate the location of our Cognitive flexibility distribution with the median, how many outliers of what magnitude are necessary to disturb the estimate by 5 points? 3.6.3 [E] Exercise 3 - Recreating data in Table 2 We assume that age in both groups is normally distributed with a mean of 48 (47) years and a standard deviation of 16 (15) years: \\(Age_{UsualCare} \\sim Normal(\\mu = 48, \\sigma = 16)\\) and \\(Age_{CFT} \\sim Normal(\\mu = 47,\\sigma = 15)\\). Under these assumptions, what is the probability, that we would see a person of age 60 or older in a new sample (in either group)? What is the probability, that we would see a person of age 18 or younger in a new sample? Give a 99% interval for the age in CFT, where we would expect a new person drawn from the same population. Let’s assume Sex is binomially distributed with a probability of \\(p = 0.59\\) for both groups. What is the probability, that we would see a woman as the next recruited person in the Usual care group? What is the probability, that we would see no man in a sample of 10 persons in the CFT group? Sometimes you want balanced samples. How many patients would we need to recruit to get at least 45 women with a probability of at least 90%. (We could solve this with simulation.) 3.6.4 [E] Exercise 4 - Z-scores Show with a simulation that the z-scores are standard normally distributed if the original variable is normally distributed. Try different parameter values for \\(\\mu\\) and \\(\\sigma\\) and plot the histogram of the z-scores. Optional: Try to prove this mathematically. 3.6.5 [M] Exercise 5 - Correlation The following R-code creates correlated samples with a (true) correlation \\(\\rho\\): set.seed(1234) n &lt;- 1000 rho &lt;- 0.5 x &lt;- rnorm(n) y &lt;- rho * x + sqrt(1 - rho^2) * rnorm(n) cor(x, y) ## [1] 0.541743 Try different values for \\(\\rho\\) and \\(n\\) and plot the scatterplot of \\(x\\) and \\(y\\). Execute the code above 1000 times and save the correlation coefficient in a vector. Calculate the sample quantiles and interquantile range of the simulated correlation coefficients. What do you observe with respect to variability? Plot the histogram of the simulated correlation coefficients. 3.6.6 [M] Exercise 6 - Bike parking locations in Switzerland Download the data from the bike parking locations json-file and execute the following code to see an interactive map of the bike parking locations in Switzerland (adapt file path if necessary): library(pacman) p_load(jsonlite, tidyverse, leaflet) bike &lt;- fromJSON(&quot;./DATA/bike_parking.json&quot;) # Extract the &quot;features&quot; data frame features &lt;- bike$features # Extract and flatten geometry (coordinates) coordinates &lt;- do.call(rbind, lapply(features$geometry$coordinates, as.numeric)) colnames(coordinates) &lt;- c(&quot;longitude&quot;, &quot;latitude&quot;) bike_data &lt;- cbind(coordinates, features$properties) head(bike_data) ## longitude latitude name stopPlaceUic stopPlaceSloid ## 1 8.902417 47.48863 Veloparking überdacht 8506013 NA ## 2 8.902425 47.48814 Veloparking überdacht 8506013 NA ## 3 8.902277 47.48854 Veloparking überdacht 8506013 NA ## 4 8.050095 47.39113 Veloparking überdacht 8502113 NA ## 5 8.050337 47.39128 Velostation 8502113 NA ## 6 7.278020 47.04381 Veloparking überdacht 8504404 NA ## source.name source.id category subCategory ## 1 prail 2f767602-03fd-49e1-bca6-7fa60e9df655 parking bike_parking ## 2 prail 4d98b698-cd5b-4b40-abb4-004827e26e5b parking bike_parking ## 3 prail 75dc4e71-5aec-4d58-b4f4-ef815f8f5f40 parking bike_parking ## 4 prail 51585d0e-ea2a-4763-a4e7-669b89f24f0b parking bike_parking ## 5 prail 9caf1c56-5cac-423b-912a-5e551b4c69a5 parking bike_parking ## 6 prail 3e7aa3c8-4245-488b-9f00-b44ee0d8b79a parking bike_parking ## address_discriminatorType address_city address_postalCode capacity ## 1 Address Aadorf 8355 48 ## 2 Address Aadorf 8355 48 ## 3 Address Aadorf 8355 80 ## 4 Address Aarau 5000 186 ## 5 Address Aarau 5000 230 ## 6 Address Aarberg 3270 72 ## properties_bookingSystem ## 1 {&#39;type&#39;: &#39;NOVA&#39;} ## 2 {&#39;type&#39;: &#39;NOVA&#39;} ## 3 {&#39;type&#39;: &#39;NOVA&#39;} ## 4 {&#39;type&#39;: &#39;NOVA&#39;} ## 5 {&#39;type&#39;: &#39;NOVA&#39;} ## 6 {&#39;type&#39;: &#39;NOVA&#39;} ## properties_capacities ## 1 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 48}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## 2 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 48}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## 3 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 80}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## 4 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 186}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## 5 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 230}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## 6 [{&#39;categoryType&#39;: &#39;STANDARD&#39;, &#39;total&#39;: 72}, {&#39;categoryType&#39;: &#39;DISABLED_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;RESERVABLE_PARKING_SPACE&#39;, &#39;total&#39;: 0}, {&#39;categoryType&#39;: &#39;WITH_CHARGING_STATION&#39;, &#39;total&#39;: 0}] ## properties_operator properties_parkingFacilityCategory ## 1 SBB BIKE ## 2 Sonstige BIKE ## 3 SBB BIKE ## 4 Sonstige BIKE ## 5 Sonstige BIKE ## 6 SBB BIKE ## properties_parkingFacilityType ## 1 BIKE_PARKING_COVERED ## 2 BIKE_PARKING_COVERED ## 3 BIKE_PARKING_COVERED ## 4 BIKE_PARKING_COVERED ## 5 BIKE_STATION ## 6 BIKE_PARKING_COVERED ## properties_bikeFacilityTraits ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 [{&#39;bikeFacilityTraitId&#39;: &#39;74de7b37-e6ce-43cb-a5bb-6283a6e87d5b&#39;, &#39;bikeFacilityTraitNameI18n&#39;: {&#39;de&#39;: &#39;Doppelstock&#39;, &#39;en&#39;: &#39; Double level&#39;, &#39;it&#39;: &#39; Due piani&#39;, &#39;fr&#39;: &#39; Deux étages&#39;}}] ## 5 [{&#39;bikeFacilityTraitId&#39;: &#39;552fd1ef-bbc7-4eb7-8ef4-12901aff5250&#39;, &#39;bikeFacilityTraitNameI18n&#39;: {&#39;de&#39;: &#39;24h-Zugang mit velocity.ch&#39;, &#39;en&#39;: &#39; 24h access with velocity.ch&#39;, &#39;it&#39;: &#39; Accesso 24 ore su 24 con velocity.ch&#39;, &#39;fr&#39;: &#39; Accès 24h/24 avec velocity.ch&#39;}}, {&#39;bikeFacilityTraitId&#39;: &#39;74de7b37-e6ce-43cb-a5bb-6283a6e87d5b&#39;, &#39;bikeFacilityTraitNameI18n&#39;: {&#39;de&#39;: &#39;Doppelstock&#39;, &#39;en&#39;: &#39; Double level&#39;, &#39;it&#39;: &#39; Due piani&#39;, &#39;fr&#39;: &#39; Deux étages&#39;}}, {&#39;bikeFacilityTraitId&#39;: &#39;ef596249-6ff8-4484-b30f-91fb1a93588e&#39;, &#39;bikeFacilityTraitNameI18n&#39;: {&#39;de&#39;: &#39;Self-Service-Reparaturwerkzeuge&#39;, &#39;en&#39;: &#39; Self-service repair tools&#39;, &#39;it&#39;: &#39; Attrezzi per riparazioni self-service&#39;, &#39;fr&#39;: &#39; Outils en libre-service pour réparer les vélos&#39;}}] ## 6 &lt;NA&gt; ## openingHours ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 [{&#39;slots&#39;: [{&#39;weekdayFrom&#39;: &#39;Mo&#39;, &#39;weekdayTo&#39;: &#39;Su&#39;, &#39;timeFrom&#39;: &#39;00:00&#39;, &#39;timeTo&#39;: &#39;23:59&#39;}]}] ## 6 &lt;NA&gt; ## address_streetName address_houseNumber ## 1 &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; ## 5 Bahnhofplatz 3a ## 6 &lt;NA&gt; &lt;NA&gt; ## properties_operationTime ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 {&#39;operatingFrom&#39;: &#39;00:00:00&#39;, &#39;operatingTo&#39;: &#39;00:00:00&#39;, &#39;daysOfWeek&#39;: [&#39;MONDAY&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;THURSDAY&#39;, &#39;FRIDAY&#39;, &#39;SATURDAY&#39;, &#39;SUNDAY&#39;]} ## 6 &lt;NA&gt; ## properties_pricingModel ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; bike_cleaned &lt;- bike_data %&gt;% dplyr::select(longitude, latitude, name, address_city, address_postalCode, capacity, properties_operator) head(bike_cleaned) ## longitude latitude name address_city address_postalCode ## 1 8.902417 47.48863 Veloparking überdacht Aadorf 8355 ## 2 8.902425 47.48814 Veloparking überdacht Aadorf 8355 ## 3 8.902277 47.48854 Veloparking überdacht Aadorf 8355 ## 4 8.050095 47.39113 Veloparking überdacht Aarau 5000 ## 5 8.050337 47.39128 Velostation Aarau 5000 ## 6 7.278020 47.04381 Veloparking überdacht Aarberg 3270 ## capacity properties_operator ## 1 48 SBB ## 2 48 Sonstige ## 3 80 SBB ## 4 186 Sonstige ## 5 230 Sonstige ## 6 72 SBB # Create an interactive map # Check the structure of capacity str(bike_cleaned$capacity) ## chr [1:1394] &quot;48&quot; &quot;48&quot; &quot;80&quot; &quot;186&quot; &quot;230&quot; &quot;72&quot; &quot;22&quot; &quot;45&quot; &quot;53&quot; &quot;44&quot; &quot;120&quot; ... # Convert capacity to numeric bike_cleaned$capacity &lt;- as.numeric(bike_cleaned$capacity) # Check for warnings if (any(is.na(bike_cleaned$capacity))) { warning(&quot;Some capacities could not be converted to numeric. Non-numeric values have been set to NA.&quot;) } # Retry the map creation leaflet(bike_cleaned) %&gt;% addTiles() %&gt;% # Add OpenStreetMap tiles addCircleMarkers( ~longitude, ~latitude, popup = ~paste( &quot;&lt;strong&gt;Name:&lt;/strong&gt;&quot;, name, &quot;&lt;br&gt;&lt;strong&gt;City:&lt;/strong&gt;&quot;, address_city, &quot;&lt;br&gt;&lt;strong&gt;Capacity:&lt;/strong&gt;&quot;, capacity, &quot;&lt;br&gt;&lt;strong&gt;Operator:&lt;/strong&gt;&quot;, properties_operator ), radius = ~log(capacity + 1) * 2, # Radius scaled by capacity color = ~ifelse(properties_operator == &quot;SBB&quot;, &quot;blue&quot;, &quot;red&quot;), fillOpacity = 0.7 ) %&gt;% addLegend(&quot;bottomright&quot;, colors = c(&quot;blue&quot;, &quot;red&quot;), labels = c(&quot;SBB&quot;, &quot;Other Operators&quot;), title = &quot;Operators&quot;) Let’s look at the data frame bike_cleaned in R. In what city is the bike parking with the highest capacity located? Which cities are represented in the dataset? How many bike parkings are operated by the SBB (Swiss Federal Railways)? Create a bar plot to visualize this. What is the average capacity of bike parkings in the dataset? Draw a histogram and a boxplot below. What are the dimensions of the data set bike_cleaned? 3.6.7 [E] Exercise 7 - Median, Mean, and Mode Find a skewed continuous probability distribution and plot the theoretical density. Draw a sample from this distribution and calculate the mean, median, and mode. Plot a histogram with vertical lines for the mean, median, and mode. How do these values compare to each other and why do they look like this considering the definition of each? Find a skewed variable in a research paper. 3.6.8 [E] Exercise 8 - Correlation by hand Calculate the correlation coefficient by hand for the following data points and compare with the R-function cor(): library(pacman) p_load(tidyverse) set.seed(123) # Für reproduzierbare Ergebnisse x &lt;- c(1, 2, 3, 4, 5) y &lt;- c(1.5, 2.5, 3.5, 4.5, 5.5) + rnorm(5, 0, 2) # Rauschen hinzufügen 3.6.9 [E] Exercise 9 - Q-Q plot Create a Q-Q plot for a sample from a normal distribution and compare it with the theoretical quantiles. Repeat this multiple times again and again to get a feeling how truly normally distributed data looks like. 3.7 Solutions Solutions for this chapter can be found here. 3.8 Sample exam questions for this chapter (in German since exam is in German) 3.8.1 Frage 1 - Mittelwert, Median, Modus Gegeben sei ein Histogramm einer Stichprobe (\\(n=1000\\)). Im Bild zu sehen ist die aus der Stichprobe geschätzte Wahrscheinlichkeitsdichte. Die 3 vertikalen strichlierten Linien zeigen den Mittelwert, Median und Modus der Stichprobe an (nicht notwendigerweise in der genannten Reihenfolge). Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Der Mittelwert ist größer als der Median. Der Modus ist größer als der Median. Die Daten wurden höchstwahrscheinlich aus einer symmetrischen Verteilung gezogen. Es handelt sich sehr wahrscheinlich um eine bimodale Verteilung. 3.8.2 Frage 2 - Normalverteilung Es seien zwei Stichproben von Körpergrößen gegeben. \\(200\\) von Frauen, \\(200\\) von Kindern. Zu sehen sind 3 Q-Q-Plots. ## [1] 164 97 ## [1] 164 72 ## [1] 160 159 Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Der Q-Q-Plot der kombinierten Körpergrößen lässt auf eine Normalverteilung schließen. Der Q-Q-Plot der Frauen-Körpergrößen ist kompatibel mit einer Normalverteilung. Der Q-Q-Plot der Kinder-Körpergrößen ist kompatibel mit einer Normalverteilung. Das Histogramm der Kinder-Körpergrößen zeigt eine bimodale Verteilung. 3.8.3 Frage 3 - Korrelation Gegeben sei folgende Stichprobe von Punktepaaren \\((x_i, y_i)\\) der Variablen \\(X\\) und \\(Y\\): Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Pearson’s Korrelationskoeffizient ist geignet um den Zusammenhang zwischen \\(X\\) und \\(Y\\) zu messen. Pearson’s Korrelationskoeffizient ist gleich 1. Spearman’s Rangkorrelationskoeffizient ist \\(&lt;1\\). Spearman’s Rangkorrelationskoeffizient misst die Stärke des linearen Zusammenhangs zwischen \\(X\\) und \\(Y\\). 3.8.4 Frage 4 Gegeben sei folgende Stichprobe: \\(x = (9.83, 13.87, 5.90, 10.56, 6.95, 9.46)\\). Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Median und Mittelwert sind identisch. Ein Modus ist \\(9.83\\). Die Stichprobenstandardabweichung ist \\(2.420024\\). Der Korrelationskoeffizient beträgt \\(0.78\\). References Westfall, Peter. 2020. Understanding Regression Analysis: An Conditional Distribution Approach. Wiley. "],["bayes_statistics.html", "Chapter 4 Bayes statistics 4.1 Derivation of Bayes’ theorem 4.2 Bayes’ theorem in the context of parameter estimation 4.3 Examples 4.4 Highest Density Intervals (HDI) 4.5 Bayesian \\(t\\)-test 4.6 Bayesian updating 4.7 More complex parameter spaces 4.8 Advantages/disadvantages of Bayesian statistics 4.9 Exercises 4.10 [E] Exercise 1 - defective product rate 4.11 [H] Exercise 2 - Bayesian updating 4.12 Solutions 4.13 Sample exam questions for this chapter (in German since exam is in German)", " Chapter 4 Bayes statistics To gain a richer understanding, I can recommend (optionally) reading the introductory chapters in John Kruschke’s book Doing Bayesian Data Analysis. As stated in the first chapter, Bayesian statistics is based on the idea that probability is a measure of our uncertainty about an event or a parameter. Here, we use prior (i.e., before/outside of our experiment) knowledge about a parameter and update this knowledge with new data using the famous Bayes’ theorem: \\[ p(\\theta | \\text{data}) = \\frac{p(\\text{data} | \\theta) \\cdot p(\\theta)}{p(\\text{data})}, \\] where: \\(p(\\theta | \\text{data})\\) is the posterior probability (respectively the posterior distribution of the parameter): the updated probability of the parameter \\(\\theta\\) given the observed data. \\(p(\\text{data} | \\theta)\\) is the likelihood: the probability of observing the data given a certain value of the parameter \\(\\theta\\). \\(p(\\theta)\\) is the prior probability (respectively the prior distribution of the parameter): the initial belief about the parameter \\(\\theta\\) before seeing the data. \\(p(\\text{data})\\) is the marginal likelihood or evidence: the probability of observing the data under all possible parameter values. In our context, \\(\\theta\\) is an effect size, a group difference (\\(\\mu_1 - \\mu_2\\)), a correlation (\\(\\rho\\)), a regression coefficient (\\(\\beta\\)), or a proportion (p)… 4.1 Derivation of Bayes’ theorem We have already come across Bayes’ theorem in the context of conditional probability where we have calulated the probability of a person having a disease given having tested positive for it: \\[ \\mathbb{P}(Dpos|Tpos) = \\frac{\\mathbb{P}(Tpos|Dpos) \\cdot \\mathbb{P}(Dpos)}{\\mathbb{P}(Tpos)}. \\] The numerator is by definition of the conditional probability just: \\(\\mathbb{P}(Tpos \\cap Dpos)\\). Let’s put this in: \\[ \\mathbb{P}(Dpos|Tpos) = \\frac{\\mathbb{P}(Tpos \\cap Dpos)}{\\mathbb{P}(Tpos)}. \\] which is also true by the definition of conditional probability. One can rewrite the denominator as: \\[ \\mathbb{P}(Tpos) = \\mathbb{P}(Tpos|Dpos)\\mathbb{P}(Dpos) + \\mathbb{P}(Tpos|Dneg)\\mathbb{P}(Dneg) = TP + FP, \\] since one can have a positive test result if one has the disease (true positive) or if one does not have the disease (false positive). This is the so-called law of total probability. To see this, just draw a binary tree starting with disease status (pos/neg) and test result (pos/neg) as branches, as we did before. Summarized, we have: \\[ \\mathbb{P}(Dpos|Tpos) = \\frac{\\mathbb{P}(Tpos|Dpos) \\cdot \\mathbb{P}(Dpos)}{\\mathbb{P}(Tpos|Dpos) \\cdot \\mathbb{P}(Dpos) + \\mathbb{P}(Tpos|Dneg) \\cdot \\mathbb{P}(Dneg)}. \\] We have proven Bayes theorem for the case of a binary test. Note the simple fact that if \\(\\mathbb{P}(Dpos)\\) (which is the prevalence of the disease) is small, the probability of having the disease given a positive test result is also small. In fact, it is arbitrarily small the smaller the prevalence is. For \\(\\mathbb{P}(Dpos)=0\\), we have \\(\\mathbb{P}(Dpos|Tpos) = \\frac{0}{0 + \\mathbb{P}(Tpos|Dneg) \\cdot 1} = 0\\), assuming that there can still be false positive test results: \\(\\mathbb{P}(Tpos|Dneg) &gt; 0\\). We could easily image that there are not only two states reported by a test. Maybe, it is a more sophisticated test reporting 3 or 4 states. We would just extend the denominator accordingly (e.g., for a 3-state test with labels \\(T_1, T_2, T_3\\), where \\(T_3\\) indicates a high concentration of some component): \\[ \\mathbb{P}(Dpos|T_3) = \\frac{\\mathbb{P}(T_3|Dpos) \\cdot \\mathbb{P}(Dpos)}{\\mathbb{P}(T_3|Dpos) \\cdot \\mathbb{P}(Dpos) + \\mathbb{P}(T_3|Dneg) \\cdot \\mathbb{P}(Dneg)}, \\] which would be the probability that the person has the disease given that the test result is \\(T_3\\). In this context, we are interested in the probability of a state of disease. We estimate this probability based on the test result (\\(Tpos\\) or \\(Tneg\\)) and prior knowledge about the disease (\\(\\mathbb{P}(Dpos)\\)). The prior knowledge does not necessarily have to be known before the test, the point is to combine knowledge in a coherent way. We will later see that the order of this combination is not important. 4.2 Bayes’ theorem in the context of parameter estimation If we use the coin flip example, you can always think of probability of a therapy working for instance. It is a placeholder for a parameter of interest. A probability distribution outlines all possible outcomes of a random process along with their associated probabilities. For instance, in the case of a coin toss, the distribution is straightforward: there are two outcomes, heads and tails, with corresponding probabilities \\(\\theta\\) and \\(1-\\theta\\). For more complex scenarios, such as measuring the height of a randomly chosen individual, the distribution is less straightforward. Here, each possible height, say 172 cm or 181 cm, is assigned a probability as we have discussed in the context of the normal distribution and continuous distributions in general. 4.3 Examples We have jumped into the deep end right away with exercise 2 in the previous chapter. Let’s now look at some more examples to get a feeling for how the Bayes theorem works. 4.3.1 Example 1 - defective products (Example from Script “Statistik und Wahrscheinlichkeitstheorie using R”, S.331 ff, Werner Gurker) A manufacturer claims that the defect rate of their products is only 5%, while the customer believes it to be 10%. Before the result of a sample inspection is known, we assign both rates an equal 50-50 chance: \\[ \\pi(0.05) = \\pi(0.10) = 0.5 \\] Assume that in a sample of size 20, there are 3 defective items. Using the binomial distribution \\(B(20, \\theta)\\), the data information is given as follows: \\[ p(3|\\theta = 0.05) = \\binom{20}{3} (0.05)^3 (0.95)^{17} = 0.0596 \\] \\[ p(3|\\theta = 0.10) = \\binom{20}{3} (0.10)^3 (0.90)^{17} = 0.1901 \\] The marginal distribution of \\(X\\) (the number of defective items in the sample) for \\(x = 3\\) is as follows: \\[ m(3) = p(3|0.05)\\pi(0.05) + p(3|0.10)\\pi(0.10) = 0.1249 \\] Here, we consider all parameters in the parameter space of interest. We are only interested in \\(\\theta = 0.05\\) vs. \\(\\theta = 0.10\\) in this case. \\[ \\pi(0.05 \\mid X = 3) = \\frac{p(3 \\mid 0.05) \\pi(0.05)}{m(3)} = 0.2387 \\] \\[ \\pi(0.10 \\mid X = 3) = \\frac{p(3 \\mid 0.10) \\pi(0.10)}{m(3)} = \\mathbf{0.7613} \\] A priori, we had no preference for either of the two defect rates. After observing a relatively high defect rate of \\(3/20 = 15\\%\\) in the sample, the posterior probability for \\(\\theta = 0.10\\) is approximately three times as likely as \\(\\theta = 0.05\\). Note that in this example the a priori probabilities were equal: R codes are in the git repository After collecting data, we have updated our prior beliefs about the defect rate: R codes are in the git repository Let’s assume, we know the manufacturer to be rather untrustworthy. Many inspections in the past revealed higher defect rates than claimed. For simplicity, we still want to decide between the two defect rates. \\(0.05\\) and \\(0.10\\). How does the calculation change if we assign a much higher probability to the defect rate of \\(0.10\\)? See Exercise 1 for this. 4.3.2 Example 2 - extending the defective products example What if we do not want to limit ourselves to the two defect rates of 5% and 10%? We now get creative and assign the following prior probability distribution to the defective rate \\(\\theta\\): R codes are in the git repository If we had to guess, we would say that the defect rate is most likely 20%. And we are rather sure about this guess. The mode (highest prior probability) of the beta distribution is at 0.2. We are also pretty sure that the defect rate is not 0% or above, say, 50%. This distribution is an expression of our prior beliefs about the defect rate. Now, we observe 11 defective items in a sample of 20. We expect as posterior distribution not a bar plot with two probabilities for the parameter values \\(\\theta = 0.05\\) and \\(\\theta = 0.10\\), but a distribution of probabilities for all possible defect rates (in the range of 0-1). Btw. we could play the same game using a coin or the effectiveness of a therapy as example. The only thing we need to do is to plug in the information into Bayes’ theorem to get the posterior distribution. The calculations would be complex by hand, so we just use R. We want to understand the concept, so calculating by hand can be put off until later (if at all necessary for us.) \\[ \\textit{p}(\\theta \\mid X = 11) = \\frac{\\textit{p}(X = 11 \\mid \\theta) \\cdot \\text{Beta}(\\theta \\mid \\alpha, \\beta)}{m(11)} \\] \\(\\text{Beta}(\\theta \\mid \\alpha, \\beta)\\) is the prior distribution (density) of the defect rate which captures our beliefs about the defect rate. \\(\\textit{p}(X = 11 \\mid \\theta)\\) is the likelihood of observing 11 defective items in a sample of 20 given a certain defect rate \\(\\theta\\). This would be the density function of a binomial distribution. \\(m(11)\\) is the marginal distribution of the data for \\(X = 11\\) considering all possible defect rates between 0 and 1. \\(\\textit{p}(\\theta \\mid X = 11)\\) is the posterior density of the defect rate given the observed data. Let’s look at the resulting posterior distribution: R codes are in the git repository As we can see, the posterior distribution has a new peak at \\(0.38\\). If we had to guess now, we would probably say that the defect rate is around 38%. We have updated our beliefs about the defect rate based on the new data. In the graph, there is also the observed defect rate of 55%. This observed rate (new data) draws our prior beliefs towards the observed rate. The more data we observe, the more the posterior distribution would be drawn towards the observed rate, because we would be surer due to the large sample size. The stronger our prior beliefs in a certain value (or range) of \\(\\theta\\), the less we are convinced by new data. One really nice aspect of the Bayesian view is that we get a full probability distribution for the parameter of interest, given the prior beliefs and the observed data. We then can elegantly make all kinds of statements when looking at the posterior, like the following: The probability that \\(\\theta\\) is between \\(0.30\\) and \\(0.50\\) is (area under the posterior) approximately \\(0.80\\): R codes are in the git repository The probability that \\(\\theta\\) is below \\(0.25\\) is (area under the posterior): R codes are in the git repository The previous two examples showcase how to estimate a proportion using prior knowledge and new data. The fact, that we get a full distribution of the parameter of interest, is a key feature of Bayesian statistics and (as far as I know) not available in frequentist statistics and Null Hypothesis Significance Testing (NHST), which we will discuss in the next chapter. 4.4 Highest Density Intervals (HDI) Apart from taking the mode or mean of the posterior distribution, another way to summarize a posterior distribution is by using the highest density interval (HDI). The HDI identifies the most credible range of values in a distribution while covering a specified portion of the distribution, such as 93%. The key feature of the HDI is that every point inside the interval has a higher probability density than any point outside the interval, making it an effective summary of the most plausible values. Here is an example for a 93% HDI for the posterior distribution of \\(\\theta\\): R codes are in the git repository Interpretation: The 93% HDI for the posterior distribution of \\(\\theta\\) is \\([0.25, 0.51]\\). With a probability of 93%, the defect rate is between 25% and 55%, given our prior beliefs and the observed data. Note that the HDI does not necessarily have to be symmetric around the peak of the distribution. There could be, for instance, two peaks in the distribution, as you can see here for instance. In that case, the HDI would consist of two intervals. One could readily image a real life case for such a distribution: Think of a group of people where the measurement of interest is the 100 m sprint time. There could be two groups of people: The hobby runners and the professional athletes. The distribution of the sprint times would probably be bimodal showing two peaks for the two groups. There are other ways, one can construct credible intervals. We could also use the quantiles of the posterior distribution to construct a credible interval. If the distribution is symmetric, the quantiles are symmetric around the peak of the distribution and the HDI is the same as the quantile-based credible interval. A credible interval is not the same as a confidence interval in frequentist statistics. We will discuss this in the next chapter. Briefly, a credible interval is a range of values for a parameter of interest that has a specified probability of containing the unobserved parameter. A confidence interval is an interval which is expected to contain the true, but unknown parameter of interest in a certain percent of times (e.g., 92%), when constructed repeatedly everytime a new sample is drawn. 4.5 Bayesian \\(t\\)-test Above, we looked at some aspects of estimating a parameter (proportion) using Bayes’ theorem. We can answer all sorts of other questions using this paradigm. In classical statistics, one often wants to know if two groups differ with respect to their true means. This answer is typically given by the famous \\(t\\)-test (small “\\(t\\)” please). I encourage you to read the short history of the \\(t\\)-test. For didactic reasons, we will look at the Baysian version of the \\(t\\)-test first. We want to try to view statistics as more than just a cookbook of recipes. Unfortunately, very often it is taught that way and the impression is given that statistics is just a set of tools to apply in a certain order. In my humble opinion, this is not the case. Statistics and model building is a creative process and - if you want to go so far - an art form. Statistics is difficult and beautiful - in that order. Unfortunately, it is not completely trivial to apply the Bayesian \\(t\\)-test, as opposte to the frequentist \\(t\\)-test, which constitutes one line of code in R. We’ll use the not anymore maintained R package BEST for a nice illustration. Later, we will use more up-to-date packages like brms or rethinking for modeling. 4.5.1 Example - Bayesian \\(t\\)-test For an explanation of the statistical model behind the Bayesian \\(t\\)-test, visit Kruschke, Figure 2. (y1 &lt;- c(-.5, 0, 1.2, 1.2, 1.2, 1.9, 2.4, 3) * 100) ## [1] -50 0 120 120 120 190 240 300 (y2 &lt;- c(-1.2, -1.2, -.5, 0, 0, .5, 1.1, 1.9) * 100) ## [1] -120 -120 -50 0 0 50 110 190 length(y1) ## [1] 8 length(y2) ## [1] 8 data &lt;- data.frame(y1, y2) psych::describe(data) ## vars n mean sd median trimmed mad min max range skew kurtosis ## y1 1 8 130.0 116.00 120 130.0 140.85 -50 300 350 -0.13 -1.39 ## y2 2 8 7.5 107.94 0 7.5 118.61 -120 190 310 0.29 -1.38 ## se ## y1 41.01 ## y2 38.16 # Boxplot: data.frame(y = c(y1, y2), group = c(rep(1, 8), rep(2, 8))) %&gt;% ggplot(aes(x = factor(group), y = y)) + # Use factor for discrete x-axis geom_boxplot() + # Add boxplot layer geom_jitter(width = 0.1) # Add jitter for individual # -&gt; Visually, there seems to be a difference between the two groups. We work with a rather small sample size, 8 in each group. We want to know if the two groups differ in their (unobserved) means. For this, we will apply the R function BESTmcmc from the package BEST. set.seed(33443) p_load(HDInterval, BEST, tictoc, psych, tidyverse) # H_0: mue1 - mue2 = delta_0 a &lt;- 20 # ROPE (region of practical equivalence) d_0 &lt;- 0 BESTout &lt;- BESTmcmc(y1, y2, verbose = TRUE) ## Waiting for parallel processing to complete...done. plot(BESTout, which = &quot;mean&quot;, compVal = d_0, ROPE = d_0 + c(-1, 1) * a, showCurve = FALSE, credMass = 0.93) BESTmcmc is a function that uses a Markov Chain Monte Carlo (MCMC) algorithm to estimate the posterior distribution of the difference between the means of two groups (the above is not just a histogram of differences). We do not care about these details for now, but try to interpret what we see. The plot shows the posterior distribution of the difference between the means of the two groups. With the parameter which = \"mean\", we are interested in the posterior distribution of difference of the means (\\(\\mu_1 - \\mu_2\\)). We see a full probability distribution again. The parameter compVal = d_0 is the value we want to compare the posterior distribution to. Here We chose delta_0 = 0, which means that we want to know if the difference between the means is different from zero. The parameter ROPE = d_0 + c(-1, 1) * a (\\(= -20\\) to \\(20\\)) defines the region of practical equivalence (ROPE). In our case, we chose a = 20. This means that we consider differences between the means of the two groups of \\(\\pm 20\\) as practically equivalent. In practice, you would choose a value that is meaningful for your particular research question. For instance, in a planned experiment, we are interested in changes on the Roland Morris Disability Questionnaire (RMDQ) of 2 points. We would then choose a value of 2 for the ROPE, since only a change beyond this value would be clinically relevant for the patient. The parameter credMass = 0.93 is the probability mass to include in credible intervals, in this case 93%. The 93% HDI for the difference between the means is shown in the graph, hence given the prior information and the observed data, we can be 93% sure that the difference between the means is in this interval. The HDI changes everytime the code of this script is run since it’s simulation based. We can do more: summary(BESTout, ROPEm = d_0 + c(-1, 1) * a, compValm = 0, digits = 5, credMass = 0.93) # summary.BEST() ## mean median mode HDI% HDIlo HDIup compVal %&gt;compVal ## mu1 130.784 130.863 130.39 93 31.8470 225.75 ## mu2 6.105 5.844 6.65 93 -84.0293 96.93 ## muDiff 124.679 124.991 128.62 93 -10.2279 255.39 0 95.6 ## sigma1 139.321 128.472 111.42 93 67.1477 224.94 ## sigma2 129.387 119.425 105.55 93 63.0326 209.59 ## sigmaDiff 9.933 9.067 8.42 93 -113.9592 136.39 0 56.8 ## nu 34.513 25.870 9.32 93 1.0944 84.21 ## log10nu 1.380 1.413 1.49 93 0.6635 2.06 ## effSz 0.969 0.957 0.91 93 -0.0697 2.02 0 95.6 ## ROPElow ROPEhigh %InROPE ## mu1 ## mu2 ## muDiff -20 20 4.6 ## sigma1 ## sigma2 ## sigmaDiff ## nu ## log10nu ## effSz If you look at the row “muDiff” and the column “%&gt;compVal”. This is the percentage of the posterior distribution that is greater than 0. We can say, with a probability of ~95%, the difference between the means is greater than 0. This is nice but not the whole story. The true difference can by greater than 0, but still rather small and clinically irrelevant. Would we conclude that the two groups differ in their means? According to Kruschke (p.336): “A parameter value is declared to be not credible, or rejected, if its entire ROPE lies outside the highest density interval (HDI) of the posterior distribution of that parameter.” This is not the case here. We would not conclude, that the means between the groups differ. One could argue, that this rule seems rather strict considering, that ~95% of the posterior distribution of the differences is greater than 0. As opposed to the frequentist \\(t\\)-test, we can also affirm the null hypothesis, that the means in the two groups are equal. This is a nice feature of the Bayesian \\(t\\)-test. “A parameter value is declared to be accepted for practical purposes if that value’s ROPE completely contains the X% HDI of the posterior of that parameter.” This is also not the case here. The 93% HDI lies outside the ROPE to a large part. Note, “The decision rule for accepting the null value says merely that the most credible values are practically equivalent to the null value according to the chosen ROPE, not necessarily that the null value has high credibility.” (Kruschke, p.337) We are above in the situation, that we cannot make a clear decision. The ROPE does not fully contain the 93% HDI, and the 93% HDI is not completely outside the ROPE. “When the HDI and ROPE overlap, with the ROPE not completely containing the HDI, then neither of the above decision rules is satisfied, and we withhold a decision. This means merely that the current data are insufficient to yield a clear decision one way or the other, according to the stated decision criteria.” (Kruschke, p.337)” What have we learned so far: Estimating a proportion using Bayes’ theorem Example about the proportion of defective products Using a whole continuous distribution as prior knowledge Example about the proportion of defective products extended Analysis if two groups differ in their means or have the same mean Example about the Bayesian t-test Latest in QM2, we will do Bayesian regression analysis using packages like brms from Paul Bürkner or rethinking from Richard McElreath. 4.6 Bayesian updating See also 5.2.1. Data-order invariance in Kruschke. The following is an animated example of Bayesian updating. The true probability for heads in a coin toss is \\(\\theta = 0.77\\). We throw a coin 100 times and we start with a uniform prior distribution, i.e., every value between 0 and 1 is equally likely at the beginning. Everytime a coin is tossed, the prior distribution is updated with the new data. The posterior distribution is the prior for the next coin toss et cetera. One can see that we are converging to the “truth” (which is normally not known). Code can be found here. ## Linking to ImageMagick 6.9.12.93 ## Enabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp ## Disabled features: fftw, ghostscript, x11 ## # A tibble: 100 × 7 ## format width height colorspace matte filesize density ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;chr&gt; ## 1 GIF 480 480 sRGB TRUE 0 72x72 ## 2 GIF 480 480 sRGB TRUE 0 72x72 ## 3 GIF 480 480 sRGB TRUE 0 72x72 ## 4 GIF 480 480 sRGB TRUE 0 72x72 ## 5 GIF 480 480 sRGB TRUE 0 72x72 ## 6 GIF 480 480 sRGB TRUE 0 72x72 ## 7 GIF 480 480 sRGB TRUE 0 72x72 ## 8 GIF 480 480 sRGB TRUE 0 72x72 ## 9 GIF 480 480 sRGB TRUE 0 72x72 ## 10 GIF 480 480 sRGB TRUE 0 72x72 ## # ℹ 90 more rows Figure 4.1: example caption This video illustrates the concept of Bayesian updating as well using two different starting points. The opinions seem to converge. 4.7 More complex parameter spaces Above we dealt with the rather simple case of one parameter (\\(\\theta\\)), the proportion of defective products or the fairness of a coin. Some of these cases can be solved analytically, like the coin toss example. One can calculate the shape of the posterior distribution exactly. We are mainly interested in simulation-based results, since we are practitioners. In the Baysian \\(t\\)-test, we had two groups and the difference of their means. Behind the scenes there were already more than two parameters estimated (see Figure 2 here). Here lies the computational bottleneck of Bayesian statistics. The more parameters we have, the more complex the parameter space becomes. In order to estimate the posterior distribution, we have to sample from the parameter space by walking through intelligently. This can be computationally intensive. The animation in this article shows the walk through of the parameter space nicely for the case of a normal distribution with two parameters (\\(\\mu\\) and \\(\\sigma\\)). For statistical models with many parameters and observations, the computation can take a long time. Very often though, computational time is not an issue and we can make use of the flexibility and intuitive interpretation of Bayesian statistics. 4.8 Advantages/disadvantages of Bayesian statistics 4.8.1 (Some) Advantages Full probability distributions: We get a full probability distribution for the parameter of interest. Flexibility: We can incorporate prior knowledge into our analysis. Interpretability: We can make statements about the probability of certain parameter values. No \\(p\\)-values: We do not need to rely on \\(p\\)-values and NHST (Null Hypothesis Significance Testing). John Kruschke points out many problems with NHST in his book and article. It seems that for many practinioners, \\(p\\)-values are hard to understand and interpret correctly. There are many misconceptions about \\(p\\)-values. And one should definitely move away from dichotomous thinking of “significant” and “non-significant” results. 4.8.2 (Some) Disadvantages Computational complexity: Calculating the posterior distribution can be computationally intensive. Even with modern computers and the latest packages, it can take a long time to get results. Classical statistical models are often estimated within fractions of a second. Barriers to entry: Statistics courses and textbooks often focus on frequentist statistics, as well as many sofware packages lean more towards frequentist statistics. This can make it difficult for newcomers to learn Bayesian statistics. When publishing, many reviewers could be sceptical or unfamiliar with Bayesian statistics. Subjectivity: The choice of the prior distribution can be subjective. 4.9 Exercises Difficulty levels of exercises: E: easy, M: intermediate, D: difficult 4.10 [E] Exercise 1 - defective product rate Let’s revisit Example 1 and change the prior probabilites to \\(\\pi(0.05) = 0.1\\) and \\(\\pi(0.10) = 0.9\\). Calculate and plot the posterior probabilities for \\(\\theta = 0.05\\) and \\(\\theta = 0.10\\). 4.11 [H] Exercise 2 - Bayesian updating We want to check empirically that the order of data collection does not influence the posterior distribution. We want to find out the probability \\(\\theta\\) of a therapy to work. Since we have absolutely no idea, how effective the therapy is, let our prior distribution be a uniform distribution between 0 and 1 (this has to be argued for more rigorously in practice). Hence, every value between 0 and 1 is equally likely: \\(\\pi(\\theta) = 1\\) for \\(\\theta \\in [0,1]\\). # Load required libraries library(ggplot2) # Generate data for a uniform distribution theta &lt;- seq(0, 1, length.out = 100) # Range of theta density &lt;- dunif(theta, min = 0, max = 1) # Uniform density # Create a data frame for ggplot df &lt;- data.frame(theta = theta, density = density) # Plot the density function ggplot(df, aes(x = theta, y = density)) + geom_line(color = &quot;blue&quot;, linewidth = 1.2) + # Density line geom_area(fill = &quot;lightblue&quot;, alpha = 0.5) + # Shaded area under the curve labs( title = &quot;Prior: Uniform Distribution&quot;, x = expression(theta), y = &quot;Density&quot; ) + theme_minimal(base_size = 14) + theme( plot.title = element_text(face = &quot;bold&quot;, size = 16, hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;) ) Notize, that the area under the curve is 1, since it is a probability density function. For example, \\(\\mathbb{P}(\\theta \\in (0.25,0.75)) = 0.5\\) Experiment 1 one shows that the therapy works in 17 out of 50 cases. Experiment 2 shows that the therapy works in 23 out of 50 cases. So we go from: Prior \\(\\rightarrow\\) experiment 1 \\(\\rightarrow\\) posterior 1 \\(\\rightarrow\\) experiment 2 \\(\\rightarrow\\) posterior12. Or Prior \\(\\rightarrow\\) experiment 2 \\(\\rightarrow\\) posterior 2 \\(\\rightarrow\\) experiment 1 \\(\\rightarrow\\) posterior21. Show that posterior12 and posterior21 should be the same and draw both posterior distributions into one plot. [Optional] Show this result analytically. 4.12 Solutions Solutions for this chapter can be found here. 4.13 Sample exam questions for this chapter (in German since exam is in German) For this section, no solutions are provided. 4.13.1 Frage 1 sleep: Daten, die die Wirkung von zwei schlaffördernden Medikamenten (Zunahme der Schlafstunden im Vergleich zur Kontrollgruppe) bei 10 Patienten zeigen. Wir führen einen Bayesian \\(t\\)-test auf Gleichheit der Mittelwerte der beiden Gruppen durch: ## Waiting for parallel processing to complete...done. ## 1.881 sec elapsed ## mean median mode HDI% HDIlo HDIup compVal %&gt;compVal ROPElow ## mu1 0.711 0.707 0.659 93 -0.537 1.971 ## mu2 2.306 2.300 2.296 93 0.822 3.686 ## muDiff -1.595 -1.593 -1.666 93 -3.520 0.301 0 6.15 -1.8 ## sigma1 2.033 1.916 1.803 93 1.082 3.100 ## sigma2 2.307 2.174 1.926 93 1.262 3.479 ## sigmaDiff -0.274 -0.252 -0.186 93 -1.920 1.434 0 36.86 ## nu 35.874 27.541 12.429 93 1.443 85.719 ## log10nu 1.409 1.440 1.531 93 0.727 2.065 ## effSz -0.753 -0.748 -0.721 93 -1.637 0.150 0 6.15 ## ROPEhigh %InROPE ## mu1 ## mu2 ## muDiff 1.8 58.1 ## sigma1 ## sigma2 ## sigmaDiff ## nu ## log10nu ## effSz Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? -1.59 ist eine (aufgrund des Bayes \\(t\\)-tests) plausible Schätzung für die Differenz der Mittelwerte der beiden Gruppen. Wäre die ROPE halb so groß, würde die Testentscheidung anders ausfallen. 93% der posterior Verteilung der Differenz der Mittelwerte liegt im Interval \\([-3.48, 0.298]\\). Die Testentscheidung lautet: “Die Mittelwerte der beiden Gruppen sind nicht gleich.” 4.13.2 Frage 2 \\[ \\mathbb{P}(Dpos|Tpos) = \\frac{\\mathbb{P}(Tpos|Dpos) \\cdot \\mathbb{P}(Dpos)}{\\mathbb{P}(Tpos|Dpos) \\cdot \\mathbb{P}(Dpos) + (1-\\mathbb{P}(Tneg|Dneg)) \\cdot \\mathbb{P}(Dneg)}. \\] Welche der folgenden Aussagen ist/sind korrekt (0-4 korrekte Antworten)? Mit einer Spezifität von 1, wäre die Wahrscheinlichkeit für eine Krankheit, wenn der Test positiv ist, gleich 1. Mit einer Sensitivität von 1, wäre die Wahrscheinlichkeit für eine Krankheit, wenn der Test positiv ist, gleich 1. Falls \\(\\mathbb{P}(Dpos) = 1\\), ist \\(\\mathbb{P}(Dpos|Tpos) = 1\\). Im Nenner des Bayes’schen Theorems steht die Wahrscheinlichkeit, dass der Test positiv ist. 4.13.3 Frage 3 Bei einer Bayes’schen Analyse eines Fehleranteils verwenden wir folgende a priori Verteilung für den Fehleranteil \\(\\theta\\): Welche der folgenden Aussagen ist/sind korrekt (0-4 korrekte Antworten)? Die a priori Wahrscheinlichkeit, dass der Fehleranteil \\(0.02\\) beträgt, ist \\(0.02\\). Die a priori Wahrscheinlichkeit, dass der Fehleranteil im (geschlossenen) Interval \\([0.02, 0.08]\\) liegt, ist 0. Hier handelt es sich um eine diskrete a priori Verteilung und man untersucht lediglich, welcher der beiden Fehleranteile wahrscheinlicher ist. Man beobachtet bei einer Stichprobe von \\(20\\) Produkten, dass \\(0\\) davon fehlerhaft sind. A posteriori wird der Fehleranteil \\(0.08\\) nun mehr als 50% Wahrscheinlichkeit erhalten. "],["nhst.html", "Chapter 5 Null Hypothesis Significance Testing (NHST) 5.1 Example in the literature 5.2 Binomial test 5.3 Proportions test 5.4 (Classical) \\(t\\)-test 5.5 Correlation test 5.6 Type 1 and Type 2 errors 5.7 The frequentist confidence interval 5.8 Simulations based approaches 5.9 Exercises 5.10 Sample exam questions for this chapter (in German since exam is in German)", " Chapter 5 Null Hypothesis Significance Testing (NHST) Null Hypothesis Significance Testing (NHST) is a statistical method widely used in research, including health sciences, to evaluate whether observed data provide sufficient evidence to refute a specific hypothesis. It operates within a framework of probability and decision-making to address the following question: Are the observed results likely to occur by chance alone if the null hypothesis is true? If one decides that the observed results are unlikely to occur by chance alone, the null hypothesis is rejected in favor of an alternative hypothesis. NHST is not designed to “prove” hypotheses but rather to provide evidence against the null hypothesis. If one reads in a paper that there was “no association”, this typically means the \\(p\\)-value was larger than the arbitrary threshold of \\(0.05\\). This does not imply that there is no association (Misconception 2 here). Underlying NHST is the idea of falsifiability. Sometimes one counter example is enough to reject a hypothesis like “all swans are white”. Seeing one black swan, proves the hypothesis wrong. Key concepts: Null Hypothesis \\(H_0\\): This represents the assumption of some specific effect or no effect at all. For example, in a clinical trial comparing two treatments, one might state that both treatments have the same effect. The alternative ist that one treatment is superior to the other. Alternative Hypothesis \\(H_1\\): This is the opposing claim to \\(H_0\\), the logical complement. Example: \\(H_0: \\theta \\le 0.4\\) \\(H_1: \\theta &gt; 0.4\\) \\(H_0\\) states that the treatment effect is less than or equal to 0.4, while \\(H_1\\) states that the treatment effect is greater than 0.4. \\(p\\)-value: The \\(p\\)-value quantifies the probability of obtaining results (test statistic) as extreme as (or more extreme than) the observed data, assuming that \\(H_0\\) is true. A smaller \\(p\\)-value indicates stronger evidence against \\(H_0\\). There are many misconeptions about \\(p\\)-values. Significance level \\(\\alpha\\): Researchers set a threshold to determine whether to reject \\(H_0\\). If the \\(p\\)-value is smaller than \\(\\alpha\\), \\(H_0\\) is rejected in favor of \\(H_1\\). Note, that there is absolutely no special reason to use \\(\\alpha = 0.05\\) as a default value. To quote Ronald Fisher: “…, and it is convenient to take twice the standard error as the limit of significance; this is roughly equivalent to the corresponding limit P=0.05, …” How NHST works: Formulate Hypotheses: Define \\(H_0\\) (e.g., “The new therapy has no effect”) and \\(H_1\\) (e.g., “The new therapy improves outcomes”). Determine the necessary sample size to find the effect. Collect data: Perform an experiment or study to gather relevant data. Calculate the test statistic: Compute a value based on the sample data that reflects the difference or effect under investigation. Compute the \\(p\\)-value: Determine the probability of observing the test statistic (or more extreme values) if \\(H_0\\) is true. Make a decision: Compare the \\(p\\)-value to the significance level: If \\(p &lt; \\alpha\\): Reject \\(H_0\\); evidence suggests \\(H_1\\) is true. If \\(p \\ge \\alpha\\): Fail to reject \\(H_0\\); insufficient evidence to support \\(H_1\\). (Some) Limitations of NHST: Focus on \\(p\\)-values with hard cut-offs: Solely relying on \\(p\\)-values can lead to overinterpretation of results without considering practical significance. Dichotomous thinking: The decision to “reject” or “fail to reject” \\(H_0\\) oversimplifies the complexity of real-world data. This binary thinking incentivizes researchers to focus on statistical significance rather than real relevance. It happened to me not only once that a colleague looked at me with a sad facial expression announcing that the \\(p\\)-value was “not significant”. This should be not an issue at all. The focus should be on doing useful analyses in the most rigorous way possible. Note that the difference between “significant” and “not significant” is itself not significant. Sample size influence: Large samples can make small, clinically irrelevant differences “statistically significant”. Example: Given an arbitrarily small difference between means of two groups. There is always a sample size that makes the difference “significant”. See Exercise 2. Publication bias: Many journals tended to publish studies with “significant” results. “Not significant” results were often not published. This leads to a distorted view of the literature. All rigorously conducted studies should be published, irrespective of the results. \\(p\\)-hacking: In the pursuit for “significant” results (which results in publications, which results in tenure), it is natural to do everything to get them. This can include data dredging, selective reporting, and other questionable practices. In my humble opinion, very often, researchers are not to blame but incentives. It is important to understand that probabilities are defined before an event has happened. The probability for the esteemed reader to win the Swiss lottery is rather small and I would take a large bet against it. But every week, someone wins (approximately). After the fact, one should not be surprised that someone won, since it follows from the law of large numbers. See Kruschke for more limitations of NHST. In practice, NHST should at last be accompanied by confidence intervals, effect size calculations, and a focus on clinical relevance to provide a more comprehensive understanding of the results. This Review of NHST or this overview might be a good entry point. This video could also help to understand the basic concepts. 5.1 Example in the literature \\(p\\)-values are omnipresent in the scientific literature. There rather few papers in our field that do not contain them. We do not use \\(p\\)-values in descriptive tables (see here and here). Here is an example from the literature. Table 2 lists studies and one column contains \\(p\\)-values. Note, that it is not good practice to present \\(p\\)-values (if one should use them at all) as dichotomy: \\(p &lt; 0.05\\) (Misconception 8 here). This statement does not allow to judge the strength of the evidence against the null hypothesis. Both, \\(p = 0.00000001\\) and \\(p = 0.049\\) would satisfy the inequality. Since this dichotomy decides around an arbitrary threshold (\\(\\alpha = 0.05\\)), the decision is also somewhat arbitrary. When reading papers, watch out for oceans of \\(p\\)-values and their selective dichotomous interpretation. This is a clear warning sign. 5.2 Binomial test In the first chapter, we invented the 1000-researcher experiment. There, we have already encountered hypothesis tests in disguise. If we would assume that the probability of a false positive is 0.04, we would “expect” (around) 40 false positives. We asked, what is the probability of observing 137 or more. This is an example of a one-sided hypothesis test: \\(H_0: \\theta \\le 0.04\\) \\(H_1: \\theta &gt; 0.04\\) Under \\(H_0\\), what is the probabilty to see the oberved number (137) of false positives (in our case, this is the test statistic) or more? The answer was \\(p = 5.551115 \\cdot 10^{-16}\\) (or less if one chooses \\(\\theta &lt; 0.04\\)). A reasonable person would say, that this result did not happen by chance alone and therefore conclude, the true, but unknown false positive rate \\(\\theta\\) is larger than 0.04. Formally, this is called a (one-sided) binomial test. Note, that \\(H_1\\) is the logical complement of \\(H_0\\). # binomial test binom.test(137, 1000, p = 0.04, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 137 and 1000 ## number of successes = 137, number of trials = 1000, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is greater than 0.04 ## 95 percent confidence interval: ## 0.1194241 1.0000000 ## sample estimates: ## probability of success ## 0.137 The output in R tells us the following: data: 137 and 1000 successes oberved alternative hypothesis: true probability of success is greater than 0.04, which we assume afterwards. \\(p\\)-value &lt; \\(2.2e-16\\). This value is smaller than the precision in R (`.Machine’). 95 percent confidence interval: \\(0.1194241\\) to \\(1.0000000\\). The upper limit of \\(1\\) occurs since we have a one-sided test. 95% is just convention and has no special meaning. Sample estimates: Estimating the true (but unknown) proportion from the sample would just be: \\(\\frac{137}{1000} = 0.137\\) Two-sided test: \\(H_0: \\theta = 0.04\\) \\(H_1: \\theta \\ne 0.04\\) One could argue that this is bad style, since we should probably know the direction of the effect (see also 4.2. here). binom.test(137, 1000, p = 0.04, alternative = &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 137 and 1000 ## number of successes = 137, number of trials = 1000, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.04 ## 95 percent confidence interval: ## 0.1162817 0.1598810 ## sample estimates: ## probability of success ## 0.137 alternative = “two.sided”, this indicates that we are interested in both directions (higher or lower than \\(0.04\\)). With some experience, one would probably not test for lower when seeing the observed number of 137. 95 percent confidence interval: \\(0.1162817\\) to \\(0.1598810\\). Interpretation of this frequentist confidence interval (CI): When drawing repeated samples, in 95% percent of the samples, the so constructed interval (which will be different everytime) contains the true but unknown parameter (see Illustration here, animations of the frequentist nature here). Note, that the “Exact binomial test” was used. There were no approximations made. I would recommend always using exact tests if available, since we are in the 21th century and computers are fast. Again, the \\(\\alpha\\) level of \\(0.05\\) has nothing special (apart from convention) to it. We can also use a \\(\\alpha = 0.14\\) level. In this case, we construct confidence intervals with a 86% confidence level. We’ll discuss the \\(\\alpha\\) level below. binom.test(137, 1000, p = 0.04, alternative = &quot;two.sided&quot;, conf.level = 0.86) ## ## Exact binomial test ## ## data: 137 and 1000 ## number of successes = 137, number of trials = 1000, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.04 ## 86 percent confidence interval: ## 0.1211304 0.1542134 ## sample estimates: ## probability of success ## 0.137 confidence interval: \\(0.1211304\\) to \\(0.1542134\\). With smaller coverage probability (86 instead of 95), we get a narrower interval. Trivially, a 100% confidence interval would be \\(0\\) to \\(1\\) and a 0% confidence interval would be \\(0.04\\) to \\(0.04\\) or any other specific value assuming that the true parameter can take any value from \\(0\\) to \\(1\\). See also Exercise 3. Comparison with Bayesian version of estimating \\(\\theta\\): We cannot include a prior distribution for the paramater \\(\\theta\\). We cannot calculate the posterior distribution of \\(\\theta\\). Hence, we cannot make statements like “the probability that \\(\\theta\\) is larger than 0.04 is 0.9”. Prior knowledge could probably be included in the form of the null hypothesis stating, for instance, that \\(\\theta \\le 0.2\\). This could be based on previous studies or expert knowledge. 5.3 Proportions test If we are interested in comparing proportions, we can use the proportions test. 5.3.1 One sample case \\(H_0: p = 0.5\\). The true, but unknown proportion is 0.5. \\(H_1: p \\ne 0.5\\). The true proportion is different from 0.5. set.seed(443) heads &lt;- rbinom(1, size = 100, prob = 0.5) # create a sample with known probability prop.test(heads, 100, conf.level = 0.94, p = 0.5) # continuity correction TRUE by default ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 0.25, df = 1, p-value = 0.6171 ## alternative hypothesis: true p is not equal to 0.5 ## 94 percent confidence interval: ## 0.3739955 0.5681618 ## sample estimates: ## p ## 0.47 prop.test(heads, 100, correct = FALSE, conf.level = 0.94, p = 0.5) # no continuity correction ## ## 1-sample proportions test without continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 0.36, df = 1, p-value = 0.5485 ## alternative hypothesis: true p is not equal to 0.5 ## 94 percent confidence interval: ## 0.3787665 0.5632834 ## sample estimates: ## p ## 0.47 correct = TRUE indicates that a correction (Yates) is used to make the test more accurate and consider the fact that the test statistic is in fact discrete. 94% confidence interval: \\(0.3739955\\) to \\(0.5681618\\). \\(\\chi^2 = 0.25\\). This is the value of the test statistic used in the test. Under the null hypothesis (in this case \\(H_0: p = 0.5\\)), the test statistic follows a \\(\\chi^2\\) distribution. See also Exercise 4. Let’s verify the test statistic for the test without continuity correction by “hand”: # Observed and expected values heads &lt;- 47 # Observed count of heads total &lt;- 100 # Total flips p_null &lt;- 0.5 # Null hypothesis proportion # Observed proportion p_observed &lt;- heads / total # Z-test statistic Z &lt;- (p_observed - p_null) / sqrt((p_null * (1 - p_null)) / total) # Chi-squared test statistic X_squared &lt;- Z^2 # Print the results Z ## [1] -0.6 X_squared ## [1] 0.36 The \\(\\chi^2\\) distribution can be defined as the sum of squared standard normals \\(Z\\). See Exercise 8. Where does the test statistic come from? According to the central limit theorem, \\(\\bar{X}\\) (= the proportion of successes in the sample) is approximately normally distributed \\[\\bar{X} = \\hat{p} \\sim N(p, \\frac{\\sigma^2}{n}) = N(p, \\frac{p(1-p)}{n}).\\] This means, the statistic \\(\\hat{p}\\) should show the pattern of a Gaußian distribution when repeated many times. \\(n\\) is in the denominator, so the variance of the approximate normal distribution decreases with increasing sample size and we can be more certain about the true proportion with increasing sample size. Note that we want to find the true but unknown proportion \\(p\\) of a Bernoulli distribution which has expected value \\(p\\) and variance \\(p(1-p)\\). Given this distributional statement, it follows that if we subtract the mean assumed under \\(H_0\\) (\\(p_0 = 0.5\\)) and divide by the standard deviation, the resulting test statistic is (approximately) distributed according to a standard normal distribution: \\[Z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\sim N(0, 1).\\] This value squared gives us the value of the test statistic above: \\(0.36\\). 5.3.1.1 Or the same with binomial test \\(H_0: p = 0.5\\) The true but unknown proportion is \\(0.5\\). \\(H_1: p \\ne 0.5\\) The true proportion is different from \\(0.5\\). This is a two-sided test. set.seed(443) heads &lt;- rbinom(1, size = 100, prob = 0.5) # create a sample with known probability binom.test(heads, 100, conf.level = 0.94, p = 0.5) ## ## Exact binomial test ## ## data: heads and 100 ## number of successes = 47, number of trials = 100, p-value = 0.6173 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 94 percent confidence interval: ## 0.3731683 0.5685327 ## sample estimates: ## probability of success ## 0.47 We get a slightly different \\(p\\)-value and confidence interval but the same conclusion. Let’s also verify the \\(p\\)-value and confidence interval by “hand”: \\(p\\)-value: What is the probability that we get 47 successes or more “extreme” (into the direction of the \\(H_1\\)) assuming \\(H_0\\) is true? \\(p\\)-value \\(=\\mathbb{P}(\\text{Number of successes} \\ge 53 \\text{ or} \\le 47)\\). pbinom(47, 100, 0.5) + (1 - pbinom(52, 100, 0.5)) ## [1] 0.6172994 And the \\(p\\)-value is spot on. 94% Confidence interval: These are the so-called exact Clopper-Pearson intervals, since they always have coverage probability of at least \\(1 - \\alpha\\). binom.test(heads, 100, conf.level = 0.94, p = 0.5)$conf.int ## [1] 0.3731683 0.5685327 ## attr(,&quot;conf.level&quot;) ## [1] 0.94 pbinom(47, 100, 0.5685327) ## [1] 0.03000002 1-pbinom(46, 100, 0.3731683) ## [1] 0.02999993 The confidence interval is indeed \\(0.3739955\\) to \\(0.5681618\\). 5.3.2 More than one proportion \\(H_0: p_1 = p_2 = p_3 = p_4 = \\frac{\\sum smokers}{\\sum patients} = 0.9370277\\). Proportions are equal in all 4 groups. \\(H_1: \\text{At least one of the proportions is different}\\) library(tidyverse) # Data from Fleiss (1981), p. 139. # H0: The null hypothesis is that the four populations from which # the patients were drawn have the same true proportion of smokers. # H1: The alternative is that this proportion is different in at # least one of the populations. smokers &lt;- c(83, 90, 129, 70) patients &lt;- c(86, 93, 136, 82) categories &lt;- c(&quot;Group 1&quot;, &quot;Group 2&quot;, &quot;Group 3&quot;, &quot;Group 4&quot;) data.frame( Category = categories, Proportion = smokers / patients ) %&gt;% ggplot(aes(x = Category, y = Proportion)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;, alpha = 0.7) + labs( title = &quot;Proportion of Smokers in Each Group&quot;, x = &quot;&quot;, y = &quot;Proportion&quot; ) + ylim(0, 1) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) prop.test(smokers, patients) ## ## 4-sample test for equality of proportions without continuity correction ## ## data: smokers out of patients ## X-squared = 12.6, df = 3, p-value = 0.005585 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 prop 4 ## 0.9651163 0.9677419 0.9485294 0.8536585 \\(p\\)-value \\(= 0.005585\\). One would argue that this test statistic is unlikely to have come about by chance alone and reject the null hypothesis, that all proportions are equal. X-squared = 12.6, df = 3. The test statistic is distributed according to a \\(\\chi^2\\) distribution with 3 degrees of freedom. See also exercise 5. Three degrees of freedom since we know the last component of the test statistic when 3 out of 4 are given. Degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. The \\(p\\)-value is calculated quickly: pchisq(12.6, df = 3, lower.tail = FALSE) ## [1] 0.005586546 Let’s verify the test statistic by “hand”: props &lt;- smokers / patients (p_null &lt;- sum(smokers) / sum(patients)) ## [1] 0.9370277 (Zs &lt;- (props - rep(p_null, 4)) / (sqrt(p_null * (1 - p_null) / patients))) ## [1] 1.072329 1.219355 0.552180 -3.107860 Zs^2 ## [1] 1.1498887 1.4868262 0.3049028 9.6587936 sum(Zs^2) ## [1] 12.60041 We get exactly the same result by using Pearson’s chi-squared test. Formally, this is a chi-squared test for independence. \\(H_0:\\) Here, we test if the proportion of smokers is independent of the group. If the \\(H_0\\) is true, the number of smokers/non-smokers (in each cell) is just determined by the number of smokers/non-smokers in the sample, how many people are in the respective group and the total number of people in the sample. sm1 &lt;- c(rep(&quot;smoke&quot;, 83), rep(&quot;nosmoke&quot;, 3)) sm2 &lt;- c(rep(&quot;smoke&quot;, 90), rep(&quot;nosmoke&quot;, 3)) sm3 &lt;- c(rep(&quot;smoke&quot;, 129), rep(&quot;nosmoke&quot;, 7)) sm4 &lt;- c(rep(&quot;smoke&quot;, 70), rep(&quot;nosmoke&quot;, 12)) sm &lt;- c(sm1, sm2, sm3, sm4) grp &lt;- c(rep(&quot;A&quot;, 86), rep(&quot;B&quot;, 93), rep(&quot;C&quot;, 136), rep(&quot;D&quot;, 82)) d &lt;- data.frame(sm, grp) table(d$sm, d$grp) ## ## A B C D ## nosmoke 3 3 7 12 ## smoke 83 90 129 70 chisq.test(table(d$sm, d$grp)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(d$sm, d$grp) ## X-squared = 12.6, df = 3, p-value = 0.005585 Let’s verify the test statistic by “hand”: Degress of freedom are: \\(df = (number\\_of\\_columns - 1) \\cdot (number\\_of\\_rows - 1) = 3 \\cdot 1 = 3\\). # Observed data observed &lt;- matrix(c(3, 83, 3, 90, 7, 129, 12, 70), nrow = 2, byrow = FALSE) rownames(observed) &lt;- c(&quot;nosmoke&quot;, &quot;smoke&quot;) colnames(observed) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) # Calculate row totals, column totals, and grand total row_totals &lt;- rowSums(observed) col_totals &lt;- colSums(observed) grand_total &lt;- sum(observed) # Calculate expected counts expected &lt;- outer(row_totals, col_totals, FUN = &quot;*&quot;) / grand_total print(observed) ## A B C D ## nosmoke 3 3 7 12 ## smoke 83 90 129 70 print(expected) ## A B C D ## nosmoke 5.415617 5.856423 8.564232 5.163728 ## smoke 80.584383 87.143577 127.435768 76.836272 # Chi-squared statistic sum((observed - expected)^2 / expected) ## [1] 12.60041 For example, the first cell (nosmoke in group A) in the table is calculated as follows: \\(\\frac{86 \\cdot 25}{397} = \\frac{86}{397} \\cdot \\frac{25}{397} \\cdot 397 = 5.415617\\). These are the expected values in this cell, if one would assume that the proportion of smokers does not depend on the group. Hence, one just multiplies the column (\\(86/397\\)) and row (\\(25/397\\)) proportions and multiples with the grand total (\\(397\\)) to get the absolute number of observations in each cell under \\(H_0\\). 5.3.3 Fisher’s exact test As mentioned above, we should always use exact tests if available. For the test for independence, we could also use Fisher’s exact test. The underlying distribution for the test statistic is a hypergeometric distribution. fisher.test(table(d$sm, d$grp)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(d$sm, d$grp) ## p-value = 0.01447 ## alternative hypothesis: two.sided 5.4 (Classical) \\(t\\)-test The \\(t\\)-test is one of the most famous classical statistical tests out there. Consider these links: 1 2 as starting point. This article could also be interesting. With the \\(t\\)-test, we want to answer the question if the true, but unobserved mean of a population is different from a specific value (one sample \\(t\\)-test) or if the true, but unobserved means of two populations are different from each other (two sample \\(t\\)-test). independent samples paired samples Conveniently, R has a built-in function for these tests. 5.4.1 One sample \\(t\\)-test \\(H_0: \\mu = \\mu_0 = 25\\) \\(H_1: \\mu \\ne \\mu_0 = 25\\) Let’s perform a one sample \\(t\\)-test in R (and let’s ignore the Shapiro-Wilk test in the link): set.seed(1234) my_data &lt;- data.frame( name = paste0(rep(&quot;M_&quot;, 10), 1:10), weight = round(rnorm(10, 20, 2), 1) ) head(my_data, 10) ## name weight ## 1 M_1 17.6 ## 2 M_2 20.6 ## 3 M_3 22.2 ## 4 M_4 15.3 ## 5 M_5 20.9 ## 6 M_6 21.0 ## 7 M_7 18.9 ## 8 M_8 18.9 ## 9 M_9 18.9 ## 10 M_10 18.2 summary(my_data$weight) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 15.30 18.38 18.90 19.25 20.82 22.20 library(ggpubr) ggboxplot(my_data$weight, ylab = &quot;Weight (g)&quot;, xlab = FALSE, ggtheme = theme_minimal()) ggqqplot(my_data$weight, ylab = &quot;Men&#39;s weight&quot;, ggtheme = theme_minimal()) t.test(my_data$weight, mu = 25, alternative = &quot;two.sided&quot;, conf.level = 0.94) ## ## One Sample t-test ## ## data: my_data$weight ## t = -9.0783, df = 9, p-value = 7.953e-06 ## alternative hypothesis: true mean is not equal to 25 ## 94 percent confidence interval: ## 17.888 20.612 ## sample estimates: ## mean of x ## 19.25 t.test(my_data$weight, mu = 25, alternative = &quot;less&quot;, conf.level = 0.94) ## ## One Sample t-test ## ## data: my_data$weight ## t = -9.0783, df = 9, p-value = 3.977e-06 ## alternative hypothesis: true mean is less than 25 ## 94 percent confidence interval: ## -Inf 20.33788 ## sample estimates: ## mean of x ## 19.25 t.test(my_data$weight, mu = 25, alternative = &quot;greater&quot;, conf.level = 0.94) ## ## One Sample t-test ## ## data: my_data$weight ## t = -9.0783, df = 9, p-value = 1 ## alternative hypothesis: true mean is greater than 25 ## 94 percent confidence interval: ## 18.16212 Inf ## sample estimates: ## mean of x ## 19.25 Let’ verify the test statistic by “hand” for the two-sided test: \\[t = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}}\\] \\(\\bar{x} = 19.25\\) \\(s = 2.002915\\) \\(n = 10\\) \\(t = \\frac{19.25 - 25}{2.002915/\\sqrt{10}} = -9.078319\\) Under the null hypothesis and assumptions met, the test statistic is distributed according to a \\(t\\)-distribution with 9 degrees of freedom. Hence, the \\(p\\)-value is: pt(-9.078319, df = 9) * 2 # two-sided ## [1] 7.953381e-06 This matches the output of the t.test function in R. Assumptions of the \\(t\\)-test: You will often see that researchers check if the data is normally distributed. This is not strictly necessary. Merely \\(\\bar{X}\\) must be normally distributed, which is often guaranteed by the central limit theorem for a large enough sample size \\(n\\). IF the data is normally distributed, then \\(\\bar{X}\\) is exactly normally distributed. This is always true and the reason for testing for normality of the data. Another assumption is that \\(\\frac{s^2(n-1)}{\\sigma^2} \\sim \\chi^2_{n-1}\\) Let’s try to verify this assumption empirically: # Load necessary libraries library(ggplot2) # Parameters n &lt;- 10 # Sample size mu_0 &lt;- 25 # Mean under null hypothesis sigma &lt;- 1 # Standard deviation num_simulations &lt;- 10000 # Number of simulations # Simulations set.seed(42) # For reproducibility chi_squared_values &lt;- numeric(num_simulations) for (i in 1:num_simulations) { # Generate random sample sample &lt;- rnorm(n, mean = mu_0, sd = sigma) # Calculate sample variance sample_variance &lt;- var(sample) # Calculate chi-squared value chi_squared_values[i] &lt;- (n - 1) * sample_variance / sigma^2 } data.frame(chi_squared = chi_squared_values) %&gt;% ggplot(aes(x = chi_squared)) + geom_histogram(aes(y = after_stat(density)), bins = 50, fill = &quot;lightblue&quot;, color = &quot;white&quot;) + stat_function(fun = dchisq, args = list(df = n - 1), color = &quot;red&quot;, linewidth = 1) + labs( title = expression(&quot;Empirical Verification of &quot; ~ chi^2 ~ &quot; Distribution&quot;), x = &quot;Value&quot;, y = &quot;Density&quot; ) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5) ) This assumption seems to be met. The last assumption listed is that \\(Z = \\bar{X} - \\mu\\) and \\(s\\) are independent. \\(Z\\) measures how far \\(\\bar{X}\\) is from \\(\\mu\\). Independence would mean here that irrespective of how far away a sample is from the true mean (respectively the mean under \\(H_0\\)), the sample variance has the same distribution. One could verify this theoretically or by applying an independence test. 5.4.2 Two sample \\(t\\)-test \\(H_0: \\mu_1 = \\mu_2\\) \\(H_1: \\mu_1 \\ne \\mu_2\\) Let’s jump right in and use our example from the previous chapter where we performed the Bayesian \\(t\\)-test: (y1 &lt;- c(-.5, 0, 1.2, 1.2, 1.2, 1.9, 2.4, 3) * 100) ## [1] -50 0 120 120 120 190 240 300 (y2 &lt;- c(-1.2, -1.2, -.5, 0, 0, .5, 1.1, 1.9) * 100) ## [1] -120 -120 -50 0 0 50 110 190 length(y1) ## [1] 8 length(y2) ## [1] 8 data &lt;- data.frame(y1, y2) psych::describe(data) ## vars n mean sd median trimmed mad min max range skew kurtosis ## y1 1 8 130.0 116.00 120 130.0 140.85 -50 300 350 -0.13 -1.39 ## y2 2 8 7.5 107.94 0 7.5 118.61 -120 190 310 0.29 -1.38 ## se ## y1 41.01 ## y2 38.16 # Boxplot: data.frame(y = c(y1, y2), group = c(rep(1, 8), rep(2, 8))) %&gt;% ggplot(aes(x = factor(group), y = y)) + # Use factor for discrete x-axis geom_boxplot() + # Add boxplot layer geom_jitter(width = 0.1) # Add jitter for individual # -&gt; Visually, there seems to be a difference between the two groups. t.test(y1, y2, conf.level = 0.93) ## ## Welch Two Sample t-test ## ## data: y1 and y2 ## t = 2.1867, df = 13.928, p-value = 0.04634 ## alternative hypothesis: true difference in means is not equal to 0 ## 93 percent confidence interval: ## 12.55847 232.44153 ## sample estimates: ## mean of x mean of y ## 130.0 7.5 The test statistic is a \\(t\\)-distribution with \\(13.928\\) degrees of freedom (under \\(H_0\\)). The value of the test statistic is \\(2.1867\\). \\(H_0: \\mu_1 = \\mu_2\\). The group means are equal. \\(H_1: \\mu_1 \\ne \\mu_2\\). The group means are different. \\(p\\)-value \\(= 0.04634\\). In the classical framework, this would be considered “significant” at the \\(\\alpha = 0.05\\) level and one would reject the null hypothesis and accept the alternative hypothesis. In the Baysian framework, we abstained from making a decision. 93% confidence interval for the difference in means is rather wide: \\(12.55847\\) to \\(232.44153\\). Let’s try to visualize this. Under the assumption that there is no difference in means, the test statistic \\[ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\] would be distributed according to a \\(t\\)-distribution with 13.928 degrees of freedom. # Load ggplot2 library(ggplot2) # Define degrees of freedom df &lt;- 13.928 # Define the range for x and the critical t-values x &lt;- seq(-4, 4, length.out = 500) critical_t &lt;- 2.1867 # Create a data frame with x and corresponding density values t_dist &lt;- data.frame( x = x, density = dt(x, df) ) # Plot the t-distribution with shaded tail areas ggplot(t_dist, aes(x = x, y = density)) + # Add the main t-distribution curve geom_line(linewidth = 1, color = &quot;blue&quot;) + # Add shaded areas below the curve outside the critical t-values geom_ribbon( data = subset(t_dist, x &lt; -critical_t), aes(ymin = 0, ymax = density), fill = &quot;red&quot;, alpha = 0.3 ) + geom_ribbon( data = subset(t_dist, x &gt; critical_t), aes(ymin = 0, ymax = density), fill = &quot;red&quot;, alpha = 0.3 ) + # Add vertical lines for the critical t-values geom_vline(xintercept = c(-critical_t, critical_t), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + # Annotate the critical t-values annotate(&quot;text&quot;, x = -critical_t, y = 0.05, label = paste0(&quot;-t = &quot;, critical_t), angle = 90, vjust = -0.5) + annotate(&quot;text&quot;, x = critical_t, y = 0.05, label = paste0(&quot;t = &quot;, critical_t), angle = 90, vjust = -0.5) + # Add labels and style labs( title = &quot;t-Distribution with Shaded Critical Areas (Two-Sided Test)&quot;, subtitle = paste(&quot;Degrees of Freedom:&quot;, df), x = &quot;t&quot;, y = &quot;Density&quot; ) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the title plot.subtitle = element_text(hjust = 0.5) # Center the subtitle ) # p-value manually: pt(-2.1867, df) * 2 ## [1] 0.04633225 The plot shows the \\(t\\)-distribution. Marked in red are the areas where the test statistic takes the value we observed or “more extreme” values. With “more extreme” we mean values that are further away from 0 in both directions since we conducted a two-sided test. The area under the curve is the \\(p\\)-value. As you can see, the \\(p\\)-value is the sum of the two red areas and matches the output of the \\(t\\)-test function in R. Interesting: This R-file demonstrates what one single outlier can do to a \\(t\\)-test. 5.4.3 Paired \\(t\\)-test In the last case, we assume, that the two samples are not independent anymore. We could, for instance, think of two weight measurements (kg) of the same person before and after a diet. Obviously, these two measurements are not independent. Higher pre-diet weights could be associated with higher post-diet weights. Analog to before, we now ask if the differences are “significantly” different from \\(\\mu_0\\). In the bodyweight example, let’s define a 5 kg weight loss as clinically relevant (in reality this is more like 5%), but we keep it simple. \\(H_0: \\mu_D \\le \\mu_0 = 5~kg\\). Participants lost 5 kg or less on average, i.e. the true difference between pre- and post weight is on average smaller than 5kg. \\(H_1: \\mu_D &gt; \\mu_0 = 5~kg\\). Participants lost more than 5 kg on average. Aussumptions for the paired \\(t\\)-test: - Interval or ratio scale - The differences are independent of each other - Let’s create some data and perform the test: library(pacman) p_load(MASS, tidyverse) # Create correlated pre post weight-data set.seed(62) # For reproducibility n &lt;- 15 # Number of pairs mu &lt;- c(80, 73) # Mean vector for Pre and Post weights rho &lt;- 0.7 # Correlation sigma &lt;- 10 # Standard deviation for both variables Sigma &lt;- matrix(c(sigma^2, rho * sigma^2, rho * sigma^2, sigma^2), nrow = 2) weights &lt;- mvrnorm(n, mu = mu, Sigma = Sigma) df &lt;- data.frame( Pre_Weight = round(weights[, 1], 2), Post_Weight = round(weights[, 2], 2) ) df ## Pre_Weight Post_Weight ## 1 87.56 80.21 ## 2 78.36 79.88 ## 3 63.56 61.55 ## 4 76.20 78.17 ## 5 101.25 93.44 ## 6 85.96 68.18 ## 7 84.48 85.37 ## 8 65.72 65.12 ## 9 75.90 78.89 ## 10 73.39 61.41 ## 11 71.29 64.28 ## 12 97.38 75.29 ## 13 80.32 78.59 ## 14 64.67 59.79 ## 15 78.41 56.64 cor(df$Pre_Weight, df$Post_Weight) ## [1] 0.7023552 df %&gt;% mutate(pre_post_diff = Post_Weight - Pre_Weight) %&gt;% ggplot(aes(x = Pre_Weight, y = Post_Weight)) + geom_point() + labs(title = &quot;Pre-Post Weights with the same person&quot;, x = &quot;Pre-Weight [kg]&quot;, y = &quot;Post-Weight [kg]&quot;) + theme(plot.title = element_text(hjust = 0.5)) t.test(df$Pre_Weight, df$Post_Weight, paired = TRUE) ## ## Paired t-test ## ## data: df$Pre_Weight and df$Post_Weight ## t = 2.987, df = 14, p-value = 0.0098 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 1.835423 11.183244 ## sample estimates: ## mean difference ## 6.509333 Let’s verify the test statistic and \\(p\\)-value by “hand”: mean_diff &lt;- mean(df$Post_Weight - df$Pre_Weight) sd_diff &lt;- sd(df$Post_Weight - df$Pre_Weight) n &lt;- nrow(df) t_stat &lt;- mean_diff / (sd_diff / sqrt(n)) t_stat ## [1] -2.987034 pt(t_stat, df = n - 1) * 2 ## [1] 0.009800118 Conclusion: It is unlikely that we would see a mean difference of 6.5 kg or more by chance alone assuming that the true mean difference is 5 kg or less. We reject the null hypothesis and accept the alternative hypothesis that the mean weight loss was more than 5 kg. 5.5 Correlation test In the chapter about descriptive statistics, we calculated the (Pearson) correlation coefficient to measure the strength of the linear relationship between two variables. Often, the null hypothesis for the correlation coefficient is that there is no correlation between the two variables (\\(\\rho=0\\)). One could argue that this is a rather baseless assumption. In reality, the true correlation coefficient is probably not exactly 0 and one could argue more precisely a range of plausible values for \\(\\rho\\) for the specific case at hand. Often, one can see an ocean of \\(p\\)-values in the literature, where the correlation coefficient is tested against 0. This is superfluous. For example, if the sample size is \\(n=234\\) and the sample correlation coefficient is \\(r = 0.76\\), it is very unlikely that the true correlation coefficient is 0. One does not need a hypothesis test to know this (see exercise 6). 5.5.1 Classical correlation test We can do a correlation test in R with the cor.test function. This article is also helpful. Maybe, let’s take the part about the Shapiro-Wilk test not too seriously. The test statistic(s) for the test(s) can be found here. If we really want to take the result of such a test seriously, we need to check the assumptions of the test. This might be an interesting read. The correlation plot from the article looks like this: library(ggpubr) mtcars %&gt;% ggscatter(x = &quot;mpg&quot;, y = &quot;wt&quot;, add = &quot;reg.line&quot;, conf.int = TRUE, cor.coef = TRUE, cor.method = &quot;pearson&quot;, xlab = &quot;Miles/(US) gallon&quot;, ylab = &quot;Weight (1000 lbs)&quot;) cor.test(mtcars$mpg, mtcars$wt, conf.level = 0.96) ## ## Pearson&#39;s product-moment correlation ## ## data: mtcars$mpg and mtcars$wt ## t = -9.559, df = 30, p-value = 1.294e-10 ## alternative hypothesis: true correlation is not equal to 0 ## 96 percent confidence interval: ## -0.9360192 -0.7362129 ## sample estimates: ## cor ## -0.8676594 \\(r=0.87\\) is the sample correlation coefficient. Interestingly, cor.test in R can only test \\(H_0: \\rho = 0\\). In the scatter plot, we can see a linear relationship between the two variables. The correlation might therefore be a useful measure to describe the relationship between the two variables. t is the \\(t\\)-test statistic value (t = -9.559), df is the degrees of freedom (df= 30), \\(p\\)-value of the \\(t\\)-test (\\(p\\)-value = \\(1.29410^{-10}\\)). conf.int is the confidence interval of the correlation coefficient at 96% (conf.int = \\([-0.9360192, -0.7362129]\\)) This is what is most interesting to us and should be interpreted in the context of the data. sample estimates is the correlation coefficient (\\(r = -0.87\\)). Let’s verify the test statistic for the two-sided test by “hand”: For pairs from an uncorrelated bivariate normal distribution, the sampling distribution of the studentized Pearson’s correlation coefficient follows a \\(t\\)-distribution with degrees of freedom \\(n − 2\\). Specifically, if the underlying variables have a bivariate normal distribution, the variable \\[t = \\frac{r \\sqrt{n-2}}{\\sqrt{1-r^2}}\\] has a student’s \\(t\\)-distribution in the null case (zero correlation). (r &lt;- cor(mtcars$mpg, mtcars$wt)) ## [1] -0.8676594 (n &lt;- nrow(mtcars)) ## [1] 32 (t &lt;- r * sqrt(n - 2) / sqrt(1 - r^2)) ## [1] -9.559044 (p &lt;- 2 * pt(-abs(t), df = n - 2)) ## [1] 1.293959e-10 This matches the output of the cor.test function in R. Study this file to understand the variability of the sample correlation coefficient under the null hypothesis (\\(\\rho=0\\)). Please be aware of this variability when interpreting the results of your master thesis. 5.5.2 Bootstrap confidence interval for the correlation coefficient We could also use the so-called simple non-parametric bootstrap to estimate the confidence interval for the correlation coefficient. These are the steps: Create a new sample by drawing with replacement from the original sample. Repeat this process many times (e.g., 1000 times). Calculate the correlation coefficient for each new sample. Calculate adequate sample quantiles of the correlation coefficients to get a confidence interval. There is a simple elegance to this method. It is very general and can be applied to many different problems. Let’s apply it to finding a confidence interval for the correlation coefficient between mpg and wt in the mtcars dataset. # Bootstrap confidence interval for the correlation coefficient set.seed(123) cors &lt;- replicate(1000, { idx &lt;- sample(1:n, n, replace = TRUE) cor(mtcars$mpg[idx], mtcars$wt[idx]) }) hist(cors) quantile(cors, c(0.02, 0.5, 0.98)) # 96% confidence interval ## 2% 50% 98% ## -0.9260149 -0.8745279 -0.7821605 We used the variability contained in the data to estimate the variability of the correlation coefficient. As you can see in the histrogram, correlations close to 0 are very unlikely. The 96% bootstrap confidence interval is \\([-0.926, -0.782]\\). You can also study the nice animation of the bootstrap here. 5.5.3 Comparison with Bayesian approach For the Bayesian approach, we can use the correlationBF function from the BayesFactor package. In analogy to the classical test, we can “test” the null hypothesis by assuming a prior distribution for the correlation coefficient symmetric around the value 0 (slee prior plot below). library(BayesFactor) ## Loading required package: coda ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ************ ## Welcome to BayesFactor 0.9.12-4.7. If you have questions, please contact Richard Morey (richarddmorey@gmail.com). ## ## Type BFManual() to open the manual. ## ************ ## ## Attaching package: &#39;BayesFactor&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## posterior library(tidyverse) set.seed(444) # Define a function for the shifted, scaled Beta prior shifted_beta_prior &lt;- function(rho, rscale) { # Transform Beta to [-1, 1] if (rho &gt;= -1 &amp;&amp; rho &lt;= 1) { # (rho + 1) / 2 transforms [-1, 1] to [0, 1] beta_density &lt;- dbeta((rho + 1) / 2, 1 / rscale, 1 / rscale) / 2 return(beta_density) } else { return(0) } } # Define rho values and rscale rho_values &lt;- seq(-1, 1, by = 0.01) rscale &lt;- 1 / 3 # Medium scale # Compute prior values prior_values &lt;- sapply(rho_values, shifted_beta_prior, rscale = rscale) # Plot the prior data.frame(rho = rho_values, density = prior_values) %&gt;% ggplot(aes(x = rho, y = density)) + geom_line(color = &quot;blue&quot;, linewidth = 1.2) + labs( title = &quot;Shifted Scaled Beta Prior Distribution for Correlation&quot;, x = &quot;Correlation (rho)&quot;, y = &quot;Density&quot; ) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5, size = 16), text = element_text(size = 14) ) # Compute the posterior samples using correlationBF() posterior_samples &lt;- correlationBF( # Bayes factors or posterior samples for correlations. y = mtcars$mpg, x = mtcars$wt, # Use &quot;medium&quot; scale (1/3); prior scale. Preset values can be given as strings rscale = &quot;medium&quot;, posterior = TRUE, # Get posterior samples iterations = 10000 # Number of MCMC iterations ) ## Independent-candidate M-H acceptance rate: 49% # Independent-candidate M-H = Independent-candiate Metropolis-Hastings algorithm posterior_samples %&gt;% ggplot(aes(x = rho)) + geom_histogram(aes(y = after_stat(density)), bins = 30, fill = &quot;lightblue&quot;, color = &quot;black&quot;) + geom_density(color = &quot;blue&quot;, linewidth = 1.2) + labs( title = &quot;Posterior Distribution of Correlation (rho)&quot;, x = &quot;Correlation (rho)&quot;, y = &quot;Density&quot; ) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5, size = 16), text = element_text(size = 14) ) # Compute 96% credible interval credible_interval &lt;- quantile(posterior_samples[, &quot;rho&quot;], probs = c(0.02, 0.98)) cat(&quot;96% Credible Interval for rho:&quot;, credible_interval[1], &quot;to&quot;, credible_interval[2], &quot;\\n&quot;) ## 96% Credible Interval for rho: -0.9083272 to -0.6384908 median(posterior_samples[, &quot;rho&quot;]) ## [1] -0.8159496 The following intervals were obtained in three different ways: Classical frequentist: 96% confidence interval: \\([-0.9360192, -0.7362129]\\) Estimate: \\(-0.8676594\\) Bootstrap: 96% boostrap confidence interval: \\([-0.926, -0.782]\\) Median of the bootstrap samples: \\(-0.8745279\\) Bayesian: 96% credible interval: \\([-0.9083272, -0.6384908]\\) Median of the posterior samples: \\(-0.8159496\\) This website should be fun to explore. 5.6 Type 1 and Type 2 errors Decision about null hypothesis (H0) Null hypothesis (H0) is true Null hypothesis (H0) is false Not reject Correct inference (true negative) (probability = 1 - α) Type II error (false negative) (probability = β) Reject Type I error (false positive) (probability = α) Correct inference (true positive) (probability = 1 - β) In the dichotomous world of hypothesis testing, we can make two types of errors: A type 1 error (false positive) occurs when we reject the null hypothesis when it is actually true. A type 2 error (false negative) occurs when we do not reject the null hypothesis when it is actually false. The expression \\(1 - \\beta\\) is called the power of the test. It is the probability of correctly rejecting the null hypothesis when it is false. This R-file could be interesting to study with respect to \\(H_0\\) and the probability of finding the truth. How to choose \\(\\alpha\\) and \\(\\beta\\)? So far, I have not seen this question answered in a practical way in papers. One typically reads that “the level of statistical significance is set at \\(\\alpha = 0.05\\)”, which is arbitrary. Mudge et al. (2012) answer the question like this: “Thus, the logical decision-making significance threshold, \\(\\alpha\\), should be the value that minimizes the probability, or occasionally, the cost of making any relevant error.” We have encountered the argument, that Bayesian statistics is subjective. Unfortunately, a similar problem arises in NHST. To quote Mudge et al. again: “… consistently using \\(\\alpha = 0.05\\) is not an objective approach. Subjectivity is merely shifted away from the choice of \\(\\alpha\\) to the choice of sample size, such that if a researcher wants to find statistical significance using \\(\\alpha = 0.05\\), they should conduct a test with a large sample size …” For the case of equal a priori probability of the \\(H_0\\) and \\(H_1\\) and equal consequence for both types or errors, the suggest to choose alpha by minimizing \\[\\omega = \\frac{\\alpha + \\beta}{2}.\\] 5.7 The frequentist confidence interval In frequentist statistics, a confidence interval (CI) is an interval which is expected to typically (with repeated sampling) contain the true but unknown parameter with a certain frequency (probability). It is very important to understand that for every sample, the confidence interval is different and in the long run, these intervals will contain the true parameter in a certain percentage of cases. This is a nice visualization of the concept. See also exercise 7. 1:1 relationship between \\(p\\)-values and confidence intervals: There is a 1:1 relationship between “statistical significance” and confidence intervals. Example 1: If we test (\\(\\alpha = 0.06\\)) the null hypothesis that the mean of a population is 0 (\\(H_1: \\mu \\ne 0\\)), and we get a \\(p\\)-value of 0.0298, then the 94% confidence interval for the mean will not contain 0. Example 2: If we test (\\(\\alpha = 0.07\\)) the null hypothesis that means of two independent populations are equal (\\(H_1: \\mu_1 \\ne \\mu_2\\)), and we get a \\(p\\)-value of 0.0433, then the 93% confidence interval for the difference in means will not contain 0. Example 3: If we test (\\(\\alpha = 0.05\\)) the null hypothesis that the risk ratio for lung cancer equas 1 in smokers vs. non-smokers (\\(H_1: RR \\ne 1\\)), and we get a \\(p\\)-value of 0.00032, then the 97% confidence interval for the risk ratio (RR) will not contain 1. 5.8 Simulations based approaches Through the power of modern computers, we can simulate all kinds of hypothesis tests. We just assume that the null hypothesis is true and draw samples from the very distribution. In the old times, computational ressources were scarce and one had to rely on tables that were precalculated. This shows you how the correlation coefficient behaves under the null hypothesis (that there is no correlation; \\(\\rho = 0\\)). This is a very useful tool to understand the behavior of a test statistic under the null hypothesis. Of course, the probability that a correlation coefficient is exactly 0 is 0 since it’s a continuous variable, but in practice we are interested in “indistinguishable from 0” which is a small value. In the Bayesian framework from Kruschke, we defined a region of practical equivalence (ROPE) for this purpose. We ask ourselves: What does my test statistic do if the null hypothesis is true? Then try to simulate it. This helps to understand what constitutes a qualitatively different result (from the null hypothesis) considering variability in the data. 5.9 Exercises 5.9.1 Exercise 1 - frequentist confidence interval Create 1000 random samples from a binomial distribution with \\(n = 100\\) and \\(p = 0.38\\). Calculate the 96% confidence interval for each sample using R. How often was the true parameter (\\(p = 0.38\\)) contained in the constructed interval? 5.9.2 Exercise 2 - everything becomes “significant” Setting: two sample \\(t\\)-test. Assume there is a small difference between the means of two groups. - Show via simulation that with increasing sample size, the \\(p\\)-value becomes smaller and smaller and will be “significant” at some point irresespective of how small the true mean difference is and how small the \\(\\alpha-\\) level is. 5.9.3 Exercise 3 - binomial test Create a sample from a binomial distribution with \\(n = 54\\) and \\(p = 0.68\\). Perform a two-sided binomial test with \\(H_0: p = 0.5\\). Calculate the 90% confidence interval for the sample proportion. Calculate the p-value for the two-sided test by “hand” (using dbinom/pbinom in R). 5.9.4 Exercise 4 - proportions test Create a sample from a binomial distribution with \\(n = 100\\) and \\(p = 0.5\\). Perform a proportions test with \\(H_0: p = 0.5\\) and interpret the results. Perform the proportions test with the whole range of possible proportions \\(H_0: p = 0.01 \\cdots p = 0.99\\) in steps of \\(0.01\\). And plot the p-values on the y-axis and the assumed proportion on the x-axis. This is called a \\(p\\)-value function. 5.9.5 Exercise 5 - proportions test 2 Use the data from the smokers proportions test example above. Draw a \\(\\chi^2\\) distribution with 3 degrees of freedom and calculate the probability of observing a value of 12.6 or larger. 5.9.6 Exercise 6 - correlation coefficent Create a sample of \\(234\\) pairs of uncorrelated observations \\((x_i,y_i)\\). \\(X_i\\) and \\(Y_i\\) are drawn from a normal distribution with mean 0 and standard deviation 1. Calculate the sample correlation coefficient \\(r\\). Repeat this 1000 times. How often was the sample correlation coefficient larger than 0.76? 5.9.7 Exercise 7 - coverage frequency of CI Create a sample (vary the sample size, start small) from a normal distribution with mean 0 and standard deviation 1. Calculate the 93% confidence interval for the mean. Repeat this 1000 times. How often was the true mean (0) contained in the constructed interval? 5.9.8 Exercise 8 - \\(\\chi^2\\)-distribution The \\(\\chi^2\\)-distribution is defined as the sum of squared standard normals. Very this by simulation in R. Draw 1000 samples of size 3 from a standard normal distribution. Calculate the sum of squared values for each sample. Do this repeatedly and plot the histogram of the resulting values with the \\(\\chi^2\\)-density in the diagram. Use Q-Q plots to compare the two distributions. The distribution should be a \\(\\chi^2\\)-distribution with 3 degrees of freedom. 5.10 Sample exam questions for this chapter (in German since exam is in German) For this section, no solutions are provided. 5.10.1 Frage 1 Welche der folgenden Aussage(n) über Nullhypothesen-Signifikanztests (NHST) ist/sind korrekt (0-4 korrekte Antworten)? Der \\(p\\)-Wert gibt die Wahrscheinlichkeit an, eine Teststatistik zu erhalten, die genauso extrem (oder extremer) ist wie die beobachtete, vorausgesetzt, die Nullhypothese ist wahr. NHST ist darauf ausgelegt, zu beweisen, dass die Alternativhypothese wahr ist. Dichotomes Denken im NHST vereinfacht die Komplexität von realen Daten zu stark. Das Signifikanzniveau (\\(\\alpha\\)) gibt die Wahrscheinlichkeit dafür an, dass man die Nullhypothese (\\(H_0\\)) beibehält, obwohl die Alternativhypothese (\\(H_1\\)) wahr ist. 5.10.2 Frage 2 Wir ziehen eine Stichprobe mit 3 Werten und erhalten folgendes Ergebnis: \\(x = (4,4,4)\\). In der Vergangenheit gingen wir davon aus, dass die Zufallsvariable \\(X\\) folgendermaßen verteilt ist: \\[ \\begin{array}{|c|c|} \\hline x &amp; P(X = x) \\\\ \\hline 1 &amp; 0.2 \\\\ 2 &amp; 0.3 \\\\ 3 &amp; 0.4 \\\\ 4 &amp; 0.1 \\\\ \\hline \\end{array} \\] Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Die Wahrscheinlichkeit, dieses Ereignis unter der Annahme der Gültigkeit obiger Verteilung zu beobachten ist \\(10^{-4}\\). Unter der Annahme der Gültigkeit obiger Verteilung wäre die Wahrscheinlichkeit mindestens einmal \\(1,2\\) oder \\(3\\) zu sehen 99,9%. Wählt man als Teststatistik die Anzahl der \\(4\\)er in der Stichprobe, so ist die Teststatistik unter der Annahme der Gültigkeit obiger Verteilung binomialverteilt. Über die Stichprobe \\((1,1,1)\\) wäre man weniger überrascht als über die Stichprobe \\((4,4,4)\\). 5.10.3 Frage 3 - \\(t\\)-Test ## ## Two Sample t-test ## ## data: group_a and group_b ## t = -0.10816, df = 58, p-value = 0.5429 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -0.4186255 Inf ## sample estimates: ## mean of x mean of y ## 4.952896 4.978338 Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Es handelt sich um einen zweiseitigen \\(t\\)-Test. Aufgrund des \\(p\\)-Wertes würde man in diesem Fall die Nullhypothese nicht verwerfen. Die Alternativhypothese lautet: \\(H_1: \\mu_{group\\_a} - \\mu_{group\\_b} &gt; 0\\). Die Nullhypothese lautet: \\(H_0: \\mu_{group\\_a} = \\mu_{group\\_b}\\). 5.10.4 Frage 4 Wir führen einen Hypothesentest für Proportionen in R durch (\\(n=100\\)). Hinweis: Nicht den \\(p\\)-Wert mit dem Parameter p (der gesuchten Proportion) verwechseln. ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability p_null ## X-squared = 6.4533, df = 1, p-value = 0.01107 ## alternative hypothesis: true p is not equal to 0.75 ## 97 percent confidence interval: ## 0.5317140 0.7356931 ## sample estimates: ## p ## 0.64 Welche der folgenden Aussage(n) ist/sind korrekt (0-4 korrekte Antworten)? Es waren 64 “Erfolge” in 100 Versuchen. Die Wahrscheinlichkeit, dass der ware aber unbekannte Parameter p im Intervall \\([0.5317140, 0.7356931]\\) liegt, beträgt 97%. Die Nullhypothese lautet: \\(H_0: \\text{p} \\ge 0.75\\). Würde man statt 97% nur 90% wählen, wäre das Intervall breiter. "],["references.html", "References", " References Westfall, Peter. 2020. Understanding Regression Analysis: An Conditional Distribution Approach. Wiley. "]]
